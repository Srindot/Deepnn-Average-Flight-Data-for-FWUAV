{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Network for Design Parameters for FWUAV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modules to import for data preprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as rnd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "# Modules to import for Torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Check for GPU device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Data loaded into pandas : \n",
      " <module 'torch.utils.data' from '/home/stimp/anaconda3/envs/mlenv/lib/python3.8/site-packages/torch/utils/data/__init__.py'>\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Features : \n",
      " [['naca8304' 0.4000000059604645 1.5 ... 0.4000000059604645 3.0 5.0]\n",
      " ['naca8304' 0.4000000059604645 1.5 ... 0.4000000059604645 3.0 15.0]\n",
      " ['naca8304' 0.4000000059604645 1.5 ... 0.4000000059604645 3.0 25.0]\n",
      " ...\n",
      " ['goe225' 0.800000011920929 2.0 ... 1.100000023841858 6.5 35.0]\n",
      " ['naca2412' 0.800000011920929 2.0 ... 1.100000023841858 6.5 35.0]\n",
      " ['naca0012' 0.800000011920929 2.0 ... 1.100000023841858 6.5 35.0]]\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Targets : \n",
      " [[-0.18576185405254364 0.010605402290821075]\n",
      " [-0.4964708685874939 -0.05340539291501045]\n",
      " [-0.8249507546424866 -0.16106195747852325]\n",
      " ...\n",
      " [-33.38216781616211 -5.573355197906494]\n",
      " [-30.313720703125 -4.444688320159912]\n",
      " [-29.279325485229492 -4.102921962738037]]\n"
     ]
    }
   ],
   "source": [
    "# Loading the data into the pandas dataframe\n",
    "\n",
    "# Printing the datafram \n",
    "print(\"\\n Data loaded into pandas : \\n\", data)\n",
    "\n",
    "# Reading the Data\n",
    "data = pd.read_csv(\"Design_Data.csv\", header=None)\n",
    "\n",
    "# Set the first column to object type\n",
    "data[:0] = data[:0].astype('object')\n",
    "\n",
    "# Set dtype for the rest of the columns to float32\n",
    "data.iloc[:, 1:] = data.iloc[:, 1:].astype('float32')\n",
    "\n",
    "print(\"\\n---------------------------------------------------------------------------\\n\")\n",
    "\n",
    "# Inserting the data columns \n",
    "data.columns = [\n",
    "    'Airfoil','Wing Span', 'Taper Ratio', 'Aspect Ratio', 'Flapping Period',\n",
    "    'Airspeed', 'Angle of Attack', 'Lift', 'Induced Drag'\n",
    "]\n",
    "\n",
    "print(\"\\n---------------------------------------------------------------------------\\n\")\n",
    "\n",
    "# Split the data into features and targets\n",
    "X = data[[\n",
    "    'Airfoil','Wing Span', 'Taper Ratio', 'Aspect Ratio', 'Flapping Period',\n",
    "    'Airspeed', 'Angle of Attack'\n",
    "]]\n",
    "\n",
    "print(\"Features : \\n\", X.values)\n",
    "\n",
    "print(\"\\n---------------------------------------------------------------------------\\n\")\n",
    "y = data[['Lift', 'Induced Drag']]\n",
    "\n",
    "print(\"Targets : \\n\",y.values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One Hot encoder and scaler fit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Encoded Airfoils:\n",
      " [[0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]]\n",
      "\n",
      "Normalized Features without Airfoil:\n",
      "        Wing Span  Taper Ratio  Aspect Ratio  Flapping Period  Airspeed  \\\n",
      "0       0.076923     0.076923      0.000000         0.222222  0.222222   \n",
      "1       0.076923     0.076923      0.000000         0.222222  0.222222   \n",
      "2       0.076923     0.076923      0.000000         0.222222  0.222222   \n",
      "3       0.076923     0.076923      0.000000         0.222222  0.222222   \n",
      "4       0.076923     0.076923      0.000000         0.222222  0.444444   \n",
      "...          ...          ...           ...              ...       ...   \n",
      "10682   0.384615     0.230769      0.333333         1.000000  1.000000   \n",
      "10683   0.384615     0.230769      0.333333         1.000000  1.000000   \n",
      "10684   0.384615     0.230769      0.333333         1.000000  1.000000   \n",
      "10685   0.384615     0.230769      0.333333         1.000000  1.000000   \n",
      "10686   0.384615     0.230769      0.333333         1.000000  1.000000   \n",
      "\n",
      "       Angle of Attack  \n",
      "0             0.000000  \n",
      "1             0.285714  \n",
      "2             0.571429  \n",
      "3             0.857143  \n",
      "4             0.000000  \n",
      "...                ...  \n",
      "10682         0.714286  \n",
      "10683         0.714286  \n",
      "10684         0.857143  \n",
      "10685         0.857143  \n",
      "10686         0.857143  \n",
      "\n",
      "[10687 rows x 6 columns]\n",
      "\n",
      "Normalized Targets:\n",
      "            Lift  Induced Drag\n",
      "0      0.999805      0.041019\n",
      "1      0.998737      0.040646\n",
      "2      0.997607      0.040018\n",
      "3      0.996447      0.039060\n",
      "4      0.999344      0.040973\n",
      "...         ...           ...\n",
      "10682  0.909011      0.022258\n",
      "10683  0.912632      0.023970\n",
      "10684  0.885678      0.008461\n",
      "10685  0.896227      0.015042\n",
      "10686  0.899784      0.017035\n",
      "\n",
      "[10687 rows x 2 columns]\n",
      "\n",
      "Encoded Airfoil DataFrame:\n",
      "        Airfoil_goe225  Airfoil_naca0012  Airfoil_naca2412  Airfoil_naca8304\n",
      "0                 0.0               0.0               0.0               1.0\n",
      "1                 0.0               0.0               0.0               1.0\n",
      "2                 0.0               0.0               0.0               1.0\n",
      "3                 0.0               0.0               0.0               1.0\n",
      "4                 0.0               0.0               0.0               1.0\n",
      "...               ...               ...               ...               ...\n",
      "10682             0.0               0.0               1.0               0.0\n",
      "10683             0.0               1.0               0.0               0.0\n",
      "10684             1.0               0.0               0.0               0.0\n",
      "10685             0.0               0.0               1.0               0.0\n",
      "10686             0.0               1.0               0.0               0.0\n",
      "\n",
      "[10687 rows x 4 columns]\n",
      "\n",
      "Scaled and Encoded Features:\n",
      "        Airfoil_goe225  Airfoil_naca0012  Airfoil_naca2412  Airfoil_naca8304  \\\n",
      "0                 0.0               0.0               0.0               1.0   \n",
      "1                 0.0               0.0               0.0               1.0   \n",
      "2                 0.0               0.0               0.0               1.0   \n",
      "3                 0.0               0.0               0.0               1.0   \n",
      "4                 0.0               0.0               0.0               1.0   \n",
      "...               ...               ...               ...               ...   \n",
      "10682             0.0               0.0               1.0               0.0   \n",
      "10683             0.0               1.0               0.0               0.0   \n",
      "10684             1.0               0.0               0.0               0.0   \n",
      "10685             0.0               0.0               1.0               0.0   \n",
      "10686             0.0               1.0               0.0               0.0   \n",
      "\n",
      "       Wing Span  Taper Ratio  Aspect Ratio  Flapping Period  Airspeed  \\\n",
      "0       0.076923     0.076923      0.000000         0.222222  0.222222   \n",
      "1       0.076923     0.076923      0.000000         0.222222  0.222222   \n",
      "2       0.076923     0.076923      0.000000         0.222222  0.222222   \n",
      "3       0.076923     0.076923      0.000000         0.222222  0.222222   \n",
      "4       0.076923     0.076923      0.000000         0.222222  0.444444   \n",
      "...          ...          ...           ...              ...       ...   \n",
      "10682   0.384615     0.230769      0.333333         1.000000  1.000000   \n",
      "10683   0.384615     0.230769      0.333333         1.000000  1.000000   \n",
      "10684   0.384615     0.230769      0.333333         1.000000  1.000000   \n",
      "10685   0.384615     0.230769      0.333333         1.000000  1.000000   \n",
      "10686   0.384615     0.230769      0.333333         1.000000  1.000000   \n",
      "\n",
      "       Angle of Attack  \n",
      "0             0.000000  \n",
      "1             0.285714  \n",
      "2             0.571429  \n",
      "3             0.857143  \n",
      "4             0.000000  \n",
      "...                ...  \n",
      "10682         0.714286  \n",
      "10683         0.714286  \n",
      "10684         0.857143  \n",
      "10685         0.857143  \n",
      "10686         0.857143  \n",
      "\n",
      "[10687 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "\n",
    "# Assuming X and y are already defined in your Jupyter notebook\n",
    "# X - Features DataFrame\n",
    "# y - Targets DataFrame\n",
    "\n",
    "# Drop the Airfoil column from the features\n",
    "airfoils = X['Airfoil']\n",
    "\n",
    "# One hot encode the airfoil column\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "encoded_airfoil = encoder.fit_transform(airfoils.values.reshape(-1, 1))\n",
    "\n",
    "print(\"\\nEncoded Airfoils:\\n\", encoded_airfoil)\n",
    "\n",
    "X_num = X.drop('Airfoil', axis=1)\n",
    "\n",
    "Scalerx = MinMaxScaler()\n",
    "Scalery = MinMaxScaler()\n",
    "\n",
    "X_scaled_np = Scalerx.fit_transform(X_num)\n",
    "X_scaled = pd.DataFrame(X_scaled_np, columns=X_num.columns)\n",
    "print(\"\\nNormalized Features without Airfoil:\\n\", X_scaled)\n",
    "\n",
    "y_scaled_np = Scalery.fit_transform(y.values)\n",
    "y_scaled = pd.DataFrame(y_scaled_np, columns=y.columns)\n",
    "print(\"\\nNormalized Targets:\\n\", y_scaled)\n",
    "\n",
    "X_encoded_pd = pd.DataFrame(encoded_airfoil, columns=encoder.get_feature_names_out(['Airfoil']))\n",
    "print(\"\\nEncoded Airfoil DataFrame:\\n\", X_encoded_pd)\n",
    "\n",
    "X_final = pd.concat([X_encoded_pd, X_scaled], axis=1)\n",
    "print(\"\\nScaled and Encoded Features:\\n\", X_final)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting the data into training and testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training features shape: (8549, 10)\n",
      "Testing features shape: (2138, 10)\n",
      "Training target shape: (8549, 2)\n",
      "Testing target shape: (2138, 2)\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_final, y_scaled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Verify the split\n",
    "print(\"Training features shape:\", X_train.shape)\n",
    "print(\"Testing features shape:\", X_test.shape)\n",
    "print(\"Training target shape:\", y_train.shape)\n",
    "print(\"Testing target shape:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeepNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(10, 128)  # Change input features to 10\n",
    "        self.dropout1 = nn.Dropout(0.5)  # Dropout layer\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.dropout2 = nn.Dropout(0.5)  # Dropout layer\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.dropout3 = nn.Dropout(0.5)  # Dropout layer\n",
    "        self.fc4 = nn.Linear(32, 2)    # Change output features to 2\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.dropout3(x)\n",
    "        x = self.fc4(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to 2D PyTorch tensors\n",
    "X_train = torch.tensor(X_train.values, dtype=torch.float32).to(device)\n",
    "y_train = torch.tensor(y_train.values, dtype=torch.float32).to(device)\n",
    "X_test = torch.tensor(X_test.values, dtype=torch.float32).to(device)\n",
    "y_test = torch.tensor(y_test.values, dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8549, 10])\n",
      "tensor([[0.9192, 0.1097],\n",
      "        [0.9863, 0.0346],\n",
      "        [0.7682, 0.2483],\n",
      "        ...,\n",
      "        [0.9631, 0.0365],\n",
      "        [0.9796, 0.0491],\n",
      "        [0.9909, 0.0391]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and eval of the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10000], Train Loss: 0.21527442336082458, Test Loss: 0.19278553128242493\n",
      "Epoch [20/10000], Train Loss: 0.09851495921611786, Test Loss: 0.037939514964818954\n",
      "Epoch [30/10000], Train Loss: 0.09136723726987839, Test Loss: 0.0349259115755558\n",
      "Epoch [40/10000], Train Loss: 0.0800982415676117, Test Loss: 0.046725936233997345\n",
      "Epoch [50/10000], Train Loss: 0.07053242623806, Test Loss: 0.025826431810855865\n",
      "Epoch [60/10000], Train Loss: 0.06253427267074585, Test Loss: 0.029848847538232803\n",
      "Epoch [70/10000], Train Loss: 0.059218183159828186, Test Loss: 0.023065762594342232\n",
      "Epoch [80/10000], Train Loss: 0.054506778717041016, Test Loss: 0.023186003789305687\n",
      "Epoch [90/10000], Train Loss: 0.05192098021507263, Test Loss: 0.02065351977944374\n",
      "Epoch [100/10000], Train Loss: 0.04812939465045929, Test Loss: 0.018981533125042915\n",
      "Epoch [110/10000], Train Loss: 0.046312954276800156, Test Loss: 0.01834794320166111\n",
      "Epoch [120/10000], Train Loss: 0.044734638184309006, Test Loss: 0.017525790259242058\n",
      "Epoch [130/10000], Train Loss: 0.04270176589488983, Test Loss: 0.016572922468185425\n",
      "Epoch [140/10000], Train Loss: 0.0415782555937767, Test Loss: 0.015336242504417896\n",
      "Epoch [150/10000], Train Loss: 0.040492355823516846, Test Loss: 0.015014978125691414\n",
      "Epoch [160/10000], Train Loss: 0.03906698152422905, Test Loss: 0.014426535926759243\n",
      "Epoch [170/10000], Train Loss: 0.037868041545152664, Test Loss: 0.013076324947178364\n",
      "Epoch [180/10000], Train Loss: 0.037515632808208466, Test Loss: 0.01280954945832491\n",
      "Epoch [190/10000], Train Loss: 0.03574158623814583, Test Loss: 0.012197060510516167\n",
      "Epoch [200/10000], Train Loss: 0.0343414805829525, Test Loss: 0.012063208967447281\n",
      "Epoch [210/10000], Train Loss: 0.03453657031059265, Test Loss: 0.012263350188732147\n",
      "Epoch [220/10000], Train Loss: 0.033602524548769, Test Loss: 0.012176766991615295\n",
      "Epoch [230/10000], Train Loss: 0.03266357630491257, Test Loss: 0.012203862890601158\n",
      "Epoch [240/10000], Train Loss: 0.03214603662490845, Test Loss: 0.011255421675741673\n",
      "Epoch [250/10000], Train Loss: 0.0315294973552227, Test Loss: 0.010936417616903782\n",
      "Epoch [260/10000], Train Loss: 0.031025400385260582, Test Loss: 0.01113218441605568\n",
      "Epoch [270/10000], Train Loss: 0.029934434220194817, Test Loss: 0.01073736883699894\n",
      "Epoch [280/10000], Train Loss: 0.029787983745336533, Test Loss: 0.010500604286789894\n",
      "Epoch [290/10000], Train Loss: 0.02900262363255024, Test Loss: 0.00994720496237278\n",
      "Epoch [300/10000], Train Loss: 0.0283916387706995, Test Loss: 0.00905273575335741\n",
      "Epoch [310/10000], Train Loss: 0.027703410014510155, Test Loss: 0.008403517305850983\n",
      "Epoch [320/10000], Train Loss: 0.02704082801938057, Test Loss: 0.008032271638512611\n",
      "Epoch [330/10000], Train Loss: 0.02641373686492443, Test Loss: 0.007946434430778027\n",
      "Epoch [340/10000], Train Loss: 0.02627740427851677, Test Loss: 0.007578217424452305\n",
      "Epoch [350/10000], Train Loss: 0.025118058547377586, Test Loss: 0.007123314775526524\n",
      "Epoch [360/10000], Train Loss: 0.025072980672121048, Test Loss: 0.007137400098145008\n",
      "Epoch [370/10000], Train Loss: 0.024254539981484413, Test Loss: 0.0070581985637545586\n",
      "Epoch [380/10000], Train Loss: 0.024353288114070892, Test Loss: 0.006570592522621155\n",
      "Epoch [390/10000], Train Loss: 0.02364594303071499, Test Loss: 0.006294263992458582\n",
      "Epoch [400/10000], Train Loss: 0.02350759506225586, Test Loss: 0.006064516492187977\n",
      "Epoch [410/10000], Train Loss: 0.02244481071829796, Test Loss: 0.00628660386428237\n",
      "Epoch [420/10000], Train Loss: 0.0219108946621418, Test Loss: 0.005894996225833893\n",
      "Epoch [430/10000], Train Loss: 0.02209874987602234, Test Loss: 0.005865240935236216\n",
      "Epoch [440/10000], Train Loss: 0.021256892010569572, Test Loss: 0.005545997992157936\n",
      "Epoch [450/10000], Train Loss: 0.020845772698521614, Test Loss: 0.00526808388531208\n",
      "Epoch [460/10000], Train Loss: 0.02074890211224556, Test Loss: 0.005326337646692991\n",
      "Epoch [470/10000], Train Loss: 0.019856110215187073, Test Loss: 0.005209966562688351\n",
      "Epoch [480/10000], Train Loss: 0.019434385001659393, Test Loss: 0.00486993370577693\n",
      "Epoch [490/10000], Train Loss: 0.019620612263679504, Test Loss: 0.004705264698714018\n",
      "Epoch [500/10000], Train Loss: 0.01902022585272789, Test Loss: 0.004529139026999474\n",
      "Epoch [510/10000], Train Loss: 0.018659505993127823, Test Loss: 0.004302808083593845\n",
      "Epoch [520/10000], Train Loss: 0.018206140026450157, Test Loss: 0.004444694146513939\n",
      "Epoch [530/10000], Train Loss: 0.017916377633810043, Test Loss: 0.00443239975720644\n",
      "Epoch [540/10000], Train Loss: 0.017849666997790337, Test Loss: 0.004390075337141752\n",
      "Epoch [550/10000], Train Loss: 0.017270894721150398, Test Loss: 0.004090955015271902\n",
      "Epoch [560/10000], Train Loss: 0.01761087216436863, Test Loss: 0.004034231882542372\n",
      "Epoch [570/10000], Train Loss: 0.01724541373550892, Test Loss: 0.004058875143527985\n",
      "Epoch [580/10000], Train Loss: 0.016436206176877022, Test Loss: 0.003909009508788586\n",
      "Epoch [590/10000], Train Loss: 0.01697722263634205, Test Loss: 0.0036502897273749113\n",
      "Epoch [600/10000], Train Loss: 0.016554417088627815, Test Loss: 0.003694363636896014\n",
      "Epoch [610/10000], Train Loss: 0.01625571772456169, Test Loss: 0.003993604332208633\n",
      "Epoch [620/10000], Train Loss: 0.016313282772898674, Test Loss: 0.003745701629668474\n",
      "Epoch [630/10000], Train Loss: 0.01612696424126625, Test Loss: 0.003729119896888733\n",
      "Epoch [640/10000], Train Loss: 0.016076764091849327, Test Loss: 0.0038041011430323124\n",
      "Epoch [650/10000], Train Loss: 0.015906130895018578, Test Loss: 0.003738154424354434\n",
      "Epoch [660/10000], Train Loss: 0.015919163823127747, Test Loss: 0.003649568185210228\n",
      "Epoch [670/10000], Train Loss: 0.015199706889688969, Test Loss: 0.0036384579725563526\n",
      "Epoch [680/10000], Train Loss: 0.015588694252073765, Test Loss: 0.0035666553303599358\n",
      "Epoch [690/10000], Train Loss: 0.01568625122308731, Test Loss: 0.003392120124772191\n",
      "Epoch [700/10000], Train Loss: 0.014349810779094696, Test Loss: 0.003382932161912322\n",
      "Epoch [710/10000], Train Loss: 0.014866700395941734, Test Loss: 0.0034084171056747437\n",
      "Epoch [720/10000], Train Loss: 0.014920351095497608, Test Loss: 0.0033456359524279833\n",
      "Epoch [730/10000], Train Loss: 0.014478692784905434, Test Loss: 0.0033830441534519196\n",
      "Epoch [740/10000], Train Loss: 0.014551364816725254, Test Loss: 0.0033210187684744596\n",
      "Epoch [750/10000], Train Loss: 0.014190109446644783, Test Loss: 0.003259574994444847\n",
      "Epoch [760/10000], Train Loss: 0.013995683752000332, Test Loss: 0.0032419764902442694\n",
      "Epoch [770/10000], Train Loss: 0.013887094333767891, Test Loss: 0.0031090767588466406\n",
      "Epoch [780/10000], Train Loss: 0.014024640433490276, Test Loss: 0.0030930095817893744\n",
      "Epoch [790/10000], Train Loss: 0.013635612092912197, Test Loss: 0.003068801946938038\n",
      "Epoch [800/10000], Train Loss: 0.013659386895596981, Test Loss: 0.003035506932064891\n",
      "Epoch [810/10000], Train Loss: 0.013373501598834991, Test Loss: 0.0029124170541763306\n",
      "Epoch [820/10000], Train Loss: 0.01339408103376627, Test Loss: 0.00282992678694427\n",
      "Epoch [830/10000], Train Loss: 0.01294616423547268, Test Loss: 0.002986093983054161\n",
      "Epoch [840/10000], Train Loss: 0.013190195895731449, Test Loss: 0.002758161863312125\n",
      "Epoch [850/10000], Train Loss: 0.013160963542759418, Test Loss: 0.0027329339645802975\n",
      "Epoch [860/10000], Train Loss: 0.012613234110176563, Test Loss: 0.0026860234793275595\n",
      "Epoch [870/10000], Train Loss: 0.012457813136279583, Test Loss: 0.0026782979257404804\n",
      "Epoch [880/10000], Train Loss: 0.012563783675432205, Test Loss: 0.0027209832333028316\n",
      "Epoch [890/10000], Train Loss: 0.012038931250572205, Test Loss: 0.002578161656856537\n",
      "Epoch [900/10000], Train Loss: 0.012312973849475384, Test Loss: 0.002595257479697466\n",
      "Epoch [910/10000], Train Loss: 0.011944026686251163, Test Loss: 0.0025625198613852262\n",
      "Epoch [920/10000], Train Loss: 0.012074865400791168, Test Loss: 0.0025965122040361166\n",
      "Epoch [930/10000], Train Loss: 0.01202001515775919, Test Loss: 0.002479763235896826\n",
      "Epoch [940/10000], Train Loss: 0.011432940140366554, Test Loss: 0.0023196886759251356\n",
      "Epoch [950/10000], Train Loss: 0.011512734927237034, Test Loss: 0.002332196803763509\n",
      "Epoch [960/10000], Train Loss: 0.011391092091798782, Test Loss: 0.0024092437233775854\n",
      "Epoch [970/10000], Train Loss: 0.011252231895923615, Test Loss: 0.0022979790810495615\n",
      "Epoch [980/10000], Train Loss: 0.010863708332180977, Test Loss: 0.0022409670054912567\n",
      "Epoch [990/10000], Train Loss: 0.010596198961138725, Test Loss: 0.0022910828702151775\n",
      "Epoch [1000/10000], Train Loss: 0.010807731188833714, Test Loss: 0.0021680444478988647\n",
      "Epoch [1010/10000], Train Loss: 0.011084167286753654, Test Loss: 0.0022400193847715855\n",
      "Epoch [1020/10000], Train Loss: 0.010353528894484043, Test Loss: 0.002239626832306385\n",
      "Epoch [1030/10000], Train Loss: 0.010547904297709465, Test Loss: 0.002092613372951746\n",
      "Epoch [1040/10000], Train Loss: 0.010267148725688457, Test Loss: 0.0020697256550192833\n",
      "Epoch [1050/10000], Train Loss: 0.010208037681877613, Test Loss: 0.0021738670766353607\n",
      "Epoch [1060/10000], Train Loss: 0.010118294507265091, Test Loss: 0.0021435534581542015\n",
      "Epoch [1070/10000], Train Loss: 0.009898663498461246, Test Loss: 0.002041865838691592\n",
      "Epoch [1080/10000], Train Loss: 0.009515900164842606, Test Loss: 0.0020607421174645424\n",
      "Epoch [1090/10000], Train Loss: 0.009832010604441166, Test Loss: 0.0021539069712162018\n",
      "Epoch [1100/10000], Train Loss: 0.00944808404892683, Test Loss: 0.002146560698747635\n",
      "Epoch [1110/10000], Train Loss: 0.009342779405415058, Test Loss: 0.0019990999717265368\n",
      "Epoch [1120/10000], Train Loss: 0.00921460147947073, Test Loss: 0.0020149729680269957\n",
      "Epoch [1130/10000], Train Loss: 0.009256252087652683, Test Loss: 0.002029085299000144\n",
      "Epoch [1140/10000], Train Loss: 0.009092627093195915, Test Loss: 0.0020261735189706087\n",
      "Epoch [1150/10000], Train Loss: 0.009236902929842472, Test Loss: 0.001962540904060006\n",
      "Epoch [1160/10000], Train Loss: 0.009244977496564388, Test Loss: 0.001956227235496044\n",
      "Epoch [1170/10000], Train Loss: 0.008906230330467224, Test Loss: 0.001896438654512167\n",
      "Epoch [1180/10000], Train Loss: 0.008879950270056725, Test Loss: 0.0018998159794136882\n",
      "Epoch [1190/10000], Train Loss: 0.00888774637132883, Test Loss: 0.0018529464723542333\n",
      "Epoch [1200/10000], Train Loss: 0.00857909582555294, Test Loss: 0.0018563930643722415\n",
      "Epoch [1210/10000], Train Loss: 0.008505060337483883, Test Loss: 0.0017838897183537483\n",
      "Epoch [1220/10000], Train Loss: 0.008571868762373924, Test Loss: 0.001882167300209403\n",
      "Epoch [1230/10000], Train Loss: 0.008452435955405235, Test Loss: 0.0019040850456804037\n",
      "Epoch [1240/10000], Train Loss: 0.008287176489830017, Test Loss: 0.00189942866563797\n",
      "Epoch [1250/10000], Train Loss: 0.008179415948688984, Test Loss: 0.0018711776938289404\n",
      "Epoch [1260/10000], Train Loss: 0.008065145462751389, Test Loss: 0.0018069796497002244\n",
      "Epoch [1270/10000], Train Loss: 0.007972360588610172, Test Loss: 0.0018014718079939485\n",
      "Epoch [1280/10000], Train Loss: 0.007987470366060734, Test Loss: 0.0017819073982536793\n",
      "Epoch [1290/10000], Train Loss: 0.007858817465603352, Test Loss: 0.0018117291620001197\n",
      "Epoch [1300/10000], Train Loss: 0.007819543592631817, Test Loss: 0.0019196370849385858\n",
      "Epoch [1310/10000], Train Loss: 0.007677757181227207, Test Loss: 0.0018331599421799183\n",
      "Epoch [1320/10000], Train Loss: 0.007639868184924126, Test Loss: 0.001832412788644433\n",
      "Epoch [1330/10000], Train Loss: 0.007446111645549536, Test Loss: 0.0018218582263216376\n",
      "Epoch [1340/10000], Train Loss: 0.007637460250407457, Test Loss: 0.0018491041846573353\n",
      "Epoch [1350/10000], Train Loss: 0.007577480282634497, Test Loss: 0.0017655357951298356\n",
      "Epoch [1360/10000], Train Loss: 0.007160244509577751, Test Loss: 0.0017824891256168485\n",
      "Epoch [1370/10000], Train Loss: 0.007290033623576164, Test Loss: 0.0018472970696166158\n",
      "Epoch [1380/10000], Train Loss: 0.007106663193553686, Test Loss: 0.0017891523893922567\n",
      "Epoch [1390/10000], Train Loss: 0.007040963042527437, Test Loss: 0.0016911927377805114\n",
      "Epoch [1400/10000], Train Loss: 0.007196336053311825, Test Loss: 0.001757302088662982\n",
      "Epoch [1410/10000], Train Loss: 0.006821752525866032, Test Loss: 0.0016937055625021458\n",
      "Epoch [1420/10000], Train Loss: 0.00699303112924099, Test Loss: 0.0017139546107500792\n",
      "Epoch [1430/10000], Train Loss: 0.006851173937320709, Test Loss: 0.0017044018022716045\n",
      "Epoch [1440/10000], Train Loss: 0.006728171370923519, Test Loss: 0.0017794989980757236\n",
      "Epoch [1450/10000], Train Loss: 0.006660901475697756, Test Loss: 0.0016893398715183139\n",
      "Epoch [1460/10000], Train Loss: 0.006637249607592821, Test Loss: 0.0017256895080208778\n",
      "Epoch [1470/10000], Train Loss: 0.0066679129377007484, Test Loss: 0.0016879125032573938\n",
      "Epoch [1480/10000], Train Loss: 0.006578122265636921, Test Loss: 0.0016999721992760897\n",
      "Epoch [1490/10000], Train Loss: 0.006525172386318445, Test Loss: 0.0016447374364361167\n",
      "Epoch [1500/10000], Train Loss: 0.006512676831334829, Test Loss: 0.0016903846990317106\n",
      "Epoch [1510/10000], Train Loss: 0.006266280077397823, Test Loss: 0.001694888575002551\n",
      "Epoch [1520/10000], Train Loss: 0.006249588448554277, Test Loss: 0.00171181233599782\n",
      "Epoch [1530/10000], Train Loss: 0.006362152751535177, Test Loss: 0.001613784465007484\n",
      "Epoch [1540/10000], Train Loss: 0.006103427615016699, Test Loss: 0.001653454964980483\n",
      "Epoch [1550/10000], Train Loss: 0.006257046014070511, Test Loss: 0.0015991394175216556\n",
      "Epoch [1560/10000], Train Loss: 0.006116089411079884, Test Loss: 0.0017117615789175034\n",
      "Epoch [1570/10000], Train Loss: 0.006073012948036194, Test Loss: 0.0016778187127783895\n",
      "Epoch [1580/10000], Train Loss: 0.005955626722425222, Test Loss: 0.0016028695972636342\n",
      "Epoch [1590/10000], Train Loss: 0.005934702232480049, Test Loss: 0.0016533582238480449\n",
      "Epoch [1600/10000], Train Loss: 0.006043497938662767, Test Loss: 0.001667686621658504\n",
      "Epoch [1610/10000], Train Loss: 0.005780769046396017, Test Loss: 0.0016297174151986837\n",
      "Epoch [1620/10000], Train Loss: 0.0058466061018407345, Test Loss: 0.001679238397628069\n",
      "Epoch [1630/10000], Train Loss: 0.005716575775295496, Test Loss: 0.001703879446722567\n",
      "Epoch [1640/10000], Train Loss: 0.005775085650384426, Test Loss: 0.0017061567632481456\n",
      "Epoch [1650/10000], Train Loss: 0.005633933935314417, Test Loss: 0.0016348763601854444\n",
      "Epoch [1660/10000], Train Loss: 0.005676831118762493, Test Loss: 0.001601740368641913\n",
      "Epoch [1670/10000], Train Loss: 0.005549839697778225, Test Loss: 0.001645470503717661\n",
      "Epoch [1680/10000], Train Loss: 0.005520856473594904, Test Loss: 0.0017250576056540012\n",
      "Epoch [1690/10000], Train Loss: 0.005378413014113903, Test Loss: 0.0016297852853313088\n",
      "Epoch [1700/10000], Train Loss: 0.005490119103342295, Test Loss: 0.0016241258708760142\n",
      "Epoch [1710/10000], Train Loss: 0.005528898444026709, Test Loss: 0.0016529234126210213\n",
      "Epoch [1720/10000], Train Loss: 0.005404608324170113, Test Loss: 0.0015788455493748188\n",
      "Epoch [1730/10000], Train Loss: 0.005353291518986225, Test Loss: 0.0016324658645316958\n",
      "Epoch [1740/10000], Train Loss: 0.005256721284240484, Test Loss: 0.0016894001746550202\n",
      "Epoch [1750/10000], Train Loss: 0.005273008719086647, Test Loss: 0.0016443440690636635\n",
      "Epoch [1760/10000], Train Loss: 0.005269668065011501, Test Loss: 0.0016268312465399504\n",
      "Epoch [1770/10000], Train Loss: 0.005048025399446487, Test Loss: 0.0015956845600157976\n",
      "Epoch [1780/10000], Train Loss: 0.005145985633134842, Test Loss: 0.0015624300576746464\n",
      "Epoch [1790/10000], Train Loss: 0.0052481526508927345, Test Loss: 0.0015826275339350104\n",
      "Epoch [1800/10000], Train Loss: 0.005067744757980108, Test Loss: 0.001562577555887401\n",
      "Epoch [1810/10000], Train Loss: 0.0050562298856675625, Test Loss: 0.0015630719717592\n",
      "Epoch [1820/10000], Train Loss: 0.005047037731856108, Test Loss: 0.0016973965102806687\n",
      "Epoch [1830/10000], Train Loss: 0.005011356435716152, Test Loss: 0.001636937609873712\n",
      "Epoch [1840/10000], Train Loss: 0.004772883839905262, Test Loss: 0.0015646327519789338\n",
      "Epoch [1850/10000], Train Loss: 0.004888332914561033, Test Loss: 0.0015680284705013037\n",
      "Epoch [1860/10000], Train Loss: 0.0049429419450461864, Test Loss: 0.0015899927821010351\n",
      "Epoch [1870/10000], Train Loss: 0.0047740451991558075, Test Loss: 0.0016105460235849023\n",
      "Epoch [1880/10000], Train Loss: 0.0046166530810296535, Test Loss: 0.0015587175730615854\n",
      "Epoch [1890/10000], Train Loss: 0.0047093783505260944, Test Loss: 0.0015950812958180904\n",
      "Epoch [1900/10000], Train Loss: 0.004761602729558945, Test Loss: 0.0015682348748669028\n",
      "Epoch [1910/10000], Train Loss: 0.004722400568425655, Test Loss: 0.0015869830967858434\n",
      "Epoch [1920/10000], Train Loss: 0.004749443847686052, Test Loss: 0.0015896460972726345\n",
      "Epoch [1930/10000], Train Loss: 0.0047039310447871685, Test Loss: 0.0016065089730545878\n",
      "Epoch [1940/10000], Train Loss: 0.004575923550873995, Test Loss: 0.0015887906774878502\n",
      "Epoch [1950/10000], Train Loss: 0.00457770423963666, Test Loss: 0.0015796239022165537\n",
      "Epoch [1960/10000], Train Loss: 0.004534211941063404, Test Loss: 0.0014553895452991128\n",
      "Epoch [1970/10000], Train Loss: 0.004662093706429005, Test Loss: 0.0015677163610234857\n",
      "Epoch [1980/10000], Train Loss: 0.0046053724363446236, Test Loss: 0.0015218268381431699\n",
      "Epoch [1990/10000], Train Loss: 0.004584120120853186, Test Loss: 0.0015580488834530115\n",
      "Epoch [2000/10000], Train Loss: 0.00463546859100461, Test Loss: 0.0015231685247272253\n",
      "Epoch [2010/10000], Train Loss: 0.004488278646022081, Test Loss: 0.0015679884236305952\n",
      "Epoch [2020/10000], Train Loss: 0.004476096015423536, Test Loss: 0.0015568877570331097\n",
      "Epoch [2030/10000], Train Loss: 0.004446354229003191, Test Loss: 0.0015384546713903546\n",
      "Epoch [2040/10000], Train Loss: 0.00442446768283844, Test Loss: 0.001583591802045703\n",
      "Epoch [2050/10000], Train Loss: 0.004327166359871626, Test Loss: 0.001521004713140428\n",
      "Epoch [2060/10000], Train Loss: 0.004349646158516407, Test Loss: 0.001519627752713859\n",
      "Epoch [2070/10000], Train Loss: 0.004260220564901829, Test Loss: 0.0015994777204468846\n",
      "Epoch [2080/10000], Train Loss: 0.004383501131087542, Test Loss: 0.0015927893109619617\n",
      "Epoch [2090/10000], Train Loss: 0.004337640479207039, Test Loss: 0.0015067277709022164\n",
      "Epoch [2100/10000], Train Loss: 0.004174322355538607, Test Loss: 0.0015868812333792448\n",
      "Epoch [2110/10000], Train Loss: 0.004167423117905855, Test Loss: 0.0015261248918250203\n",
      "Epoch [2120/10000], Train Loss: 0.004232408478856087, Test Loss: 0.00155095593072474\n",
      "Epoch [2130/10000], Train Loss: 0.004205580800771713, Test Loss: 0.0014719533501192927\n",
      "Epoch [2140/10000], Train Loss: 0.004176376387476921, Test Loss: 0.0014772315043956041\n",
      "Epoch [2150/10000], Train Loss: 0.004163241013884544, Test Loss: 0.0015739762457087636\n",
      "Epoch [2160/10000], Train Loss: 0.004203472286462784, Test Loss: 0.0015135676367208362\n",
      "Epoch [2170/10000], Train Loss: 0.004063375759869814, Test Loss: 0.0015947597566992044\n",
      "Epoch [2180/10000], Train Loss: 0.004186664242297411, Test Loss: 0.0016510792775079608\n",
      "Epoch [2190/10000], Train Loss: 0.0041313511319458485, Test Loss: 0.0015499602304771543\n",
      "Epoch [2200/10000], Train Loss: 0.004165225196629763, Test Loss: 0.0016111787408590317\n",
      "Epoch [2210/10000], Train Loss: 0.003969515208154917, Test Loss: 0.0015463235322386026\n",
      "Epoch [2220/10000], Train Loss: 0.004181334748864174, Test Loss: 0.001510498346760869\n",
      "Epoch [2230/10000], Train Loss: 0.003955317195504904, Test Loss: 0.0015264436369761825\n",
      "Epoch [2240/10000], Train Loss: 0.004158920608460903, Test Loss: 0.0015534830745309591\n",
      "Epoch [2250/10000], Train Loss: 0.003994021564722061, Test Loss: 0.0014785797102376819\n",
      "Epoch [2260/10000], Train Loss: 0.004023994319140911, Test Loss: 0.001554564805701375\n",
      "Epoch [2270/10000], Train Loss: 0.003880144329741597, Test Loss: 0.001583786215633154\n",
      "Epoch [2280/10000], Train Loss: 0.003998809959739447, Test Loss: 0.001565012615174055\n",
      "Epoch [2290/10000], Train Loss: 0.003981501795351505, Test Loss: 0.0015183135401457548\n",
      "Epoch [2300/10000], Train Loss: 0.0040051378309726715, Test Loss: 0.001545438775792718\n",
      "Epoch [2310/10000], Train Loss: 0.0038967947475612164, Test Loss: 0.0014979357365518808\n",
      "Epoch [2320/10000], Train Loss: 0.003923799842596054, Test Loss: 0.001561070908792317\n",
      "Epoch [2330/10000], Train Loss: 0.0038062124513089657, Test Loss: 0.0015266372356563807\n",
      "Epoch [2340/10000], Train Loss: 0.003973992075771093, Test Loss: 0.0015120727475732565\n",
      "Epoch [2350/10000], Train Loss: 0.004002894274890423, Test Loss: 0.0015960173914209008\n",
      "Epoch [2360/10000], Train Loss: 0.0039054956287145615, Test Loss: 0.0015575130237266421\n",
      "Epoch [2370/10000], Train Loss: 0.003895810339599848, Test Loss: 0.0015447605401277542\n",
      "Epoch [2380/10000], Train Loss: 0.0039061421994119883, Test Loss: 0.0015567522495985031\n",
      "Epoch [2390/10000], Train Loss: 0.0038428197149187326, Test Loss: 0.001526690204627812\n",
      "Epoch [2400/10000], Train Loss: 0.0037654892075806856, Test Loss: 0.001637968234717846\n",
      "Epoch [2410/10000], Train Loss: 0.0038408401887863874, Test Loss: 0.001551163848489523\n",
      "Epoch [2420/10000], Train Loss: 0.0038311625830829144, Test Loss: 0.0014576871180906892\n",
      "Epoch [2430/10000], Train Loss: 0.0037033362314105034, Test Loss: 0.0015628786059096456\n",
      "Epoch [2440/10000], Train Loss: 0.0038932058960199356, Test Loss: 0.0015287577407434583\n",
      "Epoch [2450/10000], Train Loss: 0.0037321909330785275, Test Loss: 0.0015322626568377018\n",
      "Epoch [2460/10000], Train Loss: 0.0037939678877592087, Test Loss: 0.001451431424356997\n",
      "Epoch [2470/10000], Train Loss: 0.003790132235735655, Test Loss: 0.0015437006950378418\n",
      "Epoch [2480/10000], Train Loss: 0.0038618668913841248, Test Loss: 0.0015158249298110604\n",
      "Epoch [2490/10000], Train Loss: 0.0037271545734256506, Test Loss: 0.0015091976383700967\n",
      "Epoch [2500/10000], Train Loss: 0.003847546875476837, Test Loss: 0.0014853229513391852\n",
      "Epoch [2510/10000], Train Loss: 0.0037551650311797857, Test Loss: 0.0015233533922582865\n",
      "Epoch [2520/10000], Train Loss: 0.003948063123971224, Test Loss: 0.0015215170569717884\n",
      "Epoch [2530/10000], Train Loss: 0.003711374243721366, Test Loss: 0.001528980559669435\n",
      "Epoch [2540/10000], Train Loss: 0.003793916665017605, Test Loss: 0.0016068652039393783\n",
      "Epoch [2550/10000], Train Loss: 0.0037602484226226807, Test Loss: 0.0015239381464198232\n",
      "Epoch [2560/10000], Train Loss: 0.0038705491460859776, Test Loss: 0.0014971691416576505\n",
      "Epoch [2570/10000], Train Loss: 0.0036773825995624065, Test Loss: 0.0014165593311190605\n",
      "Epoch [2580/10000], Train Loss: 0.003726707072928548, Test Loss: 0.001546051469631493\n",
      "Epoch [2590/10000], Train Loss: 0.003775482065975666, Test Loss: 0.0016026857774704695\n",
      "Epoch [2600/10000], Train Loss: 0.0037361790891736746, Test Loss: 0.0015202738577499986\n",
      "Epoch [2610/10000], Train Loss: 0.0037781200371682644, Test Loss: 0.0015526121715083718\n",
      "Epoch [2620/10000], Train Loss: 0.0036415578797459602, Test Loss: 0.0015250849537551403\n",
      "Epoch [2630/10000], Train Loss: 0.003746794071048498, Test Loss: 0.0015531467506662011\n",
      "Epoch [2640/10000], Train Loss: 0.0035000203642994165, Test Loss: 0.001547469524666667\n",
      "Epoch [2650/10000], Train Loss: 0.0038522384129464626, Test Loss: 0.0015152485575526953\n",
      "Epoch [2660/10000], Train Loss: 0.003779185237362981, Test Loss: 0.001488592242822051\n",
      "Epoch [2670/10000], Train Loss: 0.0038048340938985348, Test Loss: 0.0014252790715545416\n",
      "Epoch [2680/10000], Train Loss: 0.0037584081292152405, Test Loss: 0.0016072802245616913\n",
      "Epoch [2690/10000], Train Loss: 0.00380357401445508, Test Loss: 0.0015401900745928288\n",
      "Epoch [2700/10000], Train Loss: 0.003700506640598178, Test Loss: 0.0015209643170237541\n",
      "Epoch [2710/10000], Train Loss: 0.0036088842898607254, Test Loss: 0.0015743441181257367\n",
      "Epoch [2720/10000], Train Loss: 0.003650128375738859, Test Loss: 0.0014599584974348545\n",
      "Epoch [2730/10000], Train Loss: 0.0036614807322621346, Test Loss: 0.0015319944359362125\n",
      "Epoch [2740/10000], Train Loss: 0.0036932744551450014, Test Loss: 0.0015743530821055174\n",
      "Epoch [2750/10000], Train Loss: 0.0038746660575270653, Test Loss: 0.001533134956844151\n",
      "Epoch [2760/10000], Train Loss: 0.0037497275043278933, Test Loss: 0.0015188218094408512\n",
      "Epoch [2770/10000], Train Loss: 0.0037622705567628145, Test Loss: 0.0015568998642265797\n",
      "Epoch [2780/10000], Train Loss: 0.0036075017414987087, Test Loss: 0.0014341099886223674\n",
      "Epoch [2790/10000], Train Loss: 0.003624532837420702, Test Loss: 0.0015141305048018694\n",
      "Epoch [2800/10000], Train Loss: 0.0035890627186745405, Test Loss: 0.0016362604219466448\n",
      "Epoch [2810/10000], Train Loss: 0.003526566084474325, Test Loss: 0.0014632574748247862\n",
      "Epoch [2820/10000], Train Loss: 0.003681863658130169, Test Loss: 0.0015353669878095388\n",
      "Epoch [2830/10000], Train Loss: 0.003643154166638851, Test Loss: 0.0014862940879538655\n",
      "Epoch [2840/10000], Train Loss: 0.003591059474274516, Test Loss: 0.0016845137579366565\n",
      "Epoch [2850/10000], Train Loss: 0.003518192796036601, Test Loss: 0.0014349778648465872\n",
      "Epoch [2860/10000], Train Loss: 0.003569771070033312, Test Loss: 0.0016405911883339286\n",
      "Epoch [2870/10000], Train Loss: 0.003539913333952427, Test Loss: 0.0015509460354223847\n",
      "Epoch [2880/10000], Train Loss: 0.003567993175238371, Test Loss: 0.0014408963033929467\n",
      "Epoch [2890/10000], Train Loss: 0.003780031343922019, Test Loss: 0.0014857182977721095\n",
      "Epoch [2900/10000], Train Loss: 0.0035576019436120987, Test Loss: 0.001549615291878581\n",
      "Epoch [2910/10000], Train Loss: 0.003647878998890519, Test Loss: 0.0014863170217722654\n",
      "Epoch [2920/10000], Train Loss: 0.003560913959518075, Test Loss: 0.0015009305207058787\n",
      "Epoch [2930/10000], Train Loss: 0.0035568890161812305, Test Loss: 0.0015857547987252474\n",
      "Epoch [2940/10000], Train Loss: 0.0034071397967636585, Test Loss: 0.0015637078322470188\n",
      "Epoch [2950/10000], Train Loss: 0.003440731903538108, Test Loss: 0.0015403599245473742\n",
      "Epoch [2960/10000], Train Loss: 0.0035740986932069063, Test Loss: 0.0014995362143963575\n",
      "Epoch [2970/10000], Train Loss: 0.003611939260736108, Test Loss: 0.0015318661462515593\n",
      "Epoch [2980/10000], Train Loss: 0.003609114559367299, Test Loss: 0.0014705383218824863\n",
      "Epoch [2990/10000], Train Loss: 0.003657139604911208, Test Loss: 0.0015206792159006\n",
      "Epoch [3000/10000], Train Loss: 0.0034531389828771353, Test Loss: 0.0015187032986432314\n",
      "Epoch [3010/10000], Train Loss: 0.003592526772990823, Test Loss: 0.00144485617056489\n",
      "Epoch [3020/10000], Train Loss: 0.0035167394671589136, Test Loss: 0.0014912716578692198\n",
      "Epoch [3030/10000], Train Loss: 0.003428918542340398, Test Loss: 0.0014230140950530767\n",
      "Epoch [3040/10000], Train Loss: 0.0035313258413225412, Test Loss: 0.0013994990149512887\n",
      "Epoch [3050/10000], Train Loss: 0.0034424099139869213, Test Loss: 0.0013329756911844015\n",
      "Epoch [3060/10000], Train Loss: 0.0034125763922929764, Test Loss: 0.001428402028977871\n",
      "Epoch [3070/10000], Train Loss: 0.003548837499693036, Test Loss: 0.0014777281321585178\n",
      "Epoch [3080/10000], Train Loss: 0.0034421232994645834, Test Loss: 0.0014628332573920488\n",
      "Epoch [3090/10000], Train Loss: 0.0034512104466557503, Test Loss: 0.0013800867600366473\n",
      "Epoch [3100/10000], Train Loss: 0.0037019657902419567, Test Loss: 0.00148764590267092\n",
      "Epoch [3110/10000], Train Loss: 0.003399364184588194, Test Loss: 0.0013327597407624125\n",
      "Epoch [3120/10000], Train Loss: 0.0034897122532129288, Test Loss: 0.0013667743187397718\n",
      "Epoch [3130/10000], Train Loss: 0.003536605043336749, Test Loss: 0.0013752770610153675\n",
      "Epoch [3140/10000], Train Loss: 0.0034586391411721706, Test Loss: 0.0013902966165915132\n",
      "Epoch [3150/10000], Train Loss: 0.0033965075854212046, Test Loss: 0.0014756418531760573\n",
      "Epoch [3160/10000], Train Loss: 0.0034209180157631636, Test Loss: 0.001370798796415329\n",
      "Epoch [3170/10000], Train Loss: 0.0033708526752889156, Test Loss: 0.0014175394317135215\n",
      "Epoch [3180/10000], Train Loss: 0.0034061972983181477, Test Loss: 0.0014272629050537944\n",
      "Epoch [3190/10000], Train Loss: 0.003403546055778861, Test Loss: 0.0013736417749896646\n",
      "Epoch [3200/10000], Train Loss: 0.0034927043598145247, Test Loss: 0.001360186841338873\n",
      "Epoch [3210/10000], Train Loss: 0.0032837954349815845, Test Loss: 0.0014293850399553776\n",
      "Epoch [3220/10000], Train Loss: 0.0032229176722466946, Test Loss: 0.0014393283054232597\n",
      "Epoch [3230/10000], Train Loss: 0.003417596686631441, Test Loss: 0.0014288530219346285\n",
      "Epoch [3240/10000], Train Loss: 0.0035091175232082605, Test Loss: 0.0013806773349642754\n",
      "Epoch [3250/10000], Train Loss: 0.0033647778909653425, Test Loss: 0.0013592155883088708\n",
      "Epoch [3260/10000], Train Loss: 0.0034330508206039667, Test Loss: 0.0014411465963348746\n",
      "Epoch [3270/10000], Train Loss: 0.003389565972611308, Test Loss: 0.0013853213749825954\n",
      "Epoch [3280/10000], Train Loss: 0.0032803392969071865, Test Loss: 0.0013541922671720386\n",
      "Epoch [3290/10000], Train Loss: 0.0033161218743771315, Test Loss: 0.0013710666680708528\n",
      "Epoch [3300/10000], Train Loss: 0.0033518888521939516, Test Loss: 0.0013950587017461658\n",
      "Epoch [3310/10000], Train Loss: 0.0034246447030454874, Test Loss: 0.001392430393025279\n",
      "Epoch [3320/10000], Train Loss: 0.003281355369836092, Test Loss: 0.0014260871103033423\n",
      "Epoch [3330/10000], Train Loss: 0.0034170332364737988, Test Loss: 0.00136462296359241\n",
      "Epoch [3340/10000], Train Loss: 0.003271670313552022, Test Loss: 0.0014165624743327498\n",
      "Epoch [3350/10000], Train Loss: 0.003343891818076372, Test Loss: 0.0014379956992343068\n",
      "Epoch [3360/10000], Train Loss: 0.003302218159660697, Test Loss: 0.0013974515022709966\n",
      "Epoch [3370/10000], Train Loss: 0.003321472555398941, Test Loss: 0.0014269176172092557\n",
      "Epoch [3380/10000], Train Loss: 0.0032852988224476576, Test Loss: 0.0013711261563003063\n",
      "Epoch [3390/10000], Train Loss: 0.0033409034367650747, Test Loss: 0.0014897859655320644\n",
      "Epoch [3400/10000], Train Loss: 0.0033938137348741293, Test Loss: 0.0014482754049822688\n",
      "Epoch [3410/10000], Train Loss: 0.00328996405005455, Test Loss: 0.0013021847698837519\n",
      "Epoch [3420/10000], Train Loss: 0.0032145159784704447, Test Loss: 0.0013787969946861267\n",
      "Epoch [3430/10000], Train Loss: 0.003320410382002592, Test Loss: 0.00146712944842875\n",
      "Epoch [3440/10000], Train Loss: 0.00329268048517406, Test Loss: 0.0014017493231222034\n",
      "Epoch [3450/10000], Train Loss: 0.0033514348324388266, Test Loss: 0.0013346961932256818\n",
      "Epoch [3460/10000], Train Loss: 0.003356681438162923, Test Loss: 0.0014415242476388812\n",
      "Epoch [3470/10000], Train Loss: 0.0034224765840917826, Test Loss: 0.0014382849913090467\n",
      "Epoch [3480/10000], Train Loss: 0.0032221891451627016, Test Loss: 0.001334517146460712\n",
      "Epoch [3490/10000], Train Loss: 0.003448151284828782, Test Loss: 0.0013548923889175057\n",
      "Epoch [3500/10000], Train Loss: 0.0034268097952008247, Test Loss: 0.0013850860996171832\n",
      "Epoch [3510/10000], Train Loss: 0.003221075050532818, Test Loss: 0.0014086677692830563\n",
      "Epoch [3520/10000], Train Loss: 0.0033853547647595406, Test Loss: 0.001396923209540546\n",
      "Epoch [3530/10000], Train Loss: 0.0033299594651907682, Test Loss: 0.001387301366776228\n",
      "Epoch [3540/10000], Train Loss: 0.003388569923117757, Test Loss: 0.0012779515236616135\n",
      "Epoch [3550/10000], Train Loss: 0.0032149064354598522, Test Loss: 0.0013128386344760656\n",
      "Epoch [3560/10000], Train Loss: 0.003222857369109988, Test Loss: 0.0014687670627608895\n",
      "Epoch [3570/10000], Train Loss: 0.003422263078391552, Test Loss: 0.001488949405029416\n",
      "Epoch [3580/10000], Train Loss: 0.003361888462677598, Test Loss: 0.0013776527484878898\n",
      "Epoch [3590/10000], Train Loss: 0.003279929282143712, Test Loss: 0.0013663897989317775\n",
      "Epoch [3600/10000], Train Loss: 0.0033308989368379116, Test Loss: 0.0013634932693094015\n",
      "Epoch [3610/10000], Train Loss: 0.0034553452860563993, Test Loss: 0.0014446888817474246\n",
      "Epoch [3620/10000], Train Loss: 0.003374309279024601, Test Loss: 0.0013711416395381093\n",
      "Epoch [3630/10000], Train Loss: 0.0033002744894474745, Test Loss: 0.0013561132363975048\n",
      "Epoch [3640/10000], Train Loss: 0.0035534552298486233, Test Loss: 0.0012654748279601336\n",
      "Epoch [3650/10000], Train Loss: 0.0032794270664453506, Test Loss: 0.001398747437633574\n",
      "Epoch [3660/10000], Train Loss: 0.003230675356462598, Test Loss: 0.0014129149494692683\n",
      "Epoch [3670/10000], Train Loss: 0.0033683618530631065, Test Loss: 0.0014022713294252753\n",
      "Epoch [3680/10000], Train Loss: 0.003275245428085327, Test Loss: 0.001290397602133453\n",
      "Epoch [3690/10000], Train Loss: 0.0033006363082677126, Test Loss: 0.001336396555416286\n",
      "Epoch [3700/10000], Train Loss: 0.003389176679775119, Test Loss: 0.0014417811762541533\n",
      "Epoch [3710/10000], Train Loss: 0.00331898033618927, Test Loss: 0.0013491628924384713\n",
      "Epoch [3720/10000], Train Loss: 0.0033520162105560303, Test Loss: 0.0014308975078165531\n",
      "Epoch [3730/10000], Train Loss: 0.0032423108350485563, Test Loss: 0.0013986089034006\n",
      "Epoch [3740/10000], Train Loss: 0.0033035585656762123, Test Loss: 0.0013690278865396976\n",
      "Epoch [3750/10000], Train Loss: 0.0031208971049636602, Test Loss: 0.0013299976708367467\n",
      "Epoch [3760/10000], Train Loss: 0.00322800618596375, Test Loss: 0.0012790074106305838\n",
      "Epoch [3770/10000], Train Loss: 0.0033414638601243496, Test Loss: 0.0013415174325928092\n",
      "Epoch [3780/10000], Train Loss: 0.003232206217944622, Test Loss: 0.00134019716642797\n",
      "Epoch [3790/10000], Train Loss: 0.003262175712734461, Test Loss: 0.0013387170620262623\n",
      "Epoch [3800/10000], Train Loss: 0.0033240162301808596, Test Loss: 0.0013973037712275982\n",
      "Epoch [3810/10000], Train Loss: 0.003308286890387535, Test Loss: 0.0012985829962417483\n",
      "Epoch [3820/10000], Train Loss: 0.0031489285174757242, Test Loss: 0.001381938811391592\n",
      "Epoch [3830/10000], Train Loss: 0.003282436402514577, Test Loss: 0.001292926026508212\n",
      "Epoch [3840/10000], Train Loss: 0.003207908710464835, Test Loss: 0.001346841687336564\n",
      "Epoch [3850/10000], Train Loss: 0.003314531873911619, Test Loss: 0.0013477213215082884\n",
      "Epoch [3860/10000], Train Loss: 0.0031055144499987364, Test Loss: 0.0013829191448166966\n",
      "Epoch [3870/10000], Train Loss: 0.0031635237392038107, Test Loss: 0.0013635637005791068\n",
      "Epoch [3880/10000], Train Loss: 0.003394705941900611, Test Loss: 0.0013285023160278797\n",
      "Epoch [3890/10000], Train Loss: 0.0033195402938872576, Test Loss: 0.001326184836216271\n",
      "Epoch [3900/10000], Train Loss: 0.003316170070320368, Test Loss: 0.0012935283593833447\n",
      "Epoch [3910/10000], Train Loss: 0.0032030176371335983, Test Loss: 0.001337811234407127\n",
      "Epoch [3920/10000], Train Loss: 0.003229658119380474, Test Loss: 0.0014141075080260634\n",
      "Epoch [3930/10000], Train Loss: 0.003306994680315256, Test Loss: 0.0013945316895842552\n",
      "Epoch [3940/10000], Train Loss: 0.0032429443672299385, Test Loss: 0.001242212951183319\n",
      "Epoch [3950/10000], Train Loss: 0.00345402117818594, Test Loss: 0.001314251683652401\n",
      "Epoch [3960/10000], Train Loss: 0.0033778927754610777, Test Loss: 0.0013414259301498532\n",
      "Epoch [3970/10000], Train Loss: 0.0033526234328746796, Test Loss: 0.0013777101412415504\n",
      "Epoch [3980/10000], Train Loss: 0.0032091308385133743, Test Loss: 0.0012725931592285633\n",
      "Epoch [3990/10000], Train Loss: 0.003381145652383566, Test Loss: 0.0013430914841592312\n",
      "Epoch [4000/10000], Train Loss: 0.0032690572552382946, Test Loss: 0.0013574239565059543\n",
      "Epoch [4010/10000], Train Loss: 0.003287482773885131, Test Loss: 0.0013476122403517365\n",
      "Epoch [4020/10000], Train Loss: 0.0031993750017136335, Test Loss: 0.0012457880657166243\n",
      "Epoch [4030/10000], Train Loss: 0.003391687525436282, Test Loss: 0.0013100606156513095\n",
      "Epoch [4040/10000], Train Loss: 0.003267725231125951, Test Loss: 0.001298585906624794\n",
      "Epoch [4050/10000], Train Loss: 0.003154875012114644, Test Loss: 0.0013588337460532784\n",
      "Epoch [4060/10000], Train Loss: 0.00329647958278656, Test Loss: 0.0013412813423201442\n",
      "Epoch [4070/10000], Train Loss: 0.0034141684882342815, Test Loss: 0.0012528497027233243\n",
      "Epoch [4080/10000], Train Loss: 0.003243647515773773, Test Loss: 0.0013036502059549093\n",
      "Epoch [4090/10000], Train Loss: 0.003276214702054858, Test Loss: 0.0013859608443453908\n",
      "Epoch [4100/10000], Train Loss: 0.0033216187730431557, Test Loss: 0.0012604310177266598\n",
      "Epoch [4110/10000], Train Loss: 0.0033514630049467087, Test Loss: 0.0013018802274018526\n",
      "Epoch [4120/10000], Train Loss: 0.0034964715596288443, Test Loss: 0.001266084611415863\n",
      "Epoch [4130/10000], Train Loss: 0.0031686543952673674, Test Loss: 0.001254480448551476\n",
      "Epoch [4140/10000], Train Loss: 0.003230432979762554, Test Loss: 0.0013471193378791213\n",
      "Epoch [4150/10000], Train Loss: 0.003220705548301339, Test Loss: 0.0012994796270504594\n",
      "Epoch [4160/10000], Train Loss: 0.003229875583201647, Test Loss: 0.0012432271614670753\n",
      "Epoch [4170/10000], Train Loss: 0.0032948539592325687, Test Loss: 0.001346756354905665\n",
      "Epoch [4180/10000], Train Loss: 0.003291818080469966, Test Loss: 0.001329202437773347\n",
      "Epoch [4190/10000], Train Loss: 0.0033283240627497435, Test Loss: 0.0013025037478655577\n",
      "Epoch [4200/10000], Train Loss: 0.0032531849574297667, Test Loss: 0.0013832543045282364\n",
      "Epoch [4210/10000], Train Loss: 0.0033406426664441824, Test Loss: 0.0013455557636916637\n",
      "Epoch [4220/10000], Train Loss: 0.003335249377414584, Test Loss: 0.0012491083471104503\n",
      "Epoch [4230/10000], Train Loss: 0.0032464538235217333, Test Loss: 0.0013085356913506985\n",
      "Epoch [4240/10000], Train Loss: 0.003286958672106266, Test Loss: 0.0014077936066314578\n",
      "Epoch [4250/10000], Train Loss: 0.0031991256400942802, Test Loss: 0.0012965005589649081\n",
      "Epoch [4260/10000], Train Loss: 0.00309828226454556, Test Loss: 0.0012426511384546757\n",
      "Epoch [4270/10000], Train Loss: 0.0032219327986240387, Test Loss: 0.0012629051925614476\n",
      "Epoch [4280/10000], Train Loss: 0.003126307623460889, Test Loss: 0.0012869562488049269\n",
      "Epoch [4290/10000], Train Loss: 0.003387939417734742, Test Loss: 0.0013744201278313994\n",
      "Epoch [4300/10000], Train Loss: 0.0032381028868258, Test Loss: 0.0012810747139155865\n",
      "Epoch [4310/10000], Train Loss: 0.0031204666011035442, Test Loss: 0.0012702395906671882\n",
      "Epoch [4320/10000], Train Loss: 0.003118657972663641, Test Loss: 0.001239095930941403\n",
      "Epoch [4330/10000], Train Loss: 0.0032342385966330767, Test Loss: 0.0012586896773427725\n",
      "Epoch [4340/10000], Train Loss: 0.003279351396486163, Test Loss: 0.0013035116717219353\n",
      "Epoch [4350/10000], Train Loss: 0.0032508699223399162, Test Loss: 0.001269736560061574\n",
      "Epoch [4360/10000], Train Loss: 0.003111511003226042, Test Loss: 0.001353488420136273\n",
      "Epoch [4370/10000], Train Loss: 0.0033634870778769255, Test Loss: 0.001289652893319726\n",
      "Epoch [4380/10000], Train Loss: 0.003312424523755908, Test Loss: 0.0012790955370292068\n",
      "Epoch [4390/10000], Train Loss: 0.00320920767262578, Test Loss: 0.0013141223462298512\n",
      "Epoch [4400/10000], Train Loss: 0.0032279344741255045, Test Loss: 0.0012796672526746988\n",
      "Epoch [4410/10000], Train Loss: 0.003325143363326788, Test Loss: 0.001282852957956493\n",
      "Epoch [4420/10000], Train Loss: 0.003456995589658618, Test Loss: 0.001365691889077425\n",
      "Epoch [4430/10000], Train Loss: 0.0032828531693667173, Test Loss: 0.0012381875421851873\n",
      "Epoch [4440/10000], Train Loss: 0.0031141771469265223, Test Loss: 0.0012711405288428068\n",
      "Epoch [4450/10000], Train Loss: 0.0033144489862024784, Test Loss: 0.0013269379269331694\n",
      "Epoch [4460/10000], Train Loss: 0.0032275202684104443, Test Loss: 0.001303468132391572\n",
      "Epoch [4470/10000], Train Loss: 0.003238917561247945, Test Loss: 0.001276519033126533\n",
      "Epoch [4480/10000], Train Loss: 0.0032188391778618097, Test Loss: 0.001332986867055297\n",
      "Epoch [4490/10000], Train Loss: 0.0032797621097415686, Test Loss: 0.001259603537619114\n",
      "Epoch [4500/10000], Train Loss: 0.0032560164108872414, Test Loss: 0.001216921373270452\n",
      "Epoch [4510/10000], Train Loss: 0.0031694627832621336, Test Loss: 0.0013028094545006752\n",
      "Epoch [4520/10000], Train Loss: 0.003086947835981846, Test Loss: 0.001270596287213266\n",
      "Epoch [4530/10000], Train Loss: 0.0032238473650068045, Test Loss: 0.001243065344169736\n",
      "Epoch [4540/10000], Train Loss: 0.0033085420727729797, Test Loss: 0.001417888212017715\n",
      "Epoch [4550/10000], Train Loss: 0.0032585742883384228, Test Loss: 0.0012581287883222103\n",
      "Epoch [4560/10000], Train Loss: 0.0031009134836494923, Test Loss: 0.0013335184194147587\n",
      "Epoch [4570/10000], Train Loss: 0.0032346041407436132, Test Loss: 0.0012098085135221481\n",
      "Epoch [4580/10000], Train Loss: 0.0031357062980532646, Test Loss: 0.0013344688341021538\n",
      "Epoch [4590/10000], Train Loss: 0.0033108640927821398, Test Loss: 0.0011857447680085897\n",
      "Epoch [4600/10000], Train Loss: 0.003213429357856512, Test Loss: 0.001250295084901154\n",
      "Epoch [4610/10000], Train Loss: 0.0032271884847432375, Test Loss: 0.0012639322085306048\n",
      "Epoch [4620/10000], Train Loss: 0.0032338020391762257, Test Loss: 0.0012590201804414392\n",
      "Epoch [4630/10000], Train Loss: 0.003198891179636121, Test Loss: 0.0012266142293810844\n",
      "Epoch [4640/10000], Train Loss: 0.0031704623252153397, Test Loss: 0.0012675462057814002\n",
      "Epoch [4650/10000], Train Loss: 0.0031775024253875017, Test Loss: 0.0013062411453574896\n",
      "Epoch [4660/10000], Train Loss: 0.0032209856435656548, Test Loss: 0.0012852249201387167\n",
      "Epoch [4670/10000], Train Loss: 0.003346891375258565, Test Loss: 0.0013358459109440446\n",
      "Epoch [4680/10000], Train Loss: 0.0032342015765607357, Test Loss: 0.0012397337704896927\n",
      "Epoch [4690/10000], Train Loss: 0.0032702262979000807, Test Loss: 0.0013301841681823134\n",
      "Epoch [4700/10000], Train Loss: 0.00319880829192698, Test Loss: 0.001214269665069878\n",
      "Epoch [4710/10000], Train Loss: 0.003102413145825267, Test Loss: 0.0012323958799242973\n",
      "Epoch [4720/10000], Train Loss: 0.0031408697832375765, Test Loss: 0.0012730001471936703\n",
      "Epoch [4730/10000], Train Loss: 0.003246558830142021, Test Loss: 0.0012800015974789858\n",
      "Epoch [4740/10000], Train Loss: 0.003204871201887727, Test Loss: 0.001217808690853417\n",
      "Epoch [4750/10000], Train Loss: 0.0032729359809309244, Test Loss: 0.0012859543785452843\n",
      "Epoch [4760/10000], Train Loss: 0.003238521283492446, Test Loss: 0.0013107071863487363\n",
      "Epoch [4770/10000], Train Loss: 0.0032062488608062267, Test Loss: 0.00123602373059839\n",
      "Epoch [4780/10000], Train Loss: 0.003201535437256098, Test Loss: 0.0012681531952694058\n",
      "Epoch [4790/10000], Train Loss: 0.003217891091480851, Test Loss: 0.001327713136561215\n",
      "Epoch [4800/10000], Train Loss: 0.003127064323052764, Test Loss: 0.0012211283901706338\n",
      "Epoch [4810/10000], Train Loss: 0.003247761633247137, Test Loss: 0.0012175492011010647\n",
      "Epoch [4820/10000], Train Loss: 0.0031973535660654306, Test Loss: 0.0012402686988934875\n",
      "Epoch [4830/10000], Train Loss: 0.0031815487891435623, Test Loss: 0.0012639866909012198\n",
      "Epoch [4840/10000], Train Loss: 0.0031857690773904324, Test Loss: 0.001218439545482397\n",
      "Epoch [4850/10000], Train Loss: 0.0031158572528511286, Test Loss: 0.0012541814940050244\n",
      "Epoch [4860/10000], Train Loss: 0.003140317741781473, Test Loss: 0.001235199742950499\n",
      "Epoch [4870/10000], Train Loss: 0.0029915093909949064, Test Loss: 0.0012267592828720808\n",
      "Epoch [4880/10000], Train Loss: 0.00312627712264657, Test Loss: 0.001165446825325489\n",
      "Epoch [4890/10000], Train Loss: 0.0031403815373778343, Test Loss: 0.0012252444867044687\n",
      "Epoch [4900/10000], Train Loss: 0.003196426434442401, Test Loss: 0.0012557667214423418\n",
      "Epoch [4910/10000], Train Loss: 0.0031860272865742445, Test Loss: 0.0012824212899431586\n",
      "Epoch [4920/10000], Train Loss: 0.0031507997773587704, Test Loss: 0.0012827800819650292\n",
      "Epoch [4930/10000], Train Loss: 0.003161313710734248, Test Loss: 0.0013335001422092319\n",
      "Epoch [4940/10000], Train Loss: 0.003167331451550126, Test Loss: 0.0011552765499800444\n",
      "Epoch [4950/10000], Train Loss: 0.0031941626220941544, Test Loss: 0.0013448505196720362\n",
      "Epoch [4960/10000], Train Loss: 0.003223142120987177, Test Loss: 0.0012805411824956536\n",
      "Epoch [4970/10000], Train Loss: 0.0030417400412261486, Test Loss: 0.001325962133705616\n",
      "Epoch [4980/10000], Train Loss: 0.003137754276394844, Test Loss: 0.0011781807988882065\n",
      "Epoch [4990/10000], Train Loss: 0.0030757689382880926, Test Loss: 0.0012858347035944462\n",
      "Epoch [5000/10000], Train Loss: 0.0031810940708965063, Test Loss: 0.0012526074424386024\n",
      "Epoch [5010/10000], Train Loss: 0.0030742355156689882, Test Loss: 0.0012241383083164692\n",
      "Epoch [5020/10000], Train Loss: 0.0031239623203873634, Test Loss: 0.0012979041785001755\n",
      "Epoch [5030/10000], Train Loss: 0.0031130602583289146, Test Loss: 0.0013167477445676923\n",
      "Epoch [5040/10000], Train Loss: 0.0030296477489173412, Test Loss: 0.0012506932253018022\n",
      "Epoch [5050/10000], Train Loss: 0.003113966202363372, Test Loss: 0.001261581084690988\n",
      "Epoch [5060/10000], Train Loss: 0.003189636627212167, Test Loss: 0.0011759080225601792\n",
      "Epoch [5070/10000], Train Loss: 0.0032349310349673033, Test Loss: 0.0012396452948451042\n",
      "Epoch [5080/10000], Train Loss: 0.0031936224550008774, Test Loss: 0.001272056601010263\n",
      "Epoch [5090/10000], Train Loss: 0.003064314369112253, Test Loss: 0.0012903445167466998\n",
      "Epoch [5100/10000], Train Loss: 0.003062001196667552, Test Loss: 0.001233565155416727\n",
      "Epoch [5110/10000], Train Loss: 0.003113915678113699, Test Loss: 0.0012998625170439482\n",
      "Epoch [5120/10000], Train Loss: 0.0030895567033439875, Test Loss: 0.001261447905562818\n",
      "Epoch [5130/10000], Train Loss: 0.0031300485134124756, Test Loss: 0.0012486297637224197\n",
      "Epoch [5140/10000], Train Loss: 0.0031127308029681444, Test Loss: 0.0012241554213687778\n",
      "Epoch [5150/10000], Train Loss: 0.0032161902636289597, Test Loss: 0.0012697114143520594\n",
      "Epoch [5160/10000], Train Loss: 0.003174860728904605, Test Loss: 0.0011521547567099333\n",
      "Epoch [5170/10000], Train Loss: 0.0032731005921959877, Test Loss: 0.0012727348366752267\n",
      "Epoch [5180/10000], Train Loss: 0.0032362989149987698, Test Loss: 0.0012021040311083198\n",
      "Epoch [5190/10000], Train Loss: 0.003171240910887718, Test Loss: 0.0012558253947645426\n",
      "Epoch [5200/10000], Train Loss: 0.003086727811023593, Test Loss: 0.0012187279062345624\n",
      "Epoch [5210/10000], Train Loss: 0.003203351516276598, Test Loss: 0.0012614966835826635\n",
      "Epoch [5220/10000], Train Loss: 0.003128881799057126, Test Loss: 0.0011529517360031605\n",
      "Epoch [5230/10000], Train Loss: 0.0031529676634818316, Test Loss: 0.0012365203583613038\n",
      "Epoch [5240/10000], Train Loss: 0.0031435720156878233, Test Loss: 0.0012068438809365034\n",
      "Epoch [5250/10000], Train Loss: 0.0030455023515969515, Test Loss: 0.0011831886367872357\n",
      "Epoch [5260/10000], Train Loss: 0.002969196066260338, Test Loss: 0.0011177656706422567\n",
      "Epoch [5270/10000], Train Loss: 0.0031219429802149534, Test Loss: 0.0011284654028713703\n",
      "Epoch [5280/10000], Train Loss: 0.0031425394117832184, Test Loss: 0.0012240790529176593\n",
      "Epoch [5290/10000], Train Loss: 0.003063635667786002, Test Loss: 0.001211835420690477\n",
      "Epoch [5300/10000], Train Loss: 0.002977650845423341, Test Loss: 0.0012167536187916994\n",
      "Epoch [5310/10000], Train Loss: 0.0028652912005782127, Test Loss: 0.0011893021874129772\n",
      "Epoch [5320/10000], Train Loss: 0.0031698117963969707, Test Loss: 0.0011509787291288376\n",
      "Epoch [5330/10000], Train Loss: 0.003190258052200079, Test Loss: 0.0012053011450916529\n",
      "Epoch [5340/10000], Train Loss: 0.0030925071332603693, Test Loss: 0.0010593314655125141\n",
      "Epoch [5350/10000], Train Loss: 0.002977261319756508, Test Loss: 0.0012553961714729667\n",
      "Epoch [5360/10000], Train Loss: 0.003019756404682994, Test Loss: 0.0011442458489909768\n",
      "Epoch [5370/10000], Train Loss: 0.0030858323443681, Test Loss: 0.001149385585449636\n",
      "Epoch [5380/10000], Train Loss: 0.002973243361338973, Test Loss: 0.0012385769514366984\n",
      "Epoch [5390/10000], Train Loss: 0.0031288282480090857, Test Loss: 0.0010737007251009345\n",
      "Epoch [5400/10000], Train Loss: 0.0029088419396430254, Test Loss: 0.0012663932284340262\n",
      "Epoch [5410/10000], Train Loss: 0.00294441357254982, Test Loss: 0.0011489243479445577\n",
      "Epoch [5420/10000], Train Loss: 0.0029844820965081453, Test Loss: 0.0011286413064226508\n",
      "Epoch [5430/10000], Train Loss: 0.0029600055422633886, Test Loss: 0.001248205080628395\n",
      "Epoch [5440/10000], Train Loss: 0.00293759279884398, Test Loss: 0.0011513259960338473\n",
      "Epoch [5450/10000], Train Loss: 0.003212449373677373, Test Loss: 0.0011591590009629726\n",
      "Epoch [5460/10000], Train Loss: 0.0028674593195319176, Test Loss: 0.0012434087693691254\n",
      "Epoch [5470/10000], Train Loss: 0.002977595431730151, Test Loss: 0.0011090886546298862\n",
      "Epoch [5480/10000], Train Loss: 0.003153993980959058, Test Loss: 0.0011581219732761383\n",
      "Epoch [5490/10000], Train Loss: 0.0030410895124077797, Test Loss: 0.0011408923892304301\n",
      "Epoch [5500/10000], Train Loss: 0.0029355764854699373, Test Loss: 0.0012023408198729157\n",
      "Epoch [5510/10000], Train Loss: 0.0028871833346784115, Test Loss: 0.0011206164490431547\n",
      "Epoch [5520/10000], Train Loss: 0.002856978913769126, Test Loss: 0.0012149940012022853\n",
      "Epoch [5530/10000], Train Loss: 0.0029259107541292906, Test Loss: 0.0012313012266531587\n",
      "Epoch [5540/10000], Train Loss: 0.002988774562254548, Test Loss: 0.0011447370052337646\n",
      "Epoch [5550/10000], Train Loss: 0.0030579660087823868, Test Loss: 0.0012297364883124828\n",
      "Epoch [5560/10000], Train Loss: 0.0031435193959623575, Test Loss: 0.0011641329620033503\n",
      "Epoch [5570/10000], Train Loss: 0.0029810331761837006, Test Loss: 0.0011888844892382622\n",
      "Epoch [5580/10000], Train Loss: 0.0029160669073462486, Test Loss: 0.0011159153655171394\n",
      "Epoch [5590/10000], Train Loss: 0.002831320744007826, Test Loss: 0.0011427185963839293\n",
      "Epoch [5600/10000], Train Loss: 0.0029959226958453655, Test Loss: 0.001139658852480352\n",
      "Epoch [5610/10000], Train Loss: 0.002999215852469206, Test Loss: 0.0010933774756267667\n",
      "Epoch [5620/10000], Train Loss: 0.0029886530246585608, Test Loss: 0.0011894272174686193\n",
      "Epoch [5630/10000], Train Loss: 0.002943426137790084, Test Loss: 0.0011886436259374022\n",
      "Epoch [5640/10000], Train Loss: 0.0029883526731282473, Test Loss: 0.0011655357666313648\n",
      "Epoch [5650/10000], Train Loss: 0.0028534913435578346, Test Loss: 0.0011579908896237612\n",
      "Epoch [5660/10000], Train Loss: 0.002772142179310322, Test Loss: 0.0012232618173584342\n",
      "Epoch [5670/10000], Train Loss: 0.0030568810179829597, Test Loss: 0.0011916811345145106\n",
      "Epoch [5680/10000], Train Loss: 0.002943110652267933, Test Loss: 0.0011870410526171327\n",
      "Epoch [5690/10000], Train Loss: 0.002964354120194912, Test Loss: 0.0011289643589407206\n",
      "Epoch [5700/10000], Train Loss: 0.0028504105284810066, Test Loss: 0.0011393846943974495\n",
      "Epoch [5710/10000], Train Loss: 0.0029172440990805626, Test Loss: 0.0011420530499890447\n",
      "Epoch [5720/10000], Train Loss: 0.0029845735989511013, Test Loss: 0.0010961036896333098\n",
      "Epoch [5730/10000], Train Loss: 0.0029448664281517267, Test Loss: 0.0011850051814690232\n",
      "Epoch [5740/10000], Train Loss: 0.0028766924515366554, Test Loss: 0.0011142767034471035\n",
      "Epoch [5750/10000], Train Loss: 0.003089507343247533, Test Loss: 0.0011088059982284904\n",
      "Epoch [5760/10000], Train Loss: 0.00297853397205472, Test Loss: 0.0010726077016443014\n",
      "Epoch [5770/10000], Train Loss: 0.0029625764582306147, Test Loss: 0.0011828377610072494\n",
      "Epoch [5780/10000], Train Loss: 0.002980615012347698, Test Loss: 0.001132787438109517\n",
      "Epoch [5790/10000], Train Loss: 0.0030993919353932142, Test Loss: 0.0011502793058753014\n",
      "Epoch [5800/10000], Train Loss: 0.0029167411848902702, Test Loss: 0.0011880270903930068\n",
      "Epoch [5810/10000], Train Loss: 0.0029474913608282804, Test Loss: 0.0012705549597740173\n",
      "Epoch [5820/10000], Train Loss: 0.002864761743694544, Test Loss: 0.0010713443625718355\n",
      "Epoch [5830/10000], Train Loss: 0.0030461603309959173, Test Loss: 0.00110743404366076\n",
      "Epoch [5840/10000], Train Loss: 0.003017338225618005, Test Loss: 0.0011208888608962297\n",
      "Epoch [5850/10000], Train Loss: 0.0030174751300364733, Test Loss: 0.0010908107506111264\n",
      "Epoch [5860/10000], Train Loss: 0.003077147528529167, Test Loss: 0.0012795173097401857\n",
      "Epoch [5870/10000], Train Loss: 0.0029655948746949434, Test Loss: 0.001077367807738483\n",
      "Epoch [5880/10000], Train Loss: 0.0030854849610477686, Test Loss: 0.0011832972522825003\n",
      "Epoch [5890/10000], Train Loss: 0.0030740024521946907, Test Loss: 0.0010636342922225595\n",
      "Epoch [5900/10000], Train Loss: 0.0030836414080113173, Test Loss: 0.0011419543297961354\n",
      "Epoch [5910/10000], Train Loss: 0.0030654603615403175, Test Loss: 0.0010504958918318152\n",
      "Epoch [5920/10000], Train Loss: 0.0028416479472070932, Test Loss: 0.0013507790863513947\n",
      "Epoch [5930/10000], Train Loss: 0.002989894477650523, Test Loss: 0.0010953644523397088\n",
      "Epoch [5940/10000], Train Loss: 0.0029471712186932564, Test Loss: 0.0010848926613107324\n",
      "Epoch [5950/10000], Train Loss: 0.003158765845000744, Test Loss: 0.0010818787850439548\n",
      "Epoch [5960/10000], Train Loss: 0.002973743947222829, Test Loss: 0.0011033149203285575\n",
      "Epoch [5970/10000], Train Loss: 0.0029407956171780825, Test Loss: 0.0011093326611444354\n",
      "Epoch [5980/10000], Train Loss: 0.002934008836746216, Test Loss: 0.0011942016426473856\n",
      "Epoch [5990/10000], Train Loss: 0.002856325823813677, Test Loss: 0.0010954573517665267\n",
      "Epoch [6000/10000], Train Loss: 0.00302949920296669, Test Loss: 0.0010841860203072429\n",
      "Epoch [6010/10000], Train Loss: 0.0031094704754650593, Test Loss: 0.001187298446893692\n",
      "Epoch [6020/10000], Train Loss: 0.0031342064030468464, Test Loss: 0.0010943490779027343\n",
      "Epoch [6030/10000], Train Loss: 0.0029447597917169333, Test Loss: 0.0010594363557174802\n",
      "Epoch [6040/10000], Train Loss: 0.0029569186735898256, Test Loss: 0.0011956272646784782\n",
      "Epoch [6050/10000], Train Loss: 0.00304497592151165, Test Loss: 0.0011784289963543415\n",
      "Epoch [6060/10000], Train Loss: 0.002927100285887718, Test Loss: 0.0011195503175258636\n",
      "Epoch [6070/10000], Train Loss: 0.0029903959948569536, Test Loss: 0.0010856309672817588\n",
      "Epoch [6080/10000], Train Loss: 0.002887335140258074, Test Loss: 0.0010858784662559628\n",
      "Epoch [6090/10000], Train Loss: 0.0029209619387984276, Test Loss: 0.0011143177980557084\n",
      "Epoch [6100/10000], Train Loss: 0.0028976493049412966, Test Loss: 0.001113917212933302\n",
      "Epoch [6110/10000], Train Loss: 0.003067170502617955, Test Loss: 0.0011186704505234957\n",
      "Epoch [6120/10000], Train Loss: 0.002969132037833333, Test Loss: 0.0011568793561309576\n",
      "Epoch [6130/10000], Train Loss: 0.00279406295157969, Test Loss: 0.0011243182234466076\n",
      "Epoch [6140/10000], Train Loss: 0.002925165230408311, Test Loss: 0.0010568307479843497\n",
      "Epoch [6150/10000], Train Loss: 0.0029556385707110167, Test Loss: 0.0011097447713837028\n",
      "Epoch [6160/10000], Train Loss: 0.0029059122316539288, Test Loss: 0.0010427864035591483\n",
      "Epoch [6170/10000], Train Loss: 0.002852301811799407, Test Loss: 0.0011232023825868964\n",
      "Epoch [6180/10000], Train Loss: 0.003050834173336625, Test Loss: 0.0011192279634997249\n",
      "Epoch [6190/10000], Train Loss: 0.0030908614862710238, Test Loss: 0.001095851301215589\n",
      "Epoch [6200/10000], Train Loss: 0.002934955060482025, Test Loss: 0.0012148602399975061\n",
      "Epoch [6210/10000], Train Loss: 0.0029505526181310415, Test Loss: 0.001087418058887124\n",
      "Epoch [6220/10000], Train Loss: 0.002933660289272666, Test Loss: 0.0011717284796759486\n",
      "Epoch [6230/10000], Train Loss: 0.0029167933389544487, Test Loss: 0.0011362045770511031\n",
      "Epoch [6240/10000], Train Loss: 0.002904775319620967, Test Loss: 0.0010749404318630695\n",
      "Epoch [6250/10000], Train Loss: 0.0028398577123880386, Test Loss: 0.0011687044752761722\n",
      "Epoch [6260/10000], Train Loss: 0.003020280972123146, Test Loss: 0.001162982895039022\n",
      "Epoch [6270/10000], Train Loss: 0.002920987084507942, Test Loss: 0.0010728025808930397\n",
      "Epoch [6280/10000], Train Loss: 0.0029904451221227646, Test Loss: 0.0010773434769362211\n",
      "Epoch [6290/10000], Train Loss: 0.002866313559934497, Test Loss: 0.0011199635919183493\n",
      "Epoch [6300/10000], Train Loss: 0.0028855474665760994, Test Loss: 0.0010785231133922935\n",
      "Epoch [6310/10000], Train Loss: 0.002900604624301195, Test Loss: 0.0010994247859343886\n",
      "Epoch [6320/10000], Train Loss: 0.0031776451505720615, Test Loss: 0.0010965238325297832\n",
      "Epoch [6330/10000], Train Loss: 0.0028232699260115623, Test Loss: 0.0010402242187410593\n",
      "Epoch [6340/10000], Train Loss: 0.003035483183339238, Test Loss: 0.0011018265504390001\n",
      "Epoch [6350/10000], Train Loss: 0.0028785686008632183, Test Loss: 0.0011749303666874766\n",
      "Epoch [6360/10000], Train Loss: 0.0030397376976907253, Test Loss: 0.0010920267086476088\n",
      "Epoch [6370/10000], Train Loss: 0.002921360544860363, Test Loss: 0.0012017611879855394\n",
      "Epoch [6380/10000], Train Loss: 0.002910708775743842, Test Loss: 0.0011319032637402415\n",
      "Epoch [6390/10000], Train Loss: 0.0029428061097860336, Test Loss: 0.0011082994751632214\n",
      "Epoch [6400/10000], Train Loss: 0.0029094438068568707, Test Loss: 0.0011195284314453602\n",
      "Epoch [6410/10000], Train Loss: 0.0029950537718832493, Test Loss: 0.0010873081628233194\n",
      "Epoch [6420/10000], Train Loss: 0.0030225859954953194, Test Loss: 0.0010720548452809453\n",
      "Epoch [6430/10000], Train Loss: 0.002968257525935769, Test Loss: 0.0011044847778975964\n",
      "Epoch [6440/10000], Train Loss: 0.0029629303608089685, Test Loss: 0.001152210053987801\n",
      "Epoch [6450/10000], Train Loss: 0.0030151959508657455, Test Loss: 0.001093983999453485\n",
      "Epoch [6460/10000], Train Loss: 0.0027787007857114077, Test Loss: 0.0010324126342311502\n",
      "Epoch [6470/10000], Train Loss: 0.0030498667620122433, Test Loss: 0.0011779589112848043\n",
      "Epoch [6480/10000], Train Loss: 0.0027546153869479895, Test Loss: 0.0011292104609310627\n",
      "Epoch [6490/10000], Train Loss: 0.0029638740234076977, Test Loss: 0.0010749893262982368\n",
      "Epoch [6500/10000], Train Loss: 0.0029739972669631243, Test Loss: 0.0011717494344338775\n",
      "Epoch [6510/10000], Train Loss: 0.00292428070679307, Test Loss: 0.0010093090822920203\n",
      "Epoch [6520/10000], Train Loss: 0.002918882528319955, Test Loss: 0.0011199084110558033\n",
      "Epoch [6530/10000], Train Loss: 0.0028624965343624353, Test Loss: 0.0010111834853887558\n",
      "Epoch [6540/10000], Train Loss: 0.0030981882009655237, Test Loss: 0.0010883510112762451\n",
      "Epoch [6550/10000], Train Loss: 0.0029705010820180178, Test Loss: 0.0010735549731180072\n",
      "Epoch [6560/10000], Train Loss: 0.0030309134162962437, Test Loss: 0.0011843463871628046\n",
      "Epoch [6570/10000], Train Loss: 0.0029635054524987936, Test Loss: 0.0010681666899472475\n",
      "Epoch [6580/10000], Train Loss: 0.002951675560325384, Test Loss: 0.0011446146527305245\n",
      "Epoch [6590/10000], Train Loss: 0.0029459085781127214, Test Loss: 0.0011387743288651109\n",
      "Epoch [6600/10000], Train Loss: 0.003027313156053424, Test Loss: 0.0011190446093678474\n",
      "Epoch [6610/10000], Train Loss: 0.002833486767485738, Test Loss: 0.0012112221447750926\n",
      "Epoch [6620/10000], Train Loss: 0.0028914962895214558, Test Loss: 0.0011374251917004585\n",
      "Epoch [6630/10000], Train Loss: 0.002978831995278597, Test Loss: 0.0010903477668762207\n",
      "Epoch [6640/10000], Train Loss: 0.0028874326962977648, Test Loss: 0.0011599509743973613\n",
      "Epoch [6650/10000], Train Loss: 0.0028849763330072165, Test Loss: 0.001161178108304739\n",
      "Epoch [6660/10000], Train Loss: 0.002938047284260392, Test Loss: 0.0010080022038891912\n",
      "Epoch [6670/10000], Train Loss: 0.0030111027881503105, Test Loss: 0.001176564022898674\n",
      "Epoch [6680/10000], Train Loss: 0.002983909333124757, Test Loss: 0.001097039203159511\n",
      "Epoch [6690/10000], Train Loss: 0.0028369403444230556, Test Loss: 0.0010896294843405485\n",
      "Epoch [6700/10000], Train Loss: 0.0028583547100424767, Test Loss: 0.0010579482186585665\n",
      "Epoch [6710/10000], Train Loss: 0.002956963377073407, Test Loss: 0.0010594665072858334\n",
      "Epoch [6720/10000], Train Loss: 0.0030091162770986557, Test Loss: 0.0010950028663501143\n",
      "Epoch [6730/10000], Train Loss: 0.0028398919384926558, Test Loss: 0.001176162506453693\n",
      "Epoch [6740/10000], Train Loss: 0.002940880600363016, Test Loss: 0.0009633241570554674\n",
      "Epoch [6750/10000], Train Loss: 0.002983825048431754, Test Loss: 0.0012139088939875364\n",
      "Epoch [6760/10000], Train Loss: 0.0031332881189882755, Test Loss: 0.001097417320124805\n",
      "Epoch [6770/10000], Train Loss: 0.002855932107195258, Test Loss: 0.0011338721960783005\n",
      "Epoch [6780/10000], Train Loss: 0.002905032830312848, Test Loss: 0.0011534113436937332\n",
      "Epoch [6790/10000], Train Loss: 0.0029148219618946314, Test Loss: 0.0010249050101265311\n",
      "Epoch [6800/10000], Train Loss: 0.003070610575377941, Test Loss: 0.0011021204991266131\n",
      "Epoch [6810/10000], Train Loss: 0.0028201809618622065, Test Loss: 0.001105707953684032\n",
      "Epoch [6820/10000], Train Loss: 0.0029512320179492235, Test Loss: 0.0010436782613396645\n",
      "Epoch [6830/10000], Train Loss: 0.0030346650164574385, Test Loss: 0.0011551122879609466\n",
      "Epoch [6840/10000], Train Loss: 0.0029672235250473022, Test Loss: 0.0011108409380540252\n",
      "Epoch [6850/10000], Train Loss: 0.0029586234595626593, Test Loss: 0.0011008195579051971\n",
      "Epoch [6860/10000], Train Loss: 0.0028028807137161493, Test Loss: 0.0010303179733455181\n",
      "Epoch [6870/10000], Train Loss: 0.0029171137139201164, Test Loss: 0.0010315979598090053\n",
      "Epoch [6880/10000], Train Loss: 0.0028950113337486982, Test Loss: 0.0012030093930661678\n",
      "Epoch [6890/10000], Train Loss: 0.0028717012610286474, Test Loss: 0.0010179107775911689\n",
      "Epoch [6900/10000], Train Loss: 0.002878031926229596, Test Loss: 0.001153467921540141\n",
      "Epoch [6910/10000], Train Loss: 0.002863657893612981, Test Loss: 0.0011456869542598724\n",
      "Epoch [6920/10000], Train Loss: 0.0029100754763931036, Test Loss: 0.0010576228378340602\n",
      "Epoch [6930/10000], Train Loss: 0.0030139698646962643, Test Loss: 0.000997296185232699\n",
      "Epoch [6940/10000], Train Loss: 0.002946502761915326, Test Loss: 0.0011998012196272612\n",
      "Epoch [6950/10000], Train Loss: 0.002976995427161455, Test Loss: 0.0010520321084186435\n",
      "Epoch [6960/10000], Train Loss: 0.002735112328082323, Test Loss: 0.0011528086615726352\n",
      "Epoch [6970/10000], Train Loss: 0.0028008343651890755, Test Loss: 0.0010688371257856488\n",
      "Epoch [6980/10000], Train Loss: 0.002893964760005474, Test Loss: 0.0010852403938770294\n",
      "Epoch [6990/10000], Train Loss: 0.002967294305562973, Test Loss: 0.0011672545224428177\n",
      "Epoch [7000/10000], Train Loss: 0.0029206969775259495, Test Loss: 0.0009996751323342323\n",
      "Epoch [7010/10000], Train Loss: 0.0028676821384578943, Test Loss: 0.0011740216286852956\n",
      "Epoch [7020/10000], Train Loss: 0.002894858131185174, Test Loss: 0.0011045995634049177\n",
      "Epoch [7030/10000], Train Loss: 0.002865307964384556, Test Loss: 0.001039237016811967\n",
      "Epoch [7040/10000], Train Loss: 0.0028517625760287046, Test Loss: 0.0010938576888293028\n",
      "Epoch [7050/10000], Train Loss: 0.0028495851438492537, Test Loss: 0.0010871501872316003\n",
      "Epoch [7060/10000], Train Loss: 0.0028940336778759956, Test Loss: 0.0011271571274846792\n",
      "Epoch [7070/10000], Train Loss: 0.0030224286019802094, Test Loss: 0.0010215028887614608\n",
      "Epoch [7080/10000], Train Loss: 0.0029517202638089657, Test Loss: 0.0011118616675958037\n",
      "Epoch [7090/10000], Train Loss: 0.0028633419424295425, Test Loss: 0.0011401952942833304\n",
      "Epoch [7100/10000], Train Loss: 0.002985255792737007, Test Loss: 0.0010960974032059312\n",
      "Epoch [7110/10000], Train Loss: 0.0028850503731518984, Test Loss: 0.0010957146296277642\n",
      "Epoch [7120/10000], Train Loss: 0.0029522692784667015, Test Loss: 0.0011092191562056541\n",
      "Epoch [7130/10000], Train Loss: 0.002801201306283474, Test Loss: 0.00107920344453305\n",
      "Epoch [7140/10000], Train Loss: 0.0029394186567515135, Test Loss: 0.0010043272050097585\n",
      "Epoch [7150/10000], Train Loss: 0.003071227576583624, Test Loss: 0.001065103686414659\n",
      "Epoch [7160/10000], Train Loss: 0.0030290272552520037, Test Loss: 0.0011015126947313547\n",
      "Epoch [7170/10000], Train Loss: 0.0029845996759831905, Test Loss: 0.0010926984250545502\n",
      "Epoch [7180/10000], Train Loss: 0.002809198573231697, Test Loss: 0.0011703320778906345\n",
      "Epoch [7190/10000], Train Loss: 0.002898235572502017, Test Loss: 0.0010388370137661695\n",
      "Epoch [7200/10000], Train Loss: 0.0028543761000037193, Test Loss: 0.0010515102185308933\n",
      "Epoch [7210/10000], Train Loss: 0.002880087122321129, Test Loss: 0.0010675318771973252\n",
      "Epoch [7220/10000], Train Loss: 0.0030260554049164057, Test Loss: 0.0010934172896668315\n",
      "Epoch [7230/10000], Train Loss: 0.003026401624083519, Test Loss: 0.0011477996595203876\n",
      "Epoch [7240/10000], Train Loss: 0.0029193777590990067, Test Loss: 0.001040887669660151\n",
      "Epoch [7250/10000], Train Loss: 0.002887635724619031, Test Loss: 0.0012091428507119417\n",
      "Epoch [7260/10000], Train Loss: 0.0029252890963107347, Test Loss: 0.0009958050213754177\n",
      "Epoch [7270/10000], Train Loss: 0.0028789136558771133, Test Loss: 0.0010559037327766418\n",
      "Epoch [7280/10000], Train Loss: 0.0029852890875190496, Test Loss: 0.0012396933743730187\n",
      "Epoch [7290/10000], Train Loss: 0.0029942933470010757, Test Loss: 0.0010872146813198924\n",
      "Epoch [7300/10000], Train Loss: 0.0029505828861147165, Test Loss: 0.0010144050465896726\n",
      "Epoch [7310/10000], Train Loss: 0.0029641594737768173, Test Loss: 0.00112871453166008\n",
      "Epoch [7320/10000], Train Loss: 0.0027845818549394608, Test Loss: 0.0011260054307058454\n",
      "Epoch [7330/10000], Train Loss: 0.0029008272103965282, Test Loss: 0.000996490241959691\n",
      "Epoch [7340/10000], Train Loss: 0.0028891952242702246, Test Loss: 0.0011113546788692474\n",
      "Epoch [7350/10000], Train Loss: 0.003003070130944252, Test Loss: 0.0010970172006636858\n",
      "Epoch [7360/10000], Train Loss: 0.0028845330234616995, Test Loss: 0.0011224334593862295\n",
      "Epoch [7370/10000], Train Loss: 0.002990524284541607, Test Loss: 0.0010902720969170332\n",
      "Epoch [7380/10000], Train Loss: 0.002942450810223818, Test Loss: 0.0010614183265715837\n",
      "Epoch [7390/10000], Train Loss: 0.00289542181417346, Test Loss: 0.0010364780900999904\n",
      "Epoch [7400/10000], Train Loss: 0.002921308623626828, Test Loss: 0.0011062236735597253\n",
      "Epoch [7410/10000], Train Loss: 0.002949226414784789, Test Loss: 0.001078532892279327\n",
      "Epoch [7420/10000], Train Loss: 0.0028453445993363857, Test Loss: 0.001045814948156476\n",
      "Epoch [7430/10000], Train Loss: 0.002982113277539611, Test Loss: 0.001010725274682045\n",
      "Epoch [7440/10000], Train Loss: 0.0027661111671477556, Test Loss: 0.0010698324767872691\n",
      "Epoch [7450/10000], Train Loss: 0.0028770670760422945, Test Loss: 0.0011415587505325675\n",
      "Epoch [7460/10000], Train Loss: 0.0029762540943920612, Test Loss: 0.0010533984750509262\n",
      "Epoch [7470/10000], Train Loss: 0.002816623542457819, Test Loss: 0.0011253530392423272\n",
      "Epoch [7480/10000], Train Loss: 0.002881592372432351, Test Loss: 0.001048815087415278\n",
      "Epoch [7490/10000], Train Loss: 0.002951864618808031, Test Loss: 0.0011154840467497706\n",
      "Epoch [7500/10000], Train Loss: 0.003099511843174696, Test Loss: 0.0010446652304381132\n",
      "Epoch [7510/10000], Train Loss: 0.002864442765712738, Test Loss: 0.0010958329075947404\n",
      "Epoch [7520/10000], Train Loss: 0.002900999505072832, Test Loss: 0.0010626319563016295\n",
      "Epoch [7530/10000], Train Loss: 0.0028704877477139235, Test Loss: 0.001136349979788065\n",
      "Epoch [7540/10000], Train Loss: 0.00278279441408813, Test Loss: 0.0011482327245175838\n",
      "Epoch [7550/10000], Train Loss: 0.002813948318362236, Test Loss: 0.0010844729840755463\n",
      "Epoch [7560/10000], Train Loss: 0.0028332299552857876, Test Loss: 0.0010745626641437411\n",
      "Epoch [7570/10000], Train Loss: 0.0028803395107388496, Test Loss: 0.0011092611821368337\n",
      "Epoch [7580/10000], Train Loss: 0.0028661382384598255, Test Loss: 0.0010025746887549758\n",
      "Epoch [7590/10000], Train Loss: 0.0029509863816201687, Test Loss: 0.0011236016871407628\n",
      "Epoch [7600/10000], Train Loss: 0.0027791562024503946, Test Loss: 0.0010579098016023636\n",
      "Epoch [7610/10000], Train Loss: 0.003001376986503601, Test Loss: 0.0010930547723546624\n",
      "Epoch [7620/10000], Train Loss: 0.0029194976668804884, Test Loss: 0.0010479658376425505\n",
      "Epoch [7630/10000], Train Loss: 0.0029932421166449785, Test Loss: 0.0010576883796602488\n",
      "Epoch [7640/10000], Train Loss: 0.0028439133893698454, Test Loss: 0.001084994408302009\n",
      "Epoch [7650/10000], Train Loss: 0.0028086842503398657, Test Loss: 0.0010505514219403267\n",
      "Epoch [7660/10000], Train Loss: 0.0030484022572636604, Test Loss: 0.0010613598860800266\n",
      "Epoch [7670/10000], Train Loss: 0.0028448658995330334, Test Loss: 0.0010443682549521327\n",
      "Epoch [7680/10000], Train Loss: 0.0029446554835885763, Test Loss: 0.0011456119827926159\n",
      "Epoch [7690/10000], Train Loss: 0.0028772945515811443, Test Loss: 0.0009679723880253732\n",
      "Epoch [7700/10000], Train Loss: 0.00278016016818583, Test Loss: 0.0011645000195130706\n",
      "Early stopping at epoch 7702\n"
     ]
    }
   ],
   "source": [
    "# Move the model to GPU if available\n",
    "model = DeepNN().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Early stopping\n",
    "patience = 1000\n",
    "best_loss = float('inf')\n",
    "best_model_state = None\n",
    "no_improvement_epochs = 0\n",
    "\n",
    "# Training the model\n",
    "num_epochs = 10000\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train)\n",
    "    train_loss = criterion(outputs, y_train)\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model(X_test)\n",
    "        test_loss = criterion(test_outputs, y_test)\n",
    "\n",
    "    train_losses.append(train_loss.item())\n",
    "    test_losses.append(test_loss.item())\n",
    "\n",
    "    if test_loss < best_loss:\n",
    "        best_loss = test_loss\n",
    "        best_model_state = model.state_dict()\n",
    "        no_improvement_epochs = 0\n",
    "    else:\n",
    "        no_improvement_epochs += 1\n",
    "\n",
    "    if no_improvement_epochs >= patience:\n",
    "        print(f'Early stopping at epoch {epoch+1}')\n",
    "        break\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss.item()}, Test Loss: {test_loss.item()}')\n",
    "\n",
    "# Load the best model state\n",
    "if best_model_state is not None:\n",
    "    model.load_state_dict(best_model_state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'avg_flightdata_model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pteraenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
