{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Network for Design Parameters for FWUAV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modules to import for data preprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as rnd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing \n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "\n",
    "# Modules to import for Torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Check for GPU device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Data loaded into pandas : \n",
      " <module 'torch.utils.data' from '/home/stimp/anaconda3/envs/pteraenv/lib/python3.8/site-packages/torch/utils/data/__init__.py'>\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Features : \n",
      " [['naca8304' 0.4000000059604645 1.5 ... 0.4000000059604645 3.0 5.0]\n",
      " ['naca8304' 0.4000000059604645 1.5 ... 0.4000000059604645 3.0 15.0]\n",
      " ['naca8304' 0.4000000059604645 1.5 ... 0.4000000059604645 3.0 25.0]\n",
      " ...\n",
      " ['goe225' 0.800000011920929 2.0 ... 1.100000023841858 6.5 35.0]\n",
      " ['naca2412' 0.800000011920929 2.0 ... 1.100000023841858 6.5 35.0]\n",
      " ['naca0012' 0.800000011920929 2.0 ... 1.100000023841858 6.5 35.0]]\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "Targets : \n",
      " [[-0.18576185405254364 0.010605402290821075]\n",
      " [-0.4964708685874939 -0.05340539291501045]\n",
      " [-0.8249507546424866 -0.16106195747852325]\n",
      " ...\n",
      " [-33.38216781616211 -5.573355197906494]\n",
      " [-30.313720703125 -4.444688320159912]\n",
      " [-29.279325485229492 -4.102921962738037]]\n"
     ]
    }
   ],
   "source": [
    "# Loading the data into the pandas dataframe\n",
    "\n",
    "# Printing the datafram \n",
    "print(\"\\n Data loaded into pandas : \\n\", data)\n",
    "\n",
    "# Reading the Data\n",
    "data = pd.read_csv(\"Design_Data.csv\", header=None)\n",
    "\n",
    "# Set the first column to object type\n",
    "data[:0] = data[:0].astype('object')\n",
    "\n",
    "# Set dtype for the rest of the columns to float32\n",
    "data.iloc[:, 1:] = data.iloc[:, 1:].astype('float32')\n",
    "\n",
    "print(\"\\n---------------------------------------------------------------------------\\n\")\n",
    "\n",
    "# Inserting the data columns \n",
    "data.columns = [\n",
    "    'Airfoil','Wing Span', 'Taper Ratio', 'Aspect Ratio', 'Flapping Period',\n",
    "    'Airspeed', 'Angle of Attack', 'Lift', 'Induced Drag'\n",
    "]\n",
    "\n",
    "print(\"\\n---------------------------------------------------------------------------\\n\")\n",
    "\n",
    "# Split the data into features and targets\n",
    "X = data[[\n",
    "    'Airfoil','Wing Span', 'Taper Ratio', 'Aspect Ratio', 'Flapping Period',\n",
    "    'Airspeed', 'Angle of Attack'\n",
    "]]\n",
    "\n",
    "print(\"Features : \\n\", X.values)\n",
    "\n",
    "print(\"\\n---------------------------------------------------------------------------\\n\")\n",
    "y = data[['Lift', 'Induced Drag']]\n",
    "\n",
    "print(\"Targets : \\n\",y.values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot encoder and scaler fit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Encoded Airfoils:\n",
      " [[0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]]\n",
      "\n",
      "Normalized Features without Airfoil:\n",
      "        Wing Span  Taper Ratio  Aspect Ratio  Flapping Period  Airspeed  \\\n",
      "0       0.076923     0.076923      0.000000         0.222222  0.222222   \n",
      "1       0.076923     0.076923      0.000000         0.222222  0.222222   \n",
      "2       0.076923     0.076923      0.000000         0.222222  0.222222   \n",
      "3       0.076923     0.076923      0.000000         0.222222  0.222222   \n",
      "4       0.076923     0.076923      0.000000         0.222222  0.444444   \n",
      "...          ...          ...           ...              ...       ...   \n",
      "10682   0.384615     0.230769      0.333333         1.000000  1.000000   \n",
      "10683   0.384615     0.230769      0.333333         1.000000  1.000000   \n",
      "10684   0.384615     0.230769      0.333333         1.000000  1.000000   \n",
      "10685   0.384615     0.230769      0.333333         1.000000  1.000000   \n",
      "10686   0.384615     0.230769      0.333333         1.000000  1.000000   \n",
      "\n",
      "       Angle of Attack  \n",
      "0             0.000000  \n",
      "1             0.285714  \n",
      "2             0.571429  \n",
      "3             0.857143  \n",
      "4             0.000000  \n",
      "...                ...  \n",
      "10682         0.714286  \n",
      "10683         0.714286  \n",
      "10684         0.857143  \n",
      "10685         0.857143  \n",
      "10686         0.857143  \n",
      "\n",
      "[10687 rows x 6 columns]\n",
      "\n",
      "Normalized Targets:\n",
      "            Lift  Induced Drag\n",
      "0      0.999805      0.041019\n",
      "1      0.998737      0.040646\n",
      "2      0.997607      0.040018\n",
      "3      0.996447      0.039060\n",
      "4      0.999344      0.040973\n",
      "...         ...           ...\n",
      "10682  0.909011      0.022258\n",
      "10683  0.912632      0.023970\n",
      "10684  0.885678      0.008461\n",
      "10685  0.896227      0.015042\n",
      "10686  0.899784      0.017035\n",
      "\n",
      "[10687 rows x 2 columns]\n",
      "\n",
      "Encoded Airfoil DataFrame:\n",
      "        Airfoil_goe225  Airfoil_naca0012  Airfoil_naca2412  Airfoil_naca8304\n",
      "0                 0.0               0.0               0.0               1.0\n",
      "1                 0.0               0.0               0.0               1.0\n",
      "2                 0.0               0.0               0.0               1.0\n",
      "3                 0.0               0.0               0.0               1.0\n",
      "4                 0.0               0.0               0.0               1.0\n",
      "...               ...               ...               ...               ...\n",
      "10682             0.0               0.0               1.0               0.0\n",
      "10683             0.0               1.0               0.0               0.0\n",
      "10684             1.0               0.0               0.0               0.0\n",
      "10685             0.0               0.0               1.0               0.0\n",
      "10686             0.0               1.0               0.0               0.0\n",
      "\n",
      "[10687 rows x 4 columns]\n",
      "\n",
      "Scaled and Encoded Features:\n",
      "        Airfoil_goe225  Airfoil_naca0012  Airfoil_naca2412  Airfoil_naca8304  \\\n",
      "0                 0.0               0.0               0.0               1.0   \n",
      "1                 0.0               0.0               0.0               1.0   \n",
      "2                 0.0               0.0               0.0               1.0   \n",
      "3                 0.0               0.0               0.0               1.0   \n",
      "4                 0.0               0.0               0.0               1.0   \n",
      "...               ...               ...               ...               ...   \n",
      "10682             0.0               0.0               1.0               0.0   \n",
      "10683             0.0               1.0               0.0               0.0   \n",
      "10684             1.0               0.0               0.0               0.0   \n",
      "10685             0.0               0.0               1.0               0.0   \n",
      "10686             0.0               1.0               0.0               0.0   \n",
      "\n",
      "       Wing Span  Taper Ratio  Aspect Ratio  Flapping Period  Airspeed  \\\n",
      "0       0.076923     0.076923      0.000000         0.222222  0.222222   \n",
      "1       0.076923     0.076923      0.000000         0.222222  0.222222   \n",
      "2       0.076923     0.076923      0.000000         0.222222  0.222222   \n",
      "3       0.076923     0.076923      0.000000         0.222222  0.222222   \n",
      "4       0.076923     0.076923      0.000000         0.222222  0.444444   \n",
      "...          ...          ...           ...              ...       ...   \n",
      "10682   0.384615     0.230769      0.333333         1.000000  1.000000   \n",
      "10683   0.384615     0.230769      0.333333         1.000000  1.000000   \n",
      "10684   0.384615     0.230769      0.333333         1.000000  1.000000   \n",
      "10685   0.384615     0.230769      0.333333         1.000000  1.000000   \n",
      "10686   0.384615     0.230769      0.333333         1.000000  1.000000   \n",
      "\n",
      "       Angle of Attack  \n",
      "0             0.000000  \n",
      "1             0.285714  \n",
      "2             0.571429  \n",
      "3             0.857143  \n",
      "4             0.000000  \n",
      "...                ...  \n",
      "10682         0.714286  \n",
      "10683         0.714286  \n",
      "10684         0.857143  \n",
      "10685         0.857143  \n",
      "10686         0.857143  \n",
      "\n",
      "[10687 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "# Assuming X and y are already defined in your Jupyter notebook\n",
    "# X - Features DataFrame\n",
    "# y - Targets DataFrame\n",
    "\n",
    "# Drop the Airfoil column from the features\n",
    "airfoils = X['Airfoil']\n",
    "\n",
    "# One hot encode the airfoil column\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "encoded_airfoil = encoder.fit_transform(airfoils.values.reshape(-1, 1))\n",
    "\n",
    "print(\"\\nEncoded Airfoils:\\n\", encoded_airfoil)\n",
    "\n",
    "X_num = X.drop('Airfoil', axis=1)\n",
    "\n",
    "Scalerx = MinMaxScaler()\n",
    "Scalery = MinMaxScaler()\n",
    "\n",
    "X_scaled_np = Scalerx.fit_transform(X_num)\n",
    "X_scaled = pd.DataFrame(X_scaled_np, columns=X_num.columns)\n",
    "print(\"\\nNormalized Features without Airfoil:\\n\", X_scaled)\n",
    "\n",
    "y_scaled_np = Scalery.fit_transform(y.values)\n",
    "y_scaled = pd.DataFrame(y_scaled_np, columns=y.columns)\n",
    "print(\"\\nNormalized Targets:\\n\", y_scaled)\n",
    "\n",
    "X_encoded_pd = pd.DataFrame(encoded_airfoil, columns=encoder.get_feature_names_out(['Airfoil']))\n",
    "print(\"\\nEncoded Airfoil DataFrame:\\n\", X_encoded_pd)\n",
    "\n",
    "X_final = pd.concat([X_encoded_pd, X_scaled], axis=1)\n",
    "print(\"\\nScaled and Encoded Features:\\n\", X_final)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting the data into training and testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training features shape: (8549, 10)\n",
      "Testing features shape: (2138, 10)\n",
      "Training target shape: (8549, 2)\n",
      "Testing target shape: (2138, 2)\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_final, y_scaled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Verify the split\n",
    "print(\"Training features shape:\", X_train.shape)\n",
    "print(\"Testing features shape:\", X_test.shape)\n",
    "print(\"Training target shape:\", y_train.shape)\n",
    "print(\"Testing target shape:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeepNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(10, 128)  # Change input features to 10\n",
    "        self.dropout1 = nn.Dropout(0.5)  # Dropout layer\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.dropout2 = nn.Dropout(0.5)  # Dropout layer\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.dropout3 = nn.Dropout(0.5)  # Dropout layer\n",
    "        self.fc4 = nn.Linear(32, 2)    # Change output features to 2\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.dropout3(x)\n",
    "        x = self.fc4(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to 2D PyTorch tensors\n",
    "X_train = torch.tensor(X_train.values, dtype=torch.float32).to(device)\n",
    "y_train = torch.tensor(y_train.values, dtype=torch.float32).to(device)\n",
    "X_test = torch.tensor(X_test.values, dtype=torch.float32).to(device)\n",
    "y_test = torch.tensor(y_test.values, dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8549, 10])\n",
      "tensor([[0.9192, 0.1097],\n",
      "        [0.9863, 0.0346],\n",
      "        [0.7682, 0.2483],\n",
      "        ...,\n",
      "        [0.9631, 0.0365],\n",
      "        [0.9796, 0.0491],\n",
      "        [0.9909, 0.0391]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and eval of the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100000], Train Loss: 0.37549450993537903, Test Loss: 0.3512588441371918\n",
      "Epoch [20/100000], Train Loss: 0.1652696430683136, Test Loss: 0.09166134148836136\n",
      "Epoch [30/100000], Train Loss: 0.12689192593097687, Test Loss: 0.036328837275505066\n",
      "Epoch [40/100000], Train Loss: 0.11586745083332062, Test Loss: 0.061912886798381805\n",
      "Epoch [50/100000], Train Loss: 0.10200813412666321, Test Loss: 0.034754447638988495\n",
      "Epoch [60/100000], Train Loss: 0.0944269523024559, Test Loss: 0.036932360380887985\n",
      "Epoch [70/100000], Train Loss: 0.0874461829662323, Test Loss: 0.030865924432873726\n",
      "Epoch [80/100000], Train Loss: 0.08031661063432693, Test Loss: 0.030014552175998688\n",
      "Epoch [90/100000], Train Loss: 0.07829642295837402, Test Loss: 0.02650613896548748\n",
      "Epoch [100/100000], Train Loss: 0.07511477917432785, Test Loss: 0.025804026052355766\n",
      "Epoch [110/100000], Train Loss: 0.07022678852081299, Test Loss: 0.024081353098154068\n",
      "Epoch [120/100000], Train Loss: 0.06793159246444702, Test Loss: 0.023050522431731224\n",
      "Epoch [130/100000], Train Loss: 0.06473951786756516, Test Loss: 0.022121844813227654\n",
      "Epoch [140/100000], Train Loss: 0.06441529840230942, Test Loss: 0.020633630454540253\n",
      "Epoch [150/100000], Train Loss: 0.061020221561193466, Test Loss: 0.020292041823267937\n",
      "Epoch [160/100000], Train Loss: 0.0582503080368042, Test Loss: 0.01955338753759861\n",
      "Epoch [170/100000], Train Loss: 0.05555499345064163, Test Loss: 0.01829947531223297\n",
      "Epoch [180/100000], Train Loss: 0.0546671524643898, Test Loss: 0.017361825332045555\n",
      "Epoch [190/100000], Train Loss: 0.05265140160918236, Test Loss: 0.01639193296432495\n",
      "Epoch [200/100000], Train Loss: 0.0521322600543499, Test Loss: 0.016845816746354103\n",
      "Epoch [210/100000], Train Loss: 0.05035799369215965, Test Loss: 0.01637699082493782\n",
      "Epoch [220/100000], Train Loss: 0.04868204519152641, Test Loss: 0.015016076155006886\n",
      "Epoch [230/100000], Train Loss: 0.04799918085336685, Test Loss: 0.014759515412151814\n",
      "Epoch [240/100000], Train Loss: 0.045958273112773895, Test Loss: 0.01418361533433199\n",
      "Epoch [250/100000], Train Loss: 0.045654233545064926, Test Loss: 0.013528610579669476\n",
      "Epoch [260/100000], Train Loss: 0.043805696070194244, Test Loss: 0.011815552599728107\n",
      "Epoch [270/100000], Train Loss: 0.04283491522073746, Test Loss: 0.01052122376859188\n",
      "Epoch [280/100000], Train Loss: 0.04140050709247589, Test Loss: 0.010309213772416115\n",
      "Epoch [290/100000], Train Loss: 0.03992193937301636, Test Loss: 0.009612537920475006\n",
      "Epoch [300/100000], Train Loss: 0.0391029417514801, Test Loss: 0.009319744072854519\n",
      "Epoch [310/100000], Train Loss: 0.03804303705692291, Test Loss: 0.009156579151749611\n",
      "Epoch [320/100000], Train Loss: 0.03801630809903145, Test Loss: 0.008579425513744354\n",
      "Epoch [330/100000], Train Loss: 0.03702430799603462, Test Loss: 0.008042252622544765\n",
      "Epoch [340/100000], Train Loss: 0.03565347194671631, Test Loss: 0.007855042815208435\n",
      "Epoch [350/100000], Train Loss: 0.03442610427737236, Test Loss: 0.007557949982583523\n",
      "Epoch [360/100000], Train Loss: 0.03415883332490921, Test Loss: 0.00771698821336031\n",
      "Epoch [370/100000], Train Loss: 0.033574629575014114, Test Loss: 0.00756054325029254\n",
      "Epoch [380/100000], Train Loss: 0.03358132019639015, Test Loss: 0.006995629984885454\n",
      "Epoch [390/100000], Train Loss: 0.03225059062242508, Test Loss: 0.0066611082293093204\n",
      "Epoch [400/100000], Train Loss: 0.031927142292261124, Test Loss: 0.0066015156917274\n",
      "Epoch [410/100000], Train Loss: 0.030876507982611656, Test Loss: 0.006824246142059565\n",
      "Epoch [420/100000], Train Loss: 0.03128551319241524, Test Loss: 0.006447840016335249\n",
      "Epoch [430/100000], Train Loss: 0.030148211866617203, Test Loss: 0.005733194760978222\n",
      "Epoch [440/100000], Train Loss: 0.030017318204045296, Test Loss: 0.005845962092280388\n",
      "Epoch [450/100000], Train Loss: 0.02970990166068077, Test Loss: 0.00606701010838151\n",
      "Epoch [460/100000], Train Loss: 0.029149578884243965, Test Loss: 0.006155865732580423\n",
      "Epoch [470/100000], Train Loss: 0.02847161889076233, Test Loss: 0.0058841221034526825\n",
      "Epoch [480/100000], Train Loss: 0.02817024476826191, Test Loss: 0.005964016076177359\n",
      "Epoch [490/100000], Train Loss: 0.027715463191270828, Test Loss: 0.005514271557331085\n",
      "Epoch [500/100000], Train Loss: 0.026965001598000526, Test Loss: 0.0055358922109007835\n",
      "Epoch [510/100000], Train Loss: 0.027702558785676956, Test Loss: 0.005384386051446199\n",
      "Epoch [520/100000], Train Loss: 0.02636781334877014, Test Loss: 0.005351327825337648\n",
      "Epoch [530/100000], Train Loss: 0.025586750358343124, Test Loss: 0.00525858486071229\n",
      "Epoch [540/100000], Train Loss: 0.02532251551747322, Test Loss: 0.005291128531098366\n",
      "Epoch [550/100000], Train Loss: 0.025116100907325745, Test Loss: 0.0049437894485890865\n",
      "Epoch [560/100000], Train Loss: 0.024842582643032074, Test Loss: 0.0048620570451021194\n",
      "Epoch [570/100000], Train Loss: 0.024862416088581085, Test Loss: 0.005006353836506605\n",
      "Epoch [580/100000], Train Loss: 0.02422008290886879, Test Loss: 0.00495706545189023\n",
      "Epoch [590/100000], Train Loss: 0.023960093036293983, Test Loss: 0.00444031273946166\n",
      "Epoch [600/100000], Train Loss: 0.02343788929283619, Test Loss: 0.004641251638531685\n",
      "Epoch [610/100000], Train Loss: 0.023502172902226448, Test Loss: 0.004615976475179195\n",
      "Epoch [620/100000], Train Loss: 0.023240799084305763, Test Loss: 0.0044477470219135284\n",
      "Epoch [630/100000], Train Loss: 0.023014847189188004, Test Loss: 0.004436114802956581\n",
      "Epoch [640/100000], Train Loss: 0.02238764986395836, Test Loss: 0.004324436653405428\n",
      "Epoch [650/100000], Train Loss: 0.02275623008608818, Test Loss: 0.004066601861268282\n",
      "Epoch [660/100000], Train Loss: 0.02263224869966507, Test Loss: 0.004015385638922453\n",
      "Epoch [670/100000], Train Loss: 0.02190939150750637, Test Loss: 0.004151409026235342\n",
      "Epoch [680/100000], Train Loss: 0.02187189646065235, Test Loss: 0.003748598974198103\n",
      "Epoch [690/100000], Train Loss: 0.022004159167408943, Test Loss: 0.0038232384249567986\n",
      "Epoch [700/100000], Train Loss: 0.02149377390742302, Test Loss: 0.003847497748211026\n",
      "Epoch [710/100000], Train Loss: 0.02096480503678322, Test Loss: 0.0037878185976296663\n",
      "Epoch [720/100000], Train Loss: 0.02076488360762596, Test Loss: 0.003625015029683709\n",
      "Epoch [730/100000], Train Loss: 0.02053540199995041, Test Loss: 0.0034874267876148224\n",
      "Epoch [740/100000], Train Loss: 0.02019052766263485, Test Loss: 0.0036912315990775824\n",
      "Epoch [750/100000], Train Loss: 0.019567817449569702, Test Loss: 0.003504189196974039\n",
      "Epoch [760/100000], Train Loss: 0.020011158660054207, Test Loss: 0.003403965849429369\n",
      "Epoch [770/100000], Train Loss: 0.019612940028309822, Test Loss: 0.003330945735797286\n",
      "Epoch [780/100000], Train Loss: 0.019692687317728996, Test Loss: 0.0033320505172014236\n",
      "Epoch [790/100000], Train Loss: 0.019020531326532364, Test Loss: 0.0032396321184933186\n",
      "Epoch [800/100000], Train Loss: 0.01897943578660488, Test Loss: 0.003280664561316371\n",
      "Epoch [810/100000], Train Loss: 0.018724218010902405, Test Loss: 0.0030269105918705463\n",
      "Epoch [820/100000], Train Loss: 0.018773075193166733, Test Loss: 0.003272495698183775\n",
      "Epoch [830/100000], Train Loss: 0.018587172031402588, Test Loss: 0.003154149977490306\n",
      "Epoch [840/100000], Train Loss: 0.01819676347076893, Test Loss: 0.0030943627934902906\n",
      "Epoch [850/100000], Train Loss: 0.018473783507943153, Test Loss: 0.0029124326538294554\n",
      "Epoch [860/100000], Train Loss: 0.017970595508813858, Test Loss: 0.002811447950080037\n",
      "Epoch [870/100000], Train Loss: 0.0177150946110487, Test Loss: 0.003037199843674898\n",
      "Epoch [880/100000], Train Loss: 0.017832770943641663, Test Loss: 0.0029956421349197626\n",
      "Epoch [890/100000], Train Loss: 0.017429763451218605, Test Loss: 0.002769125858321786\n",
      "Epoch [900/100000], Train Loss: 0.01722111739218235, Test Loss: 0.002935822121798992\n",
      "Epoch [910/100000], Train Loss: 0.017045646905899048, Test Loss: 0.0028895391151309013\n",
      "Epoch [920/100000], Train Loss: 0.017096448689699173, Test Loss: 0.0027406811714172363\n",
      "Epoch [930/100000], Train Loss: 0.01698959991335869, Test Loss: 0.0027729508001357317\n",
      "Epoch [940/100000], Train Loss: 0.017074361443519592, Test Loss: 0.002762885531410575\n",
      "Epoch [950/100000], Train Loss: 0.01665610820055008, Test Loss: 0.002771792933344841\n",
      "Epoch [960/100000], Train Loss: 0.017002614215016365, Test Loss: 0.0025946858804672956\n",
      "Epoch [970/100000], Train Loss: 0.016482440754771233, Test Loss: 0.0027420474216341972\n",
      "Epoch [980/100000], Train Loss: 0.016060566529631615, Test Loss: 0.002593723824247718\n",
      "Epoch [990/100000], Train Loss: 0.015952104702591896, Test Loss: 0.002584918634966016\n",
      "Epoch [1000/100000], Train Loss: 0.01589578576385975, Test Loss: 0.0026817910838872194\n",
      "Epoch [1010/100000], Train Loss: 0.01592712104320526, Test Loss: 0.002533842111006379\n",
      "Epoch [1020/100000], Train Loss: 0.015366106294095516, Test Loss: 0.002539213979616761\n",
      "Epoch [1030/100000], Train Loss: 0.015462970361113548, Test Loss: 0.002569650998339057\n",
      "Epoch [1040/100000], Train Loss: 0.01541830599308014, Test Loss: 0.0022684584837406874\n",
      "Epoch [1050/100000], Train Loss: 0.015411555767059326, Test Loss: 0.0026188506744802\n",
      "Epoch [1060/100000], Train Loss: 0.015204761177301407, Test Loss: 0.0024128879886120558\n",
      "Epoch [1070/100000], Train Loss: 0.014932509511709213, Test Loss: 0.0024248480331152678\n",
      "Epoch [1080/100000], Train Loss: 0.014637394808232784, Test Loss: 0.002516462467610836\n",
      "Epoch [1090/100000], Train Loss: 0.014727839268743992, Test Loss: 0.002363775623962283\n",
      "Epoch [1100/100000], Train Loss: 0.014489232562482357, Test Loss: 0.002340195467695594\n",
      "Epoch [1110/100000], Train Loss: 0.014437850564718246, Test Loss: 0.0023507934529334307\n",
      "Epoch [1120/100000], Train Loss: 0.013959957286715508, Test Loss: 0.0022568479180336\n",
      "Epoch [1130/100000], Train Loss: 0.014105566777288914, Test Loss: 0.0023115775547921658\n",
      "Epoch [1140/100000], Train Loss: 0.014306210912764072, Test Loss: 0.0022771060466766357\n",
      "Epoch [1150/100000], Train Loss: 0.014158906415104866, Test Loss: 0.002241335576400161\n",
      "Epoch [1160/100000], Train Loss: 0.014044343493878841, Test Loss: 0.0022430652752518654\n",
      "Epoch [1170/100000], Train Loss: 0.013964913785457611, Test Loss: 0.002259695902466774\n",
      "Epoch [1180/100000], Train Loss: 0.013754983432590961, Test Loss: 0.0021624199580401182\n",
      "Epoch [1190/100000], Train Loss: 0.01365796197205782, Test Loss: 0.002300998428836465\n",
      "Epoch [1200/100000], Train Loss: 0.013319868594408035, Test Loss: 0.0020603719167411327\n",
      "Epoch [1210/100000], Train Loss: 0.013624856248497963, Test Loss: 0.002219343790784478\n",
      "Epoch [1220/100000], Train Loss: 0.013178909197449684, Test Loss: 0.0020435189362615347\n",
      "Epoch [1230/100000], Train Loss: 0.013069379143416882, Test Loss: 0.002138865878805518\n",
      "Epoch [1240/100000], Train Loss: 0.012948163785040379, Test Loss: 0.002149759093299508\n",
      "Epoch [1250/100000], Train Loss: 0.012930241413414478, Test Loss: 0.0022159928921610117\n",
      "Epoch [1260/100000], Train Loss: 0.012681854888796806, Test Loss: 0.0020367219112813473\n",
      "Epoch [1270/100000], Train Loss: 0.01254448015242815, Test Loss: 0.0021027864422649145\n",
      "Epoch [1280/100000], Train Loss: 0.012464196421205997, Test Loss: 0.002100275829434395\n",
      "Epoch [1290/100000], Train Loss: 0.012214620597660542, Test Loss: 0.002082684077322483\n",
      "Epoch [1300/100000], Train Loss: 0.012454026378691196, Test Loss: 0.002073263516649604\n",
      "Epoch [1310/100000], Train Loss: 0.012399359606206417, Test Loss: 0.0021003084257245064\n",
      "Epoch [1320/100000], Train Loss: 0.012170289643108845, Test Loss: 0.0020118949469178915\n",
      "Epoch [1330/100000], Train Loss: 0.011927789077162743, Test Loss: 0.0020044236443936825\n",
      "Epoch [1340/100000], Train Loss: 0.011849352158606052, Test Loss: 0.0020726104266941547\n",
      "Epoch [1350/100000], Train Loss: 0.011385205201804638, Test Loss: 0.0019977877382189035\n",
      "Epoch [1360/100000], Train Loss: 0.01162013504654169, Test Loss: 0.0021204056683927774\n",
      "Epoch [1370/100000], Train Loss: 0.011630420573055744, Test Loss: 0.001991629833355546\n",
      "Epoch [1380/100000], Train Loss: 0.011671792715787888, Test Loss: 0.0020071743056178093\n",
      "Epoch [1390/100000], Train Loss: 0.01151194330304861, Test Loss: 0.0019455316942185163\n",
      "Epoch [1400/100000], Train Loss: 0.011253670789301395, Test Loss: 0.0020115261431783438\n",
      "Epoch [1410/100000], Train Loss: 0.011401920579373837, Test Loss: 0.0018921018345281482\n",
      "Epoch [1420/100000], Train Loss: 0.01092335395514965, Test Loss: 0.0019230538746342063\n",
      "Epoch [1430/100000], Train Loss: 0.010807287879288197, Test Loss: 0.0019542237278074026\n",
      "Epoch [1440/100000], Train Loss: 0.01088273711502552, Test Loss: 0.001951076672412455\n",
      "Epoch [1450/100000], Train Loss: 0.010765623301267624, Test Loss: 0.0019315107492730021\n",
      "Epoch [1460/100000], Train Loss: 0.010739069432020187, Test Loss: 0.0019373291870579123\n",
      "Epoch [1470/100000], Train Loss: 0.010620786808431149, Test Loss: 0.0019443195778876543\n",
      "Epoch [1480/100000], Train Loss: 0.010705088265240192, Test Loss: 0.001859310083091259\n",
      "Epoch [1490/100000], Train Loss: 0.010374916717410088, Test Loss: 0.0018591133411973715\n",
      "Epoch [1500/100000], Train Loss: 0.010199478827416897, Test Loss: 0.0018918763380497694\n",
      "Epoch [1510/100000], Train Loss: 0.010300794616341591, Test Loss: 0.0019521396607160568\n",
      "Epoch [1520/100000], Train Loss: 0.010035931132733822, Test Loss: 0.0019102245569229126\n",
      "Epoch [1530/100000], Train Loss: 0.010055233724415302, Test Loss: 0.0018961627501994371\n",
      "Epoch [1540/100000], Train Loss: 0.010228700004518032, Test Loss: 0.001906882505863905\n",
      "Epoch [1550/100000], Train Loss: 0.009761979803442955, Test Loss: 0.0017817189218476415\n",
      "Epoch [1560/100000], Train Loss: 0.009835558012127876, Test Loss: 0.001842369674704969\n",
      "Epoch [1570/100000], Train Loss: 0.009444489143788815, Test Loss: 0.0017880741506814957\n",
      "Epoch [1580/100000], Train Loss: 0.009445424191653728, Test Loss: 0.0017231048550456762\n",
      "Epoch [1590/100000], Train Loss: 0.00936345849186182, Test Loss: 0.0017549990443512797\n",
      "Epoch [1600/100000], Train Loss: 0.009526942856609821, Test Loss: 0.0016593188047409058\n",
      "Epoch [1610/100000], Train Loss: 0.009471897035837173, Test Loss: 0.001715265680104494\n",
      "Epoch [1620/100000], Train Loss: 0.009358678013086319, Test Loss: 0.0017095045186579227\n",
      "Epoch [1630/100000], Train Loss: 0.00898811873048544, Test Loss: 0.001683993497863412\n",
      "Epoch [1640/100000], Train Loss: 0.009052133187651634, Test Loss: 0.0016358875436708331\n",
      "Epoch [1650/100000], Train Loss: 0.008991613984107971, Test Loss: 0.0017176943365484476\n",
      "Epoch [1660/100000], Train Loss: 0.008739111945033073, Test Loss: 0.0016930721467360854\n",
      "Epoch [1670/100000], Train Loss: 0.008802233263850212, Test Loss: 0.001649503014050424\n",
      "Epoch [1680/100000], Train Loss: 0.008668174035847187, Test Loss: 0.0017158897826448083\n",
      "Epoch [1690/100000], Train Loss: 0.00871081929653883, Test Loss: 0.0017138593830168247\n",
      "Epoch [1700/100000], Train Loss: 0.008554277941584587, Test Loss: 0.0016232312191277742\n",
      "Epoch [1710/100000], Train Loss: 0.008544765412807465, Test Loss: 0.0016824246849864721\n",
      "Epoch [1720/100000], Train Loss: 0.008342375978827477, Test Loss: 0.0017292076954618096\n",
      "Epoch [1730/100000], Train Loss: 0.00841098465025425, Test Loss: 0.0017058782977983356\n",
      "Epoch [1740/100000], Train Loss: 0.008218624629080296, Test Loss: 0.00168282026425004\n",
      "Epoch [1750/100000], Train Loss: 0.008347242139279842, Test Loss: 0.0016794606344774365\n",
      "Epoch [1760/100000], Train Loss: 0.00826184544712305, Test Loss: 0.0017117777606472373\n",
      "Epoch [1770/100000], Train Loss: 0.008170274086296558, Test Loss: 0.0016820922028273344\n",
      "Epoch [1780/100000], Train Loss: 0.008170779794454575, Test Loss: 0.0017193619860336185\n",
      "Epoch [1790/100000], Train Loss: 0.007996942847967148, Test Loss: 0.0016927652759477496\n",
      "Epoch [1800/100000], Train Loss: 0.007840143516659737, Test Loss: 0.0017145663732662797\n",
      "Epoch [1810/100000], Train Loss: 0.007873320952057838, Test Loss: 0.0017621951410546899\n",
      "Epoch [1820/100000], Train Loss: 0.007904481142759323, Test Loss: 0.0016482928767800331\n",
      "Epoch [1830/100000], Train Loss: 0.007754442282021046, Test Loss: 0.0017779110930860043\n",
      "Epoch [1840/100000], Train Loss: 0.007419906090945005, Test Loss: 0.0017228701617568731\n",
      "Epoch [1850/100000], Train Loss: 0.007500502746552229, Test Loss: 0.0016819132724776864\n",
      "Epoch [1860/100000], Train Loss: 0.007422526367008686, Test Loss: 0.00169420859310776\n",
      "Epoch [1870/100000], Train Loss: 0.007399379275739193, Test Loss: 0.0017248213989660144\n",
      "Epoch [1880/100000], Train Loss: 0.007346153259277344, Test Loss: 0.0017180513823404908\n",
      "Epoch [1890/100000], Train Loss: 0.007418549619615078, Test Loss: 0.0017059996025636792\n",
      "Epoch [1900/100000], Train Loss: 0.007197434548288584, Test Loss: 0.0017851911252364516\n",
      "Epoch [1910/100000], Train Loss: 0.007260744925588369, Test Loss: 0.0016828281804919243\n",
      "Epoch [1920/100000], Train Loss: 0.0071617295034229755, Test Loss: 0.0016948228003457189\n",
      "Epoch [1930/100000], Train Loss: 0.00712923426181078, Test Loss: 0.0017905696295201778\n",
      "Epoch [1940/100000], Train Loss: 0.007083865813910961, Test Loss: 0.0016867516096681356\n",
      "Epoch [1950/100000], Train Loss: 0.006797103676944971, Test Loss: 0.0017378864577040076\n",
      "Epoch [1960/100000], Train Loss: 0.006904850713908672, Test Loss: 0.0017753157299011946\n",
      "Epoch [1970/100000], Train Loss: 0.0068484703078866005, Test Loss: 0.0016960620414465666\n",
      "Epoch [1980/100000], Train Loss: 0.006861736997961998, Test Loss: 0.0016861375188454986\n",
      "Epoch [1990/100000], Train Loss: 0.00662937993183732, Test Loss: 0.001649416983127594\n",
      "Epoch [2000/100000], Train Loss: 0.0065995692275464535, Test Loss: 0.0017057846998795867\n",
      "Epoch [2010/100000], Train Loss: 0.006678882986307144, Test Loss: 0.001753005082719028\n",
      "Epoch [2020/100000], Train Loss: 0.006604189518839121, Test Loss: 0.0017127145547419786\n",
      "Epoch [2030/100000], Train Loss: 0.006479725241661072, Test Loss: 0.001662599854171276\n",
      "Epoch [2040/100000], Train Loss: 0.006601157132536173, Test Loss: 0.0016419274033978581\n",
      "Epoch [2050/100000], Train Loss: 0.006259465124458075, Test Loss: 0.0016397731378674507\n",
      "Epoch [2060/100000], Train Loss: 0.006312014535069466, Test Loss: 0.0017065609572455287\n",
      "Epoch [2070/100000], Train Loss: 0.006480026990175247, Test Loss: 0.0016291422070935369\n",
      "Epoch [2080/100000], Train Loss: 0.006253223866224289, Test Loss: 0.001674229628406465\n",
      "Epoch [2090/100000], Train Loss: 0.006418400444090366, Test Loss: 0.0016982662491500378\n",
      "Epoch [2100/100000], Train Loss: 0.006124868988990784, Test Loss: 0.001703221001662314\n",
      "Epoch [2110/100000], Train Loss: 0.006149287801235914, Test Loss: 0.0017124875448644161\n",
      "Epoch [2120/100000], Train Loss: 0.006255908403545618, Test Loss: 0.0016315942630171776\n",
      "Epoch [2130/100000], Train Loss: 0.00619923509657383, Test Loss: 0.0016107292613014579\n",
      "Epoch [2140/100000], Train Loss: 0.005884301383048296, Test Loss: 0.0017427816055715084\n",
      "Epoch [2150/100000], Train Loss: 0.0061014313250780106, Test Loss: 0.0016285275341942906\n",
      "Epoch [2160/100000], Train Loss: 0.0058619300834834576, Test Loss: 0.001682864734902978\n",
      "Epoch [2170/100000], Train Loss: 0.00590556301176548, Test Loss: 0.0016625563148409128\n",
      "Epoch [2180/100000], Train Loss: 0.005873034708201885, Test Loss: 0.0016665784642100334\n",
      "Epoch [2190/100000], Train Loss: 0.005882428493350744, Test Loss: 0.001729836338199675\n",
      "Epoch [2200/100000], Train Loss: 0.005720357410609722, Test Loss: 0.0017071126494556665\n",
      "Epoch [2210/100000], Train Loss: 0.005722242407500744, Test Loss: 0.0016846276121214032\n",
      "Epoch [2220/100000], Train Loss: 0.005609293468296528, Test Loss: 0.0016693087527528405\n",
      "Epoch [2230/100000], Train Loss: 0.005656408611685038, Test Loss: 0.0016263537108898163\n",
      "Epoch [2240/100000], Train Loss: 0.005524810403585434, Test Loss: 0.0016051670536398888\n",
      "Epoch [2250/100000], Train Loss: 0.005576128140091896, Test Loss: 0.001707020914182067\n",
      "Epoch [2260/100000], Train Loss: 0.005593791604042053, Test Loss: 0.001654507126659155\n",
      "Epoch [2270/100000], Train Loss: 0.005566383711993694, Test Loss: 0.0017049245070666075\n",
      "Epoch [2280/100000], Train Loss: 0.005657000932842493, Test Loss: 0.0016662103589624166\n",
      "Epoch [2290/100000], Train Loss: 0.00548939686268568, Test Loss: 0.0016776967095211148\n",
      "Epoch [2300/100000], Train Loss: 0.00538063608109951, Test Loss: 0.0016758060082793236\n",
      "Epoch [2310/100000], Train Loss: 0.005341623909771442, Test Loss: 0.0016773779643699527\n",
      "Epoch [2320/100000], Train Loss: 0.005197925493121147, Test Loss: 0.0016986462287604809\n",
      "Epoch [2330/100000], Train Loss: 0.005313842091709375, Test Loss: 0.0016121611697599292\n",
      "Epoch [2340/100000], Train Loss: 0.0051851519383490086, Test Loss: 0.0016452227719128132\n",
      "Epoch [2350/100000], Train Loss: 0.005211175419390202, Test Loss: 0.0016887475503608584\n",
      "Epoch [2360/100000], Train Loss: 0.005253566894680262, Test Loss: 0.0016415307763963938\n",
      "Epoch [2370/100000], Train Loss: 0.005184980109333992, Test Loss: 0.0015570232644677162\n",
      "Epoch [2380/100000], Train Loss: 0.005340570118278265, Test Loss: 0.0016476758755743504\n",
      "Epoch [2390/100000], Train Loss: 0.005171629134565592, Test Loss: 0.001630408107303083\n",
      "Epoch [2400/100000], Train Loss: 0.00515299616381526, Test Loss: 0.0017102101119235158\n",
      "Epoch [2410/100000], Train Loss: 0.0052460068836808205, Test Loss: 0.0016908494289964437\n",
      "Epoch [2420/100000], Train Loss: 0.005045672878623009, Test Loss: 0.0017240545712411404\n",
      "Epoch [2430/100000], Train Loss: 0.0052589490078389645, Test Loss: 0.0016885580262169242\n",
      "Epoch [2440/100000], Train Loss: 0.0050694881938397884, Test Loss: 0.0016363502945750952\n",
      "Epoch [2450/100000], Train Loss: 0.0049248202703893185, Test Loss: 0.0017170642968267202\n",
      "Epoch [2460/100000], Train Loss: 0.00491362763568759, Test Loss: 0.0016801160527393222\n",
      "Epoch [2470/100000], Train Loss: 0.004727486055344343, Test Loss: 0.0015901263104751706\n",
      "Epoch [2480/100000], Train Loss: 0.005072366911917925, Test Loss: 0.0016399334417656064\n",
      "Epoch [2490/100000], Train Loss: 0.004957844503223896, Test Loss: 0.0016481972998008132\n",
      "Epoch [2500/100000], Train Loss: 0.00496535561978817, Test Loss: 0.0016827955842018127\n",
      "Epoch [2510/100000], Train Loss: 0.004947234410792589, Test Loss: 0.001670189667493105\n",
      "Epoch [2520/100000], Train Loss: 0.004905000329017639, Test Loss: 0.0017113436479121447\n",
      "Epoch [2530/100000], Train Loss: 0.005001928191632032, Test Loss: 0.0016765353502705693\n",
      "Epoch [2540/100000], Train Loss: 0.004904064815491438, Test Loss: 0.0016520237550139427\n",
      "Epoch [2550/100000], Train Loss: 0.004932930693030357, Test Loss: 0.0016784161562100053\n",
      "Epoch [2560/100000], Train Loss: 0.0048985728062689304, Test Loss: 0.0016782694729045033\n",
      "Epoch [2570/100000], Train Loss: 0.004794259089976549, Test Loss: 0.0016028756508603692\n",
      "Epoch [2580/100000], Train Loss: 0.004672903101891279, Test Loss: 0.0016968188574537635\n",
      "Epoch [2590/100000], Train Loss: 0.004672493319958448, Test Loss: 0.0016703616129234433\n",
      "Epoch [2600/100000], Train Loss: 0.004903830122202635, Test Loss: 0.0016335417749360204\n",
      "Epoch [2610/100000], Train Loss: 0.0045297653414309025, Test Loss: 0.0016292938962578773\n",
      "Epoch [2620/100000], Train Loss: 0.004731288645416498, Test Loss: 0.0014208854408934712\n",
      "Epoch [2630/100000], Train Loss: 0.004540947265923023, Test Loss: 0.0015516347484663129\n",
      "Epoch [2640/100000], Train Loss: 0.004493420477956533, Test Loss: 0.001408347743563354\n",
      "Epoch [2650/100000], Train Loss: 0.004478756804019213, Test Loss: 0.0014716926962137222\n",
      "Epoch [2660/100000], Train Loss: 0.004545675590634346, Test Loss: 0.0014494648203253746\n",
      "Epoch [2670/100000], Train Loss: 0.004553047474473715, Test Loss: 0.001489078626036644\n",
      "Epoch [2680/100000], Train Loss: 0.004610659554600716, Test Loss: 0.0014598650159314275\n",
      "Epoch [2690/100000], Train Loss: 0.004663811065256596, Test Loss: 0.0015045552281662822\n",
      "Epoch [2700/100000], Train Loss: 0.004620146006345749, Test Loss: 0.0015081317396834493\n",
      "Epoch [2710/100000], Train Loss: 0.004300977569073439, Test Loss: 0.0014301342889666557\n",
      "Epoch [2720/100000], Train Loss: 0.004372133873403072, Test Loss: 0.0014987682225182652\n",
      "Epoch [2730/100000], Train Loss: 0.004503455013036728, Test Loss: 0.0014746211236342788\n",
      "Epoch [2740/100000], Train Loss: 0.00462975213304162, Test Loss: 0.0014806116232648492\n",
      "Epoch [2750/100000], Train Loss: 0.004405818413943052, Test Loss: 0.0015255226753652096\n",
      "Epoch [2760/100000], Train Loss: 0.0044200182892382145, Test Loss: 0.001511337119154632\n",
      "Epoch [2770/100000], Train Loss: 0.004474901128560305, Test Loss: 0.001494527910836041\n",
      "Epoch [2780/100000], Train Loss: 0.004407012369483709, Test Loss: 0.0014528202591463923\n",
      "Epoch [2790/100000], Train Loss: 0.0043835993856191635, Test Loss: 0.0014580391580238938\n",
      "Epoch [2800/100000], Train Loss: 0.004446881357580423, Test Loss: 0.0014804275706410408\n",
      "Epoch [2810/100000], Train Loss: 0.004502301104366779, Test Loss: 0.0015179361216723919\n",
      "Epoch [2820/100000], Train Loss: 0.004277041647583246, Test Loss: 0.0015307944267988205\n",
      "Epoch [2830/100000], Train Loss: 0.004386797081679106, Test Loss: 0.0015093530528247356\n",
      "Epoch [2840/100000], Train Loss: 0.004322807304561138, Test Loss: 0.0014859933871775866\n",
      "Epoch [2850/100000], Train Loss: 0.004379108548164368, Test Loss: 0.0015072625828906894\n",
      "Epoch [2860/100000], Train Loss: 0.004282486625015736, Test Loss: 0.0014513744972646236\n",
      "Epoch [2870/100000], Train Loss: 0.004468751139938831, Test Loss: 0.0015812747878953815\n",
      "Epoch [2880/100000], Train Loss: 0.004342277999967337, Test Loss: 0.0015041690785437822\n",
      "Epoch [2890/100000], Train Loss: 0.004238331690430641, Test Loss: 0.0015816419618204236\n",
      "Epoch [2900/100000], Train Loss: 0.004200124181807041, Test Loss: 0.0014876252971589565\n",
      "Epoch [2910/100000], Train Loss: 0.004431848414242268, Test Loss: 0.001554795540869236\n",
      "Epoch [2920/100000], Train Loss: 0.004273147322237492, Test Loss: 0.001502563594840467\n",
      "Epoch [2930/100000], Train Loss: 0.004214343149214983, Test Loss: 0.0015080207958817482\n",
      "Epoch [2940/100000], Train Loss: 0.004251002334058285, Test Loss: 0.0015053274109959602\n",
      "Epoch [2950/100000], Train Loss: 0.004150800872594118, Test Loss: 0.0015641482314094901\n",
      "Epoch [2960/100000], Train Loss: 0.004119706340134144, Test Loss: 0.0014891965547576547\n",
      "Epoch [2970/100000], Train Loss: 0.004200348164886236, Test Loss: 0.0015516557032242417\n",
      "Epoch [2980/100000], Train Loss: 0.004139644093811512, Test Loss: 0.0015812946949154139\n",
      "Epoch [2990/100000], Train Loss: 0.004259415436536074, Test Loss: 0.0015083084581419826\n",
      "Epoch [3000/100000], Train Loss: 0.0043334863148629665, Test Loss: 0.001509430236183107\n",
      "Epoch [3010/100000], Train Loss: 0.004322612192481756, Test Loss: 0.0014800959033891559\n",
      "Epoch [3020/100000], Train Loss: 0.004288962110877037, Test Loss: 0.0015282423701137304\n",
      "Epoch [3030/100000], Train Loss: 0.004180458374321461, Test Loss: 0.0015215316088870168\n",
      "Epoch [3040/100000], Train Loss: 0.004263332113623619, Test Loss: 0.0014817172195762396\n",
      "Epoch [3050/100000], Train Loss: 0.004155872855335474, Test Loss: 0.0015411281492561102\n",
      "Epoch [3060/100000], Train Loss: 0.004119273275136948, Test Loss: 0.0015492831589654088\n",
      "Epoch [3070/100000], Train Loss: 0.0041459412313997746, Test Loss: 0.0015670666471123695\n",
      "Epoch [3080/100000], Train Loss: 0.0040527647361159325, Test Loss: 0.0015476974658668041\n",
      "Epoch [3090/100000], Train Loss: 0.004126138985157013, Test Loss: 0.0015221715439110994\n",
      "Epoch [3100/100000], Train Loss: 0.004245342221111059, Test Loss: 0.0015191339189186692\n",
      "Epoch [3110/100000], Train Loss: 0.004090461879968643, Test Loss: 0.0014559361152350903\n",
      "Epoch [3120/100000], Train Loss: 0.004145126789808273, Test Loss: 0.0014764443039894104\n",
      "Epoch [3130/100000], Train Loss: 0.0041840896010398865, Test Loss: 0.001506016356870532\n",
      "Epoch [3140/100000], Train Loss: 0.004104291088879108, Test Loss: 0.0015431519132107496\n",
      "Epoch [3150/100000], Train Loss: 0.004183415789157152, Test Loss: 0.0015028747729957104\n",
      "Epoch [3160/100000], Train Loss: 0.004171626642346382, Test Loss: 0.0015188080724328756\n",
      "Epoch [3170/100000], Train Loss: 0.004209253471344709, Test Loss: 0.0015394437359645963\n",
      "Epoch [3180/100000], Train Loss: 0.004060830920934677, Test Loss: 0.0015078696887940168\n",
      "Epoch [3190/100000], Train Loss: 0.004216495901346207, Test Loss: 0.001497486955486238\n",
      "Epoch [3200/100000], Train Loss: 0.004038511775434017, Test Loss: 0.0014418023638427258\n",
      "Epoch [3210/100000], Train Loss: 0.004011712037026882, Test Loss: 0.001558593357913196\n",
      "Epoch [3220/100000], Train Loss: 0.003976539708673954, Test Loss: 0.0015189810656011105\n",
      "Epoch [3230/100000], Train Loss: 0.004048481117933989, Test Loss: 0.0014662939356639981\n",
      "Epoch [3240/100000], Train Loss: 0.004061582963913679, Test Loss: 0.001461162231862545\n",
      "Epoch [3250/100000], Train Loss: 0.004011687356978655, Test Loss: 0.0015990854008123279\n",
      "Epoch [3260/100000], Train Loss: 0.004239395726472139, Test Loss: 0.00148186890874058\n",
      "Epoch [3270/100000], Train Loss: 0.00412384606897831, Test Loss: 0.0015165922231972218\n",
      "Epoch [3280/100000], Train Loss: 0.004033423960208893, Test Loss: 0.0016134657198563218\n",
      "Epoch [3290/100000], Train Loss: 0.004150039050728083, Test Loss: 0.001556696486659348\n",
      "Epoch [3300/100000], Train Loss: 0.003978709224611521, Test Loss: 0.001557235955260694\n",
      "Epoch [3310/100000], Train Loss: 0.004024034831672907, Test Loss: 0.001529058674350381\n",
      "Epoch [3320/100000], Train Loss: 0.003979913890361786, Test Loss: 0.0015266615664586425\n",
      "Epoch [3330/100000], Train Loss: 0.004052845761179924, Test Loss: 0.0014404983958229423\n",
      "Epoch [3340/100000], Train Loss: 0.004016566090285778, Test Loss: 0.0015270456206053495\n",
      "Epoch [3350/100000], Train Loss: 0.003956243395805359, Test Loss: 0.001503193168900907\n",
      "Epoch [3360/100000], Train Loss: 0.004095631185919046, Test Loss: 0.0014937033411115408\n",
      "Epoch [3370/100000], Train Loss: 0.00404901010915637, Test Loss: 0.0015644226223230362\n",
      "Epoch [3380/100000], Train Loss: 0.0039273519068956375, Test Loss: 0.0014707223745062947\n",
      "Epoch [3390/100000], Train Loss: 0.003906608559191227, Test Loss: 0.0015327328583225608\n",
      "Epoch [3400/100000], Train Loss: 0.00398558983579278, Test Loss: 0.0014574805973097682\n",
      "Epoch [3410/100000], Train Loss: 0.004108881577849388, Test Loss: 0.0014323184732347727\n",
      "Epoch [3420/100000], Train Loss: 0.004059093073010445, Test Loss: 0.001474119140766561\n",
      "Epoch [3430/100000], Train Loss: 0.003844578517600894, Test Loss: 0.0015279791550710797\n",
      "Epoch [3440/100000], Train Loss: 0.0038210246711969376, Test Loss: 0.0014742843341082335\n",
      "Epoch [3450/100000], Train Loss: 0.003910500090569258, Test Loss: 0.001404535141773522\n",
      "Epoch [3460/100000], Train Loss: 0.0038740725722163916, Test Loss: 0.0014370122225955129\n",
      "Epoch [3470/100000], Train Loss: 0.0038614000659435987, Test Loss: 0.0013565608533099294\n",
      "Epoch [3480/100000], Train Loss: 0.0038014960009604692, Test Loss: 0.0013797179562970996\n",
      "Epoch [3490/100000], Train Loss: 0.003900783369317651, Test Loss: 0.0013470369158312678\n",
      "Epoch [3500/100000], Train Loss: 0.003836312796920538, Test Loss: 0.001450197771191597\n",
      "Epoch [3510/100000], Train Loss: 0.0037131260614842176, Test Loss: 0.0013457442400977015\n",
      "Epoch [3520/100000], Train Loss: 0.0039196009747684, Test Loss: 0.0013980359071865678\n",
      "Epoch [3530/100000], Train Loss: 0.003805426647886634, Test Loss: 0.0013898721663281322\n",
      "Epoch [3540/100000], Train Loss: 0.0037861927412450314, Test Loss: 0.001415129634551704\n",
      "Epoch [3550/100000], Train Loss: 0.003745801979675889, Test Loss: 0.001397188170813024\n",
      "Epoch [3560/100000], Train Loss: 0.003700293367728591, Test Loss: 0.0013536300975829363\n",
      "Epoch [3570/100000], Train Loss: 0.0037557706236839294, Test Loss: 0.0013672675704583526\n",
      "Epoch [3580/100000], Train Loss: 0.0038839424960315228, Test Loss: 0.001356600783765316\n",
      "Epoch [3590/100000], Train Loss: 0.0038424194790422916, Test Loss: 0.0013570002047345042\n",
      "Epoch [3600/100000], Train Loss: 0.003832652233541012, Test Loss: 0.0013026285450905561\n",
      "Epoch [3610/100000], Train Loss: 0.0038318829610943794, Test Loss: 0.001396857318468392\n",
      "Epoch [3620/100000], Train Loss: 0.003732598153874278, Test Loss: 0.0013976505724713206\n",
      "Epoch [3630/100000], Train Loss: 0.003752965247258544, Test Loss: 0.001456974190659821\n",
      "Epoch [3640/100000], Train Loss: 0.0037927457597106695, Test Loss: 0.001346134697087109\n",
      "Epoch [3650/100000], Train Loss: 0.003951196558773518, Test Loss: 0.0014621821464970708\n",
      "Epoch [3660/100000], Train Loss: 0.0037119141779839993, Test Loss: 0.0013422234915196896\n",
      "Epoch [3670/100000], Train Loss: 0.0036558995489031076, Test Loss: 0.0013847565278410912\n",
      "Epoch [3680/100000], Train Loss: 0.003667413955554366, Test Loss: 0.0013334231916815042\n",
      "Epoch [3690/100000], Train Loss: 0.0038946475833654404, Test Loss: 0.0013302081497386098\n",
      "Epoch [3700/100000], Train Loss: 0.003804071107879281, Test Loss: 0.0013796973507851362\n",
      "Epoch [3710/100000], Train Loss: 0.00375719484873116, Test Loss: 0.0013697466347366571\n",
      "Epoch [3720/100000], Train Loss: 0.003826551605015993, Test Loss: 0.001296205329708755\n",
      "Epoch [3730/100000], Train Loss: 0.0036967131309211254, Test Loss: 0.001341787283308804\n",
      "Epoch [3740/100000], Train Loss: 0.0038615847006440163, Test Loss: 0.001452157972380519\n",
      "Epoch [3750/100000], Train Loss: 0.003687168937176466, Test Loss: 0.0014240635791793466\n",
      "Epoch [3760/100000], Train Loss: 0.003998285159468651, Test Loss: 0.0013172014150768518\n",
      "Epoch [3770/100000], Train Loss: 0.003771781222894788, Test Loss: 0.0013363667530938983\n",
      "Epoch [3780/100000], Train Loss: 0.0036098072305321693, Test Loss: 0.0012676621554419398\n",
      "Epoch [3790/100000], Train Loss: 0.003709463169798255, Test Loss: 0.001376507105305791\n",
      "Epoch [3800/100000], Train Loss: 0.0038866412360221148, Test Loss: 0.0014087818562984467\n",
      "Epoch [3810/100000], Train Loss: 0.0037690645549446344, Test Loss: 0.001336118672043085\n",
      "Epoch [3820/100000], Train Loss: 0.003753481898456812, Test Loss: 0.0014012210303917527\n",
      "Epoch [3830/100000], Train Loss: 0.003906724043190479, Test Loss: 0.0013926028041169047\n",
      "Epoch [3840/100000], Train Loss: 0.0038139785174280405, Test Loss: 0.0014502431731671095\n",
      "Epoch [3850/100000], Train Loss: 0.003994596190750599, Test Loss: 0.0013677185634151101\n",
      "Epoch [3860/100000], Train Loss: 0.003620568197220564, Test Loss: 0.001301274634897709\n",
      "Epoch [3870/100000], Train Loss: 0.003711779834702611, Test Loss: 0.001373484032228589\n",
      "Epoch [3880/100000], Train Loss: 0.003641346003860235, Test Loss: 0.0013495394960045815\n",
      "Epoch [3890/100000], Train Loss: 0.0036835307255387306, Test Loss: 0.0014235003618523479\n",
      "Epoch [3900/100000], Train Loss: 0.0036991052329540253, Test Loss: 0.00139689224306494\n",
      "Epoch [3910/100000], Train Loss: 0.003609798150137067, Test Loss: 0.001350792939774692\n",
      "Epoch [3920/100000], Train Loss: 0.0037569815758615732, Test Loss: 0.0013535916805267334\n",
      "Epoch [3930/100000], Train Loss: 0.003747619455680251, Test Loss: 0.0014550727792084217\n",
      "Epoch [3940/100000], Train Loss: 0.00372317829169333, Test Loss: 0.0013216041261330247\n",
      "Epoch [3950/100000], Train Loss: 0.003932831343263388, Test Loss: 0.0013478311011567712\n",
      "Epoch [3960/100000], Train Loss: 0.0036635007709264755, Test Loss: 0.0013239390682429075\n",
      "Epoch [3970/100000], Train Loss: 0.003769778646528721, Test Loss: 0.00132624467369169\n",
      "Epoch [3980/100000], Train Loss: 0.003787017660215497, Test Loss: 0.0013328082859516144\n",
      "Epoch [3990/100000], Train Loss: 0.0037912505213171244, Test Loss: 0.0014400362269952893\n",
      "Epoch [4000/100000], Train Loss: 0.0037044030614197254, Test Loss: 0.0013049440458416939\n",
      "Epoch [4010/100000], Train Loss: 0.003815882373601198, Test Loss: 0.001418850151821971\n",
      "Epoch [4020/100000], Train Loss: 0.0038889823481440544, Test Loss: 0.0014298828318715096\n",
      "Epoch [4030/100000], Train Loss: 0.0036466342862695456, Test Loss: 0.0013969703577458858\n",
      "Epoch [4040/100000], Train Loss: 0.003799993544816971, Test Loss: 0.0013508587144315243\n",
      "Epoch [4050/100000], Train Loss: 0.0038103843107819557, Test Loss: 0.0013822331093251705\n",
      "Epoch [4060/100000], Train Loss: 0.0035335293505340815, Test Loss: 0.0014228301588445902\n",
      "Epoch [4070/100000], Train Loss: 0.003652866929769516, Test Loss: 0.0013606295688077807\n",
      "Epoch [4080/100000], Train Loss: 0.0038388813845813274, Test Loss: 0.0013003062922507524\n",
      "Epoch [4090/100000], Train Loss: 0.003818476339802146, Test Loss: 0.0013612833572551608\n",
      "Epoch [4100/100000], Train Loss: 0.003661011578515172, Test Loss: 0.001335266511887312\n",
      "Epoch [4110/100000], Train Loss: 0.003756995778530836, Test Loss: 0.001423541340045631\n",
      "Epoch [4120/100000], Train Loss: 0.0037021273747086525, Test Loss: 0.0013546928530558944\n",
      "Epoch [4130/100000], Train Loss: 0.0037308132741600275, Test Loss: 0.0013256705133244395\n",
      "Epoch [4140/100000], Train Loss: 0.0036694728769361973, Test Loss: 0.001400795066729188\n",
      "Epoch [4150/100000], Train Loss: 0.003833556082099676, Test Loss: 0.0013441104674711823\n",
      "Epoch [4160/100000], Train Loss: 0.003628379199653864, Test Loss: 0.0013374116970226169\n",
      "Epoch [4170/100000], Train Loss: 0.0036950604990124702, Test Loss: 0.0014134987723082304\n",
      "Epoch [4180/100000], Train Loss: 0.0037281387485563755, Test Loss: 0.0013412272091954947\n",
      "Epoch [4190/100000], Train Loss: 0.003795756259933114, Test Loss: 0.001348252291791141\n",
      "Epoch [4200/100000], Train Loss: 0.00386889954097569, Test Loss: 0.0013359638396650553\n",
      "Epoch [4210/100000], Train Loss: 0.00367908738553524, Test Loss: 0.0013982235686853528\n",
      "Epoch [4220/100000], Train Loss: 0.003698798594996333, Test Loss: 0.0014725603396072984\n",
      "Epoch [4230/100000], Train Loss: 0.0036997906863689423, Test Loss: 0.0013549986761063337\n",
      "Epoch [4240/100000], Train Loss: 0.0037141437642276287, Test Loss: 0.0012886556796729565\n",
      "Epoch [4250/100000], Train Loss: 0.003601192496716976, Test Loss: 0.0013096557231619954\n",
      "Epoch [4260/100000], Train Loss: 0.0036666064988821745, Test Loss: 0.0013575800694525242\n",
      "Epoch [4270/100000], Train Loss: 0.0037394692189991474, Test Loss: 0.0013379284646362066\n",
      "Epoch [4280/100000], Train Loss: 0.0037611243315041065, Test Loss: 0.0013652907218784094\n",
      "Epoch [4290/100000], Train Loss: 0.0037779435515403748, Test Loss: 0.0013613372575491667\n",
      "Epoch [4300/100000], Train Loss: 0.0036569663789123297, Test Loss: 0.0013809630181640387\n",
      "Epoch [4310/100000], Train Loss: 0.0036834338679909706, Test Loss: 0.0013388046063482761\n",
      "Epoch [4320/100000], Train Loss: 0.003610536688938737, Test Loss: 0.0013239431427791715\n",
      "Epoch [4330/100000], Train Loss: 0.0035853495355695486, Test Loss: 0.0013973289169371128\n",
      "Epoch [4340/100000], Train Loss: 0.003640342503786087, Test Loss: 0.0013813958503305912\n",
      "Epoch [4350/100000], Train Loss: 0.0035779441241174936, Test Loss: 0.00133642612490803\n",
      "Epoch [4360/100000], Train Loss: 0.003822728293016553, Test Loss: 0.0013432040577754378\n",
      "Epoch [4370/100000], Train Loss: 0.0037690941244363785, Test Loss: 0.00128605414647609\n",
      "Epoch [4380/100000], Train Loss: 0.003810297232121229, Test Loss: 0.0013620806857943535\n",
      "Epoch [4390/100000], Train Loss: 0.0035449685528874397, Test Loss: 0.0013610763708129525\n",
      "Epoch [4400/100000], Train Loss: 0.003744045738130808, Test Loss: 0.0014506245497614145\n",
      "Epoch [4410/100000], Train Loss: 0.0036829824093729258, Test Loss: 0.001343900803476572\n",
      "Epoch [4420/100000], Train Loss: 0.0037626877892762423, Test Loss: 0.0014370310818776488\n",
      "Epoch [4430/100000], Train Loss: 0.003700406290590763, Test Loss: 0.001358363893814385\n",
      "Epoch [4440/100000], Train Loss: 0.0035129983443766832, Test Loss: 0.0013223568676039577\n",
      "Epoch [4450/100000], Train Loss: 0.00367817678488791, Test Loss: 0.0013427861267700791\n",
      "Epoch [4460/100000], Train Loss: 0.003700656583532691, Test Loss: 0.001330988947302103\n",
      "Epoch [4470/100000], Train Loss: 0.003671825397759676, Test Loss: 0.0013305285247042775\n",
      "Epoch [4480/100000], Train Loss: 0.0037425016053020954, Test Loss: 0.0013748699566349387\n",
      "Epoch [4490/100000], Train Loss: 0.0037204409018158913, Test Loss: 0.0013253248762339354\n",
      "Epoch [4500/100000], Train Loss: 0.003518333425745368, Test Loss: 0.0012817755341529846\n",
      "Epoch [4510/100000], Train Loss: 0.003528202883899212, Test Loss: 0.0013502968940883875\n",
      "Epoch [4520/100000], Train Loss: 0.0036824580747634172, Test Loss: 0.0013729602796956897\n",
      "Epoch [4530/100000], Train Loss: 0.0037062664050608873, Test Loss: 0.0013132485328242183\n",
      "Epoch [4540/100000], Train Loss: 0.003596169175580144, Test Loss: 0.0012526027858257294\n",
      "Epoch [4550/100000], Train Loss: 0.0036023256834596395, Test Loss: 0.001361506525427103\n",
      "Epoch [4560/100000], Train Loss: 0.003657225053757429, Test Loss: 0.0013186598662286997\n",
      "Epoch [4570/100000], Train Loss: 0.0036614825949072838, Test Loss: 0.0012955639977008104\n",
      "Epoch [4580/100000], Train Loss: 0.0038309914525598288, Test Loss: 0.0013337359996512532\n",
      "Epoch [4590/100000], Train Loss: 0.00372687797062099, Test Loss: 0.0013291561044752598\n",
      "Epoch [4600/100000], Train Loss: 0.0036705229431390762, Test Loss: 0.0013131425948813558\n",
      "Epoch [4610/100000], Train Loss: 0.0036648150999099016, Test Loss: 0.0013726946199312806\n",
      "Epoch [4620/100000], Train Loss: 0.0037668461445719004, Test Loss: 0.0013764911564067006\n",
      "Epoch [4630/100000], Train Loss: 0.003783920081332326, Test Loss: 0.0013139386428520083\n",
      "Epoch [4640/100000], Train Loss: 0.003643099218606949, Test Loss: 0.0013759596040472388\n",
      "Epoch [4650/100000], Train Loss: 0.0037952230777591467, Test Loss: 0.0012808878673240542\n",
      "Epoch [4660/100000], Train Loss: 0.003643278032541275, Test Loss: 0.0013486112002283335\n",
      "Epoch [4670/100000], Train Loss: 0.0037297646049410105, Test Loss: 0.0013956512557342649\n",
      "Epoch [4680/100000], Train Loss: 0.003695716615766287, Test Loss: 0.0012412182986736298\n",
      "Epoch [4690/100000], Train Loss: 0.0035521485842764378, Test Loss: 0.0014054217608645558\n",
      "Epoch [4700/100000], Train Loss: 0.0036153984256088734, Test Loss: 0.001305881654843688\n",
      "Epoch [4710/100000], Train Loss: 0.0036793118342757225, Test Loss: 0.0013764923205599189\n",
      "Epoch [4720/100000], Train Loss: 0.003522801212966442, Test Loss: 0.0012879578862339258\n",
      "Epoch [4730/100000], Train Loss: 0.0036658416502177715, Test Loss: 0.001331157749518752\n",
      "Epoch [4740/100000], Train Loss: 0.0036559647414833307, Test Loss: 0.0012475012335926294\n",
      "Epoch [4750/100000], Train Loss: 0.003641406772658229, Test Loss: 0.001368512399494648\n",
      "Epoch [4760/100000], Train Loss: 0.003639812581241131, Test Loss: 0.0014057069784030318\n",
      "Epoch [4770/100000], Train Loss: 0.0036388440057635307, Test Loss: 0.001347147161141038\n",
      "Epoch [4780/100000], Train Loss: 0.0036508559715002775, Test Loss: 0.0013314306270331144\n",
      "Epoch [4790/100000], Train Loss: 0.0035668357741087675, Test Loss: 0.0012975945137441158\n",
      "Epoch [4800/100000], Train Loss: 0.003564364742487669, Test Loss: 0.0013713176595047116\n",
      "Epoch [4810/100000], Train Loss: 0.003562609199434519, Test Loss: 0.0012824496952816844\n",
      "Epoch [4820/100000], Train Loss: 0.0036436154041439295, Test Loss: 0.0014261739561334252\n",
      "Epoch [4830/100000], Train Loss: 0.0036950877401977777, Test Loss: 0.0013097503688186407\n",
      "Epoch [4840/100000], Train Loss: 0.0036698828916996717, Test Loss: 0.0013119472423568368\n",
      "Epoch [4850/100000], Train Loss: 0.003735267324373126, Test Loss: 0.0012648887932300568\n",
      "Epoch [4860/100000], Train Loss: 0.0035463180392980576, Test Loss: 0.001330674858763814\n",
      "Epoch [4870/100000], Train Loss: 0.003532351227477193, Test Loss: 0.0012892447412014008\n",
      "Epoch [4880/100000], Train Loss: 0.0037125595845282078, Test Loss: 0.0013242713175714016\n",
      "Epoch [4890/100000], Train Loss: 0.0037628095597028732, Test Loss: 0.001319841481745243\n",
      "Epoch [4900/100000], Train Loss: 0.003507401095703244, Test Loss: 0.0013312853407114744\n",
      "Epoch [4910/100000], Train Loss: 0.003672198858112097, Test Loss: 0.0013028231915086508\n",
      "Epoch [4920/100000], Train Loss: 0.003640271257609129, Test Loss: 0.0012932963436469436\n",
      "Epoch [4930/100000], Train Loss: 0.0036324237007647753, Test Loss: 0.001279019983485341\n",
      "Epoch [4940/100000], Train Loss: 0.003722859313711524, Test Loss: 0.0013471911661326885\n",
      "Epoch [4950/100000], Train Loss: 0.0036024139262735844, Test Loss: 0.0012408348266035318\n",
      "Epoch [4960/100000], Train Loss: 0.003655452746897936, Test Loss: 0.0014536358648911119\n",
      "Epoch [4970/100000], Train Loss: 0.0036723546218127012, Test Loss: 0.0012904602335765958\n",
      "Epoch [4980/100000], Train Loss: 0.0035439403727650642, Test Loss: 0.0013545275432989001\n",
      "Epoch [4990/100000], Train Loss: 0.0037970589473843575, Test Loss: 0.0012727453140541911\n",
      "Epoch [5000/100000], Train Loss: 0.003504827618598938, Test Loss: 0.0012561359908431768\n",
      "Epoch [5010/100000], Train Loss: 0.0035146414302289486, Test Loss: 0.0012939419830217957\n",
      "Epoch [5020/100000], Train Loss: 0.003583563957363367, Test Loss: 0.001329104765318334\n",
      "Epoch [5030/100000], Train Loss: 0.0035225115716457367, Test Loss: 0.0013050355482846498\n",
      "Epoch [5040/100000], Train Loss: 0.003473366843536496, Test Loss: 0.0012540376046672463\n",
      "Epoch [5050/100000], Train Loss: 0.0035986402072012424, Test Loss: 0.001314040506258607\n",
      "Epoch [5060/100000], Train Loss: 0.00351755996234715, Test Loss: 0.0012933292891830206\n",
      "Epoch [5070/100000], Train Loss: 0.003685730742290616, Test Loss: 0.001286034006625414\n",
      "Epoch [5080/100000], Train Loss: 0.0036585871130228043, Test Loss: 0.001280485070310533\n",
      "Epoch [5090/100000], Train Loss: 0.0034978326875716448, Test Loss: 0.0012893343809992075\n",
      "Epoch [5100/100000], Train Loss: 0.0037192641757428646, Test Loss: 0.001281988457776606\n",
      "Epoch [5110/100000], Train Loss: 0.003582655917853117, Test Loss: 0.0013210669858381152\n",
      "Epoch [5120/100000], Train Loss: 0.0034428576473146677, Test Loss: 0.0012875470565631986\n",
      "Epoch [5130/100000], Train Loss: 0.003579195821657777, Test Loss: 0.0012791157932952046\n",
      "Epoch [5140/100000], Train Loss: 0.003798806807026267, Test Loss: 0.0013293226948007941\n",
      "Epoch [5150/100000], Train Loss: 0.003543626284226775, Test Loss: 0.0012104593915864825\n",
      "Epoch [5160/100000], Train Loss: 0.00360456807538867, Test Loss: 0.0012779304524883628\n",
      "Epoch [5170/100000], Train Loss: 0.0034861464519053698, Test Loss: 0.0013413641136139631\n",
      "Epoch [5180/100000], Train Loss: 0.003549490822479129, Test Loss: 0.001260379794985056\n",
      "Epoch [5190/100000], Train Loss: 0.003556309500709176, Test Loss: 0.0012269325088709593\n",
      "Epoch [5200/100000], Train Loss: 0.0036085478495806456, Test Loss: 0.0013393310364335775\n",
      "Epoch [5210/100000], Train Loss: 0.0036548522766679525, Test Loss: 0.0013886024244129658\n",
      "Epoch [5220/100000], Train Loss: 0.003515231888741255, Test Loss: 0.001208530506119132\n",
      "Epoch [5230/100000], Train Loss: 0.003747727023437619, Test Loss: 0.001312444219365716\n",
      "Epoch [5240/100000], Train Loss: 0.003555192146450281, Test Loss: 0.0011531964410096407\n",
      "Epoch [5250/100000], Train Loss: 0.00355132226832211, Test Loss: 0.0013567028800025582\n",
      "Epoch [5260/100000], Train Loss: 0.0036121790762990713, Test Loss: 0.0013594848569482565\n",
      "Epoch [5270/100000], Train Loss: 0.0036816573701798916, Test Loss: 0.001340172253549099\n",
      "Epoch [5280/100000], Train Loss: 0.0034847045317292213, Test Loss: 0.0012430360075086355\n",
      "Epoch [5290/100000], Train Loss: 0.0034762932918965816, Test Loss: 0.0011920424876734614\n",
      "Epoch [5300/100000], Train Loss: 0.0034954447764903307, Test Loss: 0.001072825863957405\n",
      "Epoch [5310/100000], Train Loss: 0.003508866997435689, Test Loss: 0.001182034146040678\n",
      "Epoch [5320/100000], Train Loss: 0.0033269785344600677, Test Loss: 0.0011589870555326343\n",
      "Epoch [5330/100000], Train Loss: 0.003474719123914838, Test Loss: 0.0011836457997560501\n",
      "Epoch [5340/100000], Train Loss: 0.003369707614183426, Test Loss: 0.0011514369398355484\n",
      "Epoch [5350/100000], Train Loss: 0.003411752637475729, Test Loss: 0.001123897498473525\n",
      "Epoch [5360/100000], Train Loss: 0.003305575577542186, Test Loss: 0.001175326993688941\n",
      "Epoch [5370/100000], Train Loss: 0.0034743989817798138, Test Loss: 0.0011404104297980666\n",
      "Epoch [5380/100000], Train Loss: 0.0032253852114081383, Test Loss: 0.001183520071208477\n",
      "Epoch [5390/100000], Train Loss: 0.003308817744255066, Test Loss: 0.0011490092147141695\n",
      "Epoch [5400/100000], Train Loss: 0.003206309163942933, Test Loss: 0.0010730224894359708\n",
      "Epoch [5410/100000], Train Loss: 0.0033619320020079613, Test Loss: 0.0010806258069351315\n",
      "Epoch [5420/100000], Train Loss: 0.003302483819425106, Test Loss: 0.0010995263000950217\n",
      "Epoch [5430/100000], Train Loss: 0.0033453807700425386, Test Loss: 0.0011265954235568643\n",
      "Epoch [5440/100000], Train Loss: 0.003391008358448744, Test Loss: 0.0010664059082046151\n",
      "Epoch [5450/100000], Train Loss: 0.003319116774946451, Test Loss: 0.0011078917887061834\n",
      "Epoch [5460/100000], Train Loss: 0.0032146114390343428, Test Loss: 0.001055898959748447\n",
      "Epoch [5470/100000], Train Loss: 0.003276776522397995, Test Loss: 0.001163398614153266\n",
      "Epoch [5480/100000], Train Loss: 0.0033980999141931534, Test Loss: 0.0011067390441894531\n",
      "Epoch [5490/100000], Train Loss: 0.0032556718215346336, Test Loss: 0.0010842790361493826\n",
      "Epoch [5500/100000], Train Loss: 0.003273916430771351, Test Loss: 0.001169788301922381\n",
      "Epoch [5510/100000], Train Loss: 0.0033479768317192793, Test Loss: 0.0011849096044898033\n",
      "Epoch [5520/100000], Train Loss: 0.00322731607593596, Test Loss: 0.0009688354330137372\n",
      "Epoch [5530/100000], Train Loss: 0.0031692369375377893, Test Loss: 0.0012423137668520212\n",
      "Epoch [5540/100000], Train Loss: 0.0032853535376489162, Test Loss: 0.0011795629980042577\n",
      "Epoch [5550/100000], Train Loss: 0.0034092345740646124, Test Loss: 0.0010852349223569036\n",
      "Epoch [5560/100000], Train Loss: 0.0033510366920381784, Test Loss: 0.0011118194088339806\n",
      "Epoch [5570/100000], Train Loss: 0.003331233048811555, Test Loss: 0.0010589128360152245\n",
      "Epoch [5580/100000], Train Loss: 0.0032922769896686077, Test Loss: 0.0010645719012245536\n",
      "Epoch [5590/100000], Train Loss: 0.0033331606537103653, Test Loss: 0.0011972486972808838\n",
      "Epoch [5600/100000], Train Loss: 0.0031383156310766935, Test Loss: 0.001147876842878759\n",
      "Epoch [5610/100000], Train Loss: 0.0033695257734507322, Test Loss: 0.0010678895050659776\n",
      "Epoch [5620/100000], Train Loss: 0.0031379235442727804, Test Loss: 0.001165670808404684\n",
      "Epoch [5630/100000], Train Loss: 0.0031338727567344904, Test Loss: 0.001049675396643579\n",
      "Epoch [5640/100000], Train Loss: 0.0033355143386870623, Test Loss: 0.0011119528207927942\n",
      "Epoch [5650/100000], Train Loss: 0.0031497536692768335, Test Loss: 0.001150229130871594\n",
      "Epoch [5660/100000], Train Loss: 0.0032926429994404316, Test Loss: 0.0010981963714584708\n",
      "Epoch [5670/100000], Train Loss: 0.003108932403847575, Test Loss: 0.0011443047551438212\n",
      "Epoch [5680/100000], Train Loss: 0.0032422214280813932, Test Loss: 0.0010911183198913932\n",
      "Epoch [5690/100000], Train Loss: 0.003281697165220976, Test Loss: 0.0011661055032163858\n",
      "Epoch [5700/100000], Train Loss: 0.0033279627095907927, Test Loss: 0.0010786745697259903\n",
      "Epoch [5710/100000], Train Loss: 0.0032070516608655453, Test Loss: 0.0011092106578871608\n",
      "Epoch [5720/100000], Train Loss: 0.003260386409237981, Test Loss: 0.0011200008448213339\n",
      "Epoch [5730/100000], Train Loss: 0.003310517640784383, Test Loss: 0.0010602594120427966\n",
      "Epoch [5740/100000], Train Loss: 0.003230865579098463, Test Loss: 0.0010391117539256811\n",
      "Epoch [5750/100000], Train Loss: 0.003231634618714452, Test Loss: 0.0010940601350739598\n",
      "Epoch [5760/100000], Train Loss: 0.003284541890025139, Test Loss: 0.000999827985651791\n",
      "Epoch [5770/100000], Train Loss: 0.003244713181629777, Test Loss: 0.0010725206229835749\n",
      "Epoch [5780/100000], Train Loss: 0.003243271494284272, Test Loss: 0.001091018202714622\n",
      "Epoch [5790/100000], Train Loss: 0.0033438147511333227, Test Loss: 0.001111632795073092\n",
      "Epoch [5800/100000], Train Loss: 0.003259961260482669, Test Loss: 0.0010267416946589947\n",
      "Epoch [5810/100000], Train Loss: 0.0032840482890605927, Test Loss: 0.0011014393530786037\n",
      "Epoch [5820/100000], Train Loss: 0.0031594447791576385, Test Loss: 0.0010956840123981237\n",
      "Epoch [5830/100000], Train Loss: 0.0033193493727594614, Test Loss: 0.001134627964347601\n",
      "Epoch [5840/100000], Train Loss: 0.003227907232940197, Test Loss: 0.0011221746681258082\n",
      "Epoch [5850/100000], Train Loss: 0.0031809278298169374, Test Loss: 0.0010072434088215232\n",
      "Epoch [5860/100000], Train Loss: 0.0032090407330542803, Test Loss: 0.0009910889202728868\n",
      "Epoch [5870/100000], Train Loss: 0.0031705782748758793, Test Loss: 0.001108092605136335\n",
      "Epoch [5880/100000], Train Loss: 0.0032922413665801287, Test Loss: 0.0010850538965314627\n",
      "Epoch [5890/100000], Train Loss: 0.00324618024751544, Test Loss: 0.0010684613371267915\n",
      "Epoch [5900/100000], Train Loss: 0.0032402866054326296, Test Loss: 0.001097732805646956\n",
      "Epoch [5910/100000], Train Loss: 0.0031884871423244476, Test Loss: 0.0010546520352363586\n",
      "Epoch [5920/100000], Train Loss: 0.003177021164447069, Test Loss: 0.0010511605069041252\n",
      "Epoch [5930/100000], Train Loss: 0.0031779110431671143, Test Loss: 0.0011572533985599875\n",
      "Epoch [5940/100000], Train Loss: 0.0032844343222677708, Test Loss: 0.0009429327328689396\n",
      "Epoch [5950/100000], Train Loss: 0.0032231113873422146, Test Loss: 0.0009773214114829898\n",
      "Epoch [5960/100000], Train Loss: 0.0032114849891513586, Test Loss: 0.0010391923133283854\n",
      "Epoch [5970/100000], Train Loss: 0.003258679760619998, Test Loss: 0.001118016429245472\n",
      "Epoch [5980/100000], Train Loss: 0.0030186441726982594, Test Loss: 0.0011264394270256162\n",
      "Epoch [5990/100000], Train Loss: 0.003157255472615361, Test Loss: 0.0009696452179923654\n",
      "Epoch [6000/100000], Train Loss: 0.003351985476911068, Test Loss: 0.0010923586087301373\n",
      "Epoch [6010/100000], Train Loss: 0.0033835936337709427, Test Loss: 0.0010583276161924005\n",
      "Epoch [6020/100000], Train Loss: 0.003246176987886429, Test Loss: 0.0010302739683538675\n",
      "Epoch [6030/100000], Train Loss: 0.0031586652621626854, Test Loss: 0.0010446759406477213\n",
      "Epoch [6040/100000], Train Loss: 0.003226347966119647, Test Loss: 0.0010413099080324173\n",
      "Epoch [6050/100000], Train Loss: 0.00318229291588068, Test Loss: 0.000983935664407909\n",
      "Epoch [6060/100000], Train Loss: 0.0031701712869107723, Test Loss: 0.0010445315856486559\n",
      "Epoch [6070/100000], Train Loss: 0.0031228680163621902, Test Loss: 0.0009744127164594829\n",
      "Epoch [6080/100000], Train Loss: 0.0030950596556067467, Test Loss: 0.001059665810316801\n",
      "Epoch [6090/100000], Train Loss: 0.0032551733311265707, Test Loss: 0.0010229782201349735\n",
      "Epoch [6100/100000], Train Loss: 0.003285191021859646, Test Loss: 0.0010725854663178325\n",
      "Epoch [6110/100000], Train Loss: 0.0030486800242215395, Test Loss: 0.0010465119266882539\n",
      "Epoch [6120/100000], Train Loss: 0.0031831085216253996, Test Loss: 0.001017055124975741\n",
      "Epoch [6130/100000], Train Loss: 0.0031323146540671587, Test Loss: 0.0010676314122974873\n",
      "Epoch [6140/100000], Train Loss: 0.003276754403486848, Test Loss: 0.0009974036365747452\n",
      "Epoch [6150/100000], Train Loss: 0.0032005896791815758, Test Loss: 0.001074009109288454\n",
      "Epoch [6160/100000], Train Loss: 0.0032391443382948637, Test Loss: 0.000994814676232636\n",
      "Epoch [6170/100000], Train Loss: 0.003144084010273218, Test Loss: 0.001064757350832224\n",
      "Epoch [6180/100000], Train Loss: 0.0032577980309724808, Test Loss: 0.0010774109978228807\n",
      "Epoch [6190/100000], Train Loss: 0.0031496428418904543, Test Loss: 0.0009556511067785323\n",
      "Epoch [6200/100000], Train Loss: 0.0031435673590749502, Test Loss: 0.0009929658845067024\n",
      "Epoch [6210/100000], Train Loss: 0.0032014502212405205, Test Loss: 0.001038788934238255\n",
      "Epoch [6220/100000], Train Loss: 0.003249733941629529, Test Loss: 0.0010502993827685714\n",
      "Epoch [6230/100000], Train Loss: 0.003235451178625226, Test Loss: 0.001003239769488573\n",
      "Epoch [6240/100000], Train Loss: 0.0032884525135159492, Test Loss: 0.0011030135210603476\n",
      "Epoch [6250/100000], Train Loss: 0.003295106114819646, Test Loss: 0.0009921257151290774\n",
      "Epoch [6260/100000], Train Loss: 0.0033016386441886425, Test Loss: 0.0009949365630745888\n",
      "Epoch [6270/100000], Train Loss: 0.003266687272116542, Test Loss: 0.0010007225209847093\n",
      "Epoch [6280/100000], Train Loss: 0.003280600532889366, Test Loss: 0.0010597261134535074\n",
      "Epoch [6290/100000], Train Loss: 0.0032049540895968676, Test Loss: 0.0010386448120698333\n",
      "Epoch [6300/100000], Train Loss: 0.0031422569882124662, Test Loss: 0.0009580653277225792\n",
      "Epoch [6310/100000], Train Loss: 0.0031370532233268023, Test Loss: 0.0010614889906719327\n",
      "Epoch [6320/100000], Train Loss: 0.003163828980177641, Test Loss: 0.0010725483298301697\n",
      "Epoch [6330/100000], Train Loss: 0.003323393175378442, Test Loss: 0.0009867064654827118\n",
      "Epoch [6340/100000], Train Loss: 0.0032293591648340225, Test Loss: 0.0009482286986894906\n",
      "Epoch [6350/100000], Train Loss: 0.00314917741343379, Test Loss: 0.0010730740614235401\n",
      "Epoch [6360/100000], Train Loss: 0.003162998240441084, Test Loss: 0.0010211713379248977\n",
      "Epoch [6370/100000], Train Loss: 0.0031079845502972603, Test Loss: 0.001005442813038826\n",
      "Epoch [6380/100000], Train Loss: 0.0031177198980003595, Test Loss: 0.001003987155854702\n",
      "Epoch [6390/100000], Train Loss: 0.0031433168333023787, Test Loss: 0.0010136001510545611\n",
      "Epoch [6400/100000], Train Loss: 0.0032113518100231886, Test Loss: 0.0011223957408219576\n",
      "Epoch [6410/100000], Train Loss: 0.0030474504455924034, Test Loss: 0.0010243267752230167\n",
      "Epoch [6420/100000], Train Loss: 0.003080884926021099, Test Loss: 0.0009599836193956435\n",
      "Epoch [6430/100000], Train Loss: 0.0031978993210941553, Test Loss: 0.0010275309905409813\n",
      "Epoch [6440/100000], Train Loss: 0.003110236721113324, Test Loss: 0.0010170841123908758\n",
      "Epoch [6450/100000], Train Loss: 0.0032674288377165794, Test Loss: 0.0010376254795119166\n",
      "Epoch [6460/100000], Train Loss: 0.0032542687840759754, Test Loss: 0.0010229165200144053\n",
      "Epoch [6470/100000], Train Loss: 0.0031416499987244606, Test Loss: 0.0010324433678761125\n",
      "Epoch [6480/100000], Train Loss: 0.0033241689670830965, Test Loss: 0.0010150679154321551\n",
      "Epoch [6490/100000], Train Loss: 0.003328531514853239, Test Loss: 0.0010029427940025926\n",
      "Epoch [6500/100000], Train Loss: 0.003206441178917885, Test Loss: 0.001059116912074387\n",
      "Epoch [6510/100000], Train Loss: 0.0031574845779687166, Test Loss: 0.0009673098684288561\n",
      "Epoch [6520/100000], Train Loss: 0.0029716112185269594, Test Loss: 0.0008620769949629903\n",
      "Epoch [6530/100000], Train Loss: 0.0033567349892109632, Test Loss: 0.0009893799433484674\n",
      "Epoch [6540/100000], Train Loss: 0.003230556845664978, Test Loss: 0.0010442992206662893\n",
      "Epoch [6550/100000], Train Loss: 0.0031790025532245636, Test Loss: 0.0010217333910986781\n",
      "Epoch [6560/100000], Train Loss: 0.003067055018618703, Test Loss: 0.0009865069296211004\n",
      "Epoch [6570/100000], Train Loss: 0.003089240286499262, Test Loss: 0.0011194320395588875\n",
      "Epoch [6580/100000], Train Loss: 0.0030782201793044806, Test Loss: 0.0010171002941206098\n",
      "Epoch [6590/100000], Train Loss: 0.003016902133822441, Test Loss: 0.0010147321736440063\n",
      "Epoch [6600/100000], Train Loss: 0.0031645335257053375, Test Loss: 0.0010853278217837214\n",
      "Epoch [6610/100000], Train Loss: 0.003023379249498248, Test Loss: 0.0009540727478452027\n",
      "Epoch [6620/100000], Train Loss: 0.0031248608138412237, Test Loss: 0.0010283744195476174\n",
      "Epoch [6630/100000], Train Loss: 0.0031053831335157156, Test Loss: 0.0010268321493640542\n",
      "Epoch [6640/100000], Train Loss: 0.0031424874905496836, Test Loss: 0.0009865122847259045\n",
      "Epoch [6650/100000], Train Loss: 0.003207359928637743, Test Loss: 0.0010153433540835977\n",
      "Epoch [6660/100000], Train Loss: 0.0030422196723520756, Test Loss: 0.0010476025054231286\n",
      "Epoch [6670/100000], Train Loss: 0.003193992655724287, Test Loss: 0.0009923652978613973\n",
      "Epoch [6680/100000], Train Loss: 0.003115892643108964, Test Loss: 0.0010191380279138684\n",
      "Epoch [6690/100000], Train Loss: 0.002972739515826106, Test Loss: 0.0009763995767571032\n",
      "Epoch [6700/100000], Train Loss: 0.0030598954763263464, Test Loss: 0.0009868093766272068\n",
      "Epoch [6710/100000], Train Loss: 0.0031569621060043573, Test Loss: 0.0009399176342412829\n",
      "Epoch [6720/100000], Train Loss: 0.0031736111268401146, Test Loss: 0.0009851910872384906\n",
      "Epoch [6730/100000], Train Loss: 0.003095964901149273, Test Loss: 0.0010090101277455688\n",
      "Epoch [6740/100000], Train Loss: 0.0030708410777151585, Test Loss: 0.000930414127651602\n",
      "Epoch [6750/100000], Train Loss: 0.003127089235931635, Test Loss: 0.0009878120617941022\n",
      "Epoch [6760/100000], Train Loss: 0.0031118053011596203, Test Loss: 0.0009097730508074164\n",
      "Epoch [6770/100000], Train Loss: 0.0031059591565281153, Test Loss: 0.000956356234382838\n",
      "Epoch [6780/100000], Train Loss: 0.0031855700071901083, Test Loss: 0.0010711344657465816\n",
      "Epoch [6790/100000], Train Loss: 0.0031144183594733477, Test Loss: 0.0010395728750154376\n",
      "Epoch [6800/100000], Train Loss: 0.00309343496337533, Test Loss: 0.0009757803054526448\n",
      "Epoch [6810/100000], Train Loss: 0.003154174890369177, Test Loss: 0.0009819336701184511\n",
      "Epoch [6820/100000], Train Loss: 0.003121680114418268, Test Loss: 0.0009547078516334295\n",
      "Epoch [6830/100000], Train Loss: 0.0030965502373874187, Test Loss: 0.0009563479688949883\n",
      "Epoch [6840/100000], Train Loss: 0.0030025020241737366, Test Loss: 0.0010702366707846522\n",
      "Epoch [6850/100000], Train Loss: 0.003137041348963976, Test Loss: 0.0009889907669276\n",
      "Epoch [6860/100000], Train Loss: 0.003067516488954425, Test Loss: 0.0009318572119809687\n",
      "Epoch [6870/100000], Train Loss: 0.0030897799879312515, Test Loss: 0.0010883426293730736\n",
      "Epoch [6880/100000], Train Loss: 0.0031049498356878757, Test Loss: 0.0009569046669639647\n",
      "Epoch [6890/100000], Train Loss: 0.0030517096165567636, Test Loss: 0.0008764562662690878\n",
      "Epoch [6900/100000], Train Loss: 0.003197276033461094, Test Loss: 0.0010315062245354056\n",
      "Epoch [6910/100000], Train Loss: 0.0032029321882873774, Test Loss: 0.0010701713617891073\n",
      "Epoch [6920/100000], Train Loss: 0.0031178805511444807, Test Loss: 0.0010901866480708122\n",
      "Epoch [6930/100000], Train Loss: 0.0030177824664860964, Test Loss: 0.0009743805858306587\n",
      "Epoch [6940/100000], Train Loss: 0.003218141384422779, Test Loss: 0.0009843388106673956\n",
      "Epoch [6950/100000], Train Loss: 0.0029275829438120127, Test Loss: 0.0009218408958986402\n",
      "Epoch [6960/100000], Train Loss: 0.0031739480327814817, Test Loss: 0.001021919073536992\n",
      "Epoch [6970/100000], Train Loss: 0.0031814149115234613, Test Loss: 0.0009579150937497616\n",
      "Epoch [6980/100000], Train Loss: 0.0030603136401623487, Test Loss: 0.0009559537866152823\n",
      "Epoch [6990/100000], Train Loss: 0.0030073148664087057, Test Loss: 0.000995379639789462\n",
      "Epoch [7000/100000], Train Loss: 0.0031576810870319605, Test Loss: 0.0010242954595014453\n",
      "Epoch [7010/100000], Train Loss: 0.0031792917288839817, Test Loss: 0.0009723055409267545\n",
      "Epoch [7020/100000], Train Loss: 0.0030996801797300577, Test Loss: 0.000945713953115046\n",
      "Epoch [7030/100000], Train Loss: 0.0031142367515712976, Test Loss: 0.0010188915766775608\n",
      "Epoch [7040/100000], Train Loss: 0.003012014552950859, Test Loss: 0.0009669671999290586\n",
      "Epoch [7050/100000], Train Loss: 0.0030559373553842306, Test Loss: 0.000953545852098614\n",
      "Epoch [7060/100000], Train Loss: 0.003024062607437372, Test Loss: 0.0010040722554549575\n",
      "Epoch [7070/100000], Train Loss: 0.0032117541413754225, Test Loss: 0.0009590885601937771\n",
      "Epoch [7080/100000], Train Loss: 0.003238714998587966, Test Loss: 0.0009510563686490059\n",
      "Epoch [7090/100000], Train Loss: 0.0030076191760599613, Test Loss: 0.0009226826950907707\n",
      "Epoch [7100/100000], Train Loss: 0.003190199378877878, Test Loss: 0.0010551920859143138\n",
      "Epoch [7110/100000], Train Loss: 0.003112525213509798, Test Loss: 0.000926734006498009\n",
      "Epoch [7120/100000], Train Loss: 0.0030720664653927088, Test Loss: 0.0009534278651699424\n",
      "Epoch [7130/100000], Train Loss: 0.003079386427998543, Test Loss: 0.0009958407608792186\n",
      "Epoch [7140/100000], Train Loss: 0.0030284321401268244, Test Loss: 0.0009459430002607405\n",
      "Epoch [7150/100000], Train Loss: 0.003098891582340002, Test Loss: 0.000925424334127456\n",
      "Epoch [7160/100000], Train Loss: 0.002962725469842553, Test Loss: 0.001000917050987482\n",
      "Epoch [7170/100000], Train Loss: 0.00317784515209496, Test Loss: 0.0009158215834759176\n",
      "Epoch [7180/100000], Train Loss: 0.0029621697030961514, Test Loss: 0.0009624293306842446\n",
      "Epoch [7190/100000], Train Loss: 0.003036815905943513, Test Loss: 0.0009923538891598582\n",
      "Epoch [7200/100000], Train Loss: 0.0030052687507122755, Test Loss: 0.0009358286042697728\n",
      "Epoch [7210/100000], Train Loss: 0.00304985954426229, Test Loss: 0.0009933984838426113\n",
      "Epoch [7220/100000], Train Loss: 0.0031617700587958097, Test Loss: 0.0009488615323789418\n",
      "Epoch [7230/100000], Train Loss: 0.003091939026489854, Test Loss: 0.0009279060759581625\n",
      "Epoch [7240/100000], Train Loss: 0.0030729584395885468, Test Loss: 0.0009604734950698912\n",
      "Epoch [7250/100000], Train Loss: 0.003022708697244525, Test Loss: 0.0009535409626550972\n",
      "Epoch [7260/100000], Train Loss: 0.0030556933488696814, Test Loss: 0.0009643289959058166\n",
      "Epoch [7270/100000], Train Loss: 0.0031776900868862867, Test Loss: 0.001091169542632997\n",
      "Epoch [7280/100000], Train Loss: 0.003080140333622694, Test Loss: 0.0009593197610229254\n",
      "Epoch [7290/100000], Train Loss: 0.003127574920654297, Test Loss: 0.0008680088794790208\n",
      "Epoch [7300/100000], Train Loss: 0.0029816152527928352, Test Loss: 0.0009455913677811623\n",
      "Epoch [7310/100000], Train Loss: 0.003084057243540883, Test Loss: 0.0009532819967716932\n",
      "Epoch [7320/100000], Train Loss: 0.003019578056409955, Test Loss: 0.0009746503783389926\n",
      "Epoch [7330/100000], Train Loss: 0.003013840178027749, Test Loss: 0.0009338789968751371\n",
      "Epoch [7340/100000], Train Loss: 0.0030812122859060764, Test Loss: 0.0009295835625380278\n",
      "Epoch [7350/100000], Train Loss: 0.0029894413892179728, Test Loss: 0.0009162515052594244\n",
      "Epoch [7360/100000], Train Loss: 0.0029376025777310133, Test Loss: 0.0009847900364547968\n",
      "Epoch [7370/100000], Train Loss: 0.003091563703492284, Test Loss: 0.0009386478341184556\n",
      "Epoch [7380/100000], Train Loss: 0.003079855814576149, Test Loss: 0.0009868732886388898\n",
      "Epoch [7390/100000], Train Loss: 0.003054839326068759, Test Loss: 0.0009137096349149942\n",
      "Epoch [7400/100000], Train Loss: 0.0030164464842528105, Test Loss: 0.0008877717773430049\n",
      "Epoch [7410/100000], Train Loss: 0.0032053894829005003, Test Loss: 0.0010023982031270862\n",
      "Epoch [7420/100000], Train Loss: 0.0030059630516916513, Test Loss: 0.0010030526900663972\n",
      "Epoch [7430/100000], Train Loss: 0.0031056723091751337, Test Loss: 0.0008421930833719671\n",
      "Epoch [7440/100000], Train Loss: 0.0030662058852612972, Test Loss: 0.0009339470416307449\n",
      "Epoch [7450/100000], Train Loss: 0.002931172028183937, Test Loss: 0.0008900600369088352\n",
      "Epoch [7460/100000], Train Loss: 0.002924850210547447, Test Loss: 0.0008553795050829649\n",
      "Epoch [7470/100000], Train Loss: 0.0029233316890895367, Test Loss: 0.0008436253992840648\n",
      "Epoch [7480/100000], Train Loss: 0.002919892780482769, Test Loss: 0.0009247721172869205\n",
      "Epoch [7490/100000], Train Loss: 0.0030348512809723616, Test Loss: 0.000938228506129235\n",
      "Epoch [7500/100000], Train Loss: 0.0030079567804932594, Test Loss: 0.0009574485011398792\n",
      "Epoch [7510/100000], Train Loss: 0.0030022612772881985, Test Loss: 0.0009000884601846337\n",
      "Epoch [7520/100000], Train Loss: 0.0029870427679270506, Test Loss: 0.0009155255975201726\n",
      "Epoch [7530/100000], Train Loss: 0.0029971078038215637, Test Loss: 0.0009142372291535139\n",
      "Epoch [7540/100000], Train Loss: 0.0029569712933152914, Test Loss: 0.0009728276636451483\n",
      "Epoch [7550/100000], Train Loss: 0.0031046762596815825, Test Loss: 0.0008550575585104525\n",
      "Epoch [7560/100000], Train Loss: 0.00294342334382236, Test Loss: 0.0009141528280451894\n",
      "Epoch [7570/100000], Train Loss: 0.0029418254271149635, Test Loss: 0.0009837368270382285\n",
      "Epoch [7580/100000], Train Loss: 0.0029558585956692696, Test Loss: 0.0008566464530304074\n",
      "Epoch [7590/100000], Train Loss: 0.0029345788061618805, Test Loss: 0.0009220528299920261\n",
      "Epoch [7600/100000], Train Loss: 0.0028499397449195385, Test Loss: 0.0008977485122159123\n",
      "Epoch [7610/100000], Train Loss: 0.0031119866762310266, Test Loss: 0.0008904422866180539\n",
      "Epoch [7620/100000], Train Loss: 0.0029002658557146788, Test Loss: 0.0008467762963846326\n",
      "Epoch [7630/100000], Train Loss: 0.0030020377598702908, Test Loss: 0.0009253846947103739\n",
      "Epoch [7640/100000], Train Loss: 0.002907422836869955, Test Loss: 0.0008776810136623681\n",
      "Epoch [7650/100000], Train Loss: 0.0029618293046951294, Test Loss: 0.0009401981951668859\n",
      "Epoch [7660/100000], Train Loss: 0.0029281622264534235, Test Loss: 0.0009184067603200674\n",
      "Epoch [7670/100000], Train Loss: 0.002916249679401517, Test Loss: 0.0008712647249922156\n",
      "Epoch [7680/100000], Train Loss: 0.0028599423822015524, Test Loss: 0.0009720103698782623\n",
      "Epoch [7690/100000], Train Loss: 0.0029326584190130234, Test Loss: 0.0009392043575644493\n",
      "Epoch [7700/100000], Train Loss: 0.002940072678029537, Test Loss: 0.0008797472692094743\n",
      "Epoch [7710/100000], Train Loss: 0.002915505087003112, Test Loss: 0.0008214712142944336\n",
      "Epoch [7720/100000], Train Loss: 0.003051887731999159, Test Loss: 0.000888578244484961\n",
      "Epoch [7730/100000], Train Loss: 0.0027841285336762667, Test Loss: 0.0009158424218185246\n",
      "Epoch [7740/100000], Train Loss: 0.002982557285577059, Test Loss: 0.0008458346128463745\n",
      "Epoch [7750/100000], Train Loss: 0.002940982347354293, Test Loss: 0.000868177623488009\n",
      "Epoch [7760/100000], Train Loss: 0.002992725931107998, Test Loss: 0.0009975336724892259\n",
      "Epoch [7770/100000], Train Loss: 0.0031344215385615826, Test Loss: 0.0008346590329892933\n",
      "Epoch [7780/100000], Train Loss: 0.002911682240664959, Test Loss: 0.0009143775678239763\n",
      "Epoch [7790/100000], Train Loss: 0.002939803060144186, Test Loss: 0.0008654965786263347\n",
      "Epoch [7800/100000], Train Loss: 0.0028803294990211725, Test Loss: 0.0009342273115180433\n",
      "Epoch [7810/100000], Train Loss: 0.0030069469939917326, Test Loss: 0.0009459819993935525\n",
      "Epoch [7820/100000], Train Loss: 0.0028988474514335394, Test Loss: 0.0008888227748684585\n",
      "Epoch [7830/100000], Train Loss: 0.002959711477160454, Test Loss: 0.0010261371498927474\n",
      "Epoch [7840/100000], Train Loss: 0.002997639589011669, Test Loss: 0.0008755953749641776\n",
      "Epoch [7850/100000], Train Loss: 0.0030270901042968035, Test Loss: 0.0009145070798695087\n",
      "Epoch [7860/100000], Train Loss: 0.0029040733352303505, Test Loss: 0.0009062587632797658\n",
      "Epoch [7870/100000], Train Loss: 0.0029388098046183586, Test Loss: 0.0008816307527013123\n",
      "Epoch [7880/100000], Train Loss: 0.003092339029535651, Test Loss: 0.0009811874479055405\n",
      "Epoch [7890/100000], Train Loss: 0.003003528108820319, Test Loss: 0.0008586985059082508\n",
      "Epoch [7900/100000], Train Loss: 0.0028908441308885813, Test Loss: 0.0008951693307608366\n",
      "Epoch [7910/100000], Train Loss: 0.0027925618924200535, Test Loss: 0.0008445971761830151\n",
      "Epoch [7920/100000], Train Loss: 0.0028308965265750885, Test Loss: 0.0009146390366367996\n",
      "Epoch [7930/100000], Train Loss: 0.003055004868656397, Test Loss: 0.0009172879508696496\n",
      "Epoch [7940/100000], Train Loss: 0.0029026330448687077, Test Loss: 0.0009239548235200346\n",
      "Epoch [7950/100000], Train Loss: 0.002860396634787321, Test Loss: 0.0008994300733320415\n",
      "Epoch [7960/100000], Train Loss: 0.002895339159294963, Test Loss: 0.0008731765556149185\n",
      "Epoch [7970/100000], Train Loss: 0.0027643961366266012, Test Loss: 0.0009496978018432856\n",
      "Epoch [7980/100000], Train Loss: 0.002857750514522195, Test Loss: 0.000817829801235348\n",
      "Epoch [7990/100000], Train Loss: 0.0028713771607726812, Test Loss: 0.0008541728020645678\n",
      "Epoch [8000/100000], Train Loss: 0.002846876159310341, Test Loss: 0.0009162062779068947\n",
      "Epoch [8010/100000], Train Loss: 0.0030054240487515926, Test Loss: 0.0009972500847652555\n",
      "Epoch [8020/100000], Train Loss: 0.003021825337782502, Test Loss: 0.0009613660513423383\n",
      "Epoch [8030/100000], Train Loss: 0.002987728687003255, Test Loss: 0.0008821458905003965\n",
      "Epoch [8040/100000], Train Loss: 0.002993355505168438, Test Loss: 0.0009178845211863518\n",
      "Epoch [8050/100000], Train Loss: 0.0030950959771871567, Test Loss: 0.00091225077630952\n",
      "Epoch [8060/100000], Train Loss: 0.0029685492627322674, Test Loss: 0.000906942063011229\n",
      "Epoch [8070/100000], Train Loss: 0.00307151535525918, Test Loss: 0.0009074887493625283\n",
      "Epoch [8080/100000], Train Loss: 0.0029164666775614023, Test Loss: 0.0009044292382895947\n",
      "Epoch [8090/100000], Train Loss: 0.0029458883218467236, Test Loss: 0.0008785648969933391\n",
      "Epoch [8100/100000], Train Loss: 0.0029523575212806463, Test Loss: 0.0009076533024199307\n",
      "Epoch [8110/100000], Train Loss: 0.0029391921125352383, Test Loss: 0.0009190474520437419\n",
      "Epoch [8120/100000], Train Loss: 0.0030261308420449495, Test Loss: 0.0008320617489516735\n",
      "Epoch [8130/100000], Train Loss: 0.003076632972806692, Test Loss: 0.0009028232889249921\n",
      "Epoch [8140/100000], Train Loss: 0.0028852534014731646, Test Loss: 0.0009054302354343235\n",
      "Epoch [8150/100000], Train Loss: 0.0030114990659058094, Test Loss: 0.0009072857210412621\n",
      "Epoch [8160/100000], Train Loss: 0.00286352657712996, Test Loss: 0.0009693746687844396\n",
      "Epoch [8170/100000], Train Loss: 0.0028353219386190176, Test Loss: 0.0008759937481954694\n",
      "Epoch [8180/100000], Train Loss: 0.002926770132035017, Test Loss: 0.0008919895044527948\n",
      "Epoch [8190/100000], Train Loss: 0.003000975353643298, Test Loss: 0.0009038143907673657\n",
      "Epoch [8200/100000], Train Loss: 0.0030024887528270483, Test Loss: 0.0009282928658649325\n",
      "Epoch [8210/100000], Train Loss: 0.002916853642091155, Test Loss: 0.0009088509250432253\n",
      "Epoch [8220/100000], Train Loss: 0.0029565661679953337, Test Loss: 0.0008433368639089167\n",
      "Epoch [8230/100000], Train Loss: 0.0028029943350702524, Test Loss: 0.0008738646283745766\n",
      "Epoch [8240/100000], Train Loss: 0.00291827111504972, Test Loss: 0.0009540265891700983\n",
      "Epoch [8250/100000], Train Loss: 0.002858626190572977, Test Loss: 0.0009509222581982613\n",
      "Epoch [8260/100000], Train Loss: 0.0028447480872273445, Test Loss: 0.0008100327104330063\n",
      "Epoch [8270/100000], Train Loss: 0.002911950461566448, Test Loss: 0.0008997771656140685\n",
      "Epoch [8280/100000], Train Loss: 0.002945478307083249, Test Loss: 0.0009630551794543862\n",
      "Epoch [8290/100000], Train Loss: 0.0027889993507415056, Test Loss: 0.0009115854045376182\n",
      "Epoch [8300/100000], Train Loss: 0.002854126039892435, Test Loss: 0.0008053537458181381\n",
      "Epoch [8310/100000], Train Loss: 0.0028773120138794184, Test Loss: 0.0008994028903543949\n",
      "Epoch [8320/100000], Train Loss: 0.002963562961667776, Test Loss: 0.0007584226550534368\n",
      "Epoch [8330/100000], Train Loss: 0.0028669778257608414, Test Loss: 0.0009528387454338372\n",
      "Epoch [8340/100000], Train Loss: 0.0030131132807582617, Test Loss: 0.0008685524226166308\n",
      "Epoch [8350/100000], Train Loss: 0.002890604315325618, Test Loss: 0.0008797713089734316\n",
      "Epoch [8360/100000], Train Loss: 0.0029103681445121765, Test Loss: 0.0008185749757103622\n",
      "Epoch [8370/100000], Train Loss: 0.0028790875803679228, Test Loss: 0.0008377204067073762\n",
      "Epoch [8380/100000], Train Loss: 0.0029028847347944975, Test Loss: 0.0008717369055375457\n",
      "Epoch [8390/100000], Train Loss: 0.0029962812550365925, Test Loss: 0.0009410685743205249\n",
      "Epoch [8400/100000], Train Loss: 0.002967187436297536, Test Loss: 0.0008208491490222514\n",
      "Epoch [8410/100000], Train Loss: 0.0029092682525515556, Test Loss: 0.0009146811789833009\n",
      "Epoch [8420/100000], Train Loss: 0.0028916443698108196, Test Loss: 0.0009460190194658935\n",
      "Epoch [8430/100000], Train Loss: 0.0029881757218390703, Test Loss: 0.0008739478071220219\n",
      "Epoch [8440/100000], Train Loss: 0.0027651023119688034, Test Loss: 0.0008455811184830964\n",
      "Epoch [8450/100000], Train Loss: 0.0029587014578282833, Test Loss: 0.0008691581315360963\n",
      "Epoch [8460/100000], Train Loss: 0.0029177977703511715, Test Loss: 0.0008389610447920859\n",
      "Epoch [8470/100000], Train Loss: 0.0030144844204187393, Test Loss: 0.0009196956525556743\n",
      "Epoch [8480/100000], Train Loss: 0.0028917815070599318, Test Loss: 0.0009568900568410754\n",
      "Epoch [8490/100000], Train Loss: 0.002948653418570757, Test Loss: 0.000900225481018424\n",
      "Epoch [8500/100000], Train Loss: 0.0028064213693141937, Test Loss: 0.0009369468898512423\n",
      "Epoch [8510/100000], Train Loss: 0.0030369434971362352, Test Loss: 0.0008418508805334568\n",
      "Epoch [8520/100000], Train Loss: 0.002714718459174037, Test Loss: 0.0008950484916567802\n",
      "Epoch [8530/100000], Train Loss: 0.0029215491376817226, Test Loss: 0.0008678588783368468\n",
      "Epoch [8540/100000], Train Loss: 0.002986837411299348, Test Loss: 0.000881848216522485\n",
      "Epoch [8550/100000], Train Loss: 0.0028860154561698437, Test Loss: 0.0008995458483695984\n",
      "Epoch [8560/100000], Train Loss: 0.002799759618937969, Test Loss: 0.0008088925387710333\n",
      "Epoch [8570/100000], Train Loss: 0.0030169575475156307, Test Loss: 0.0009314644848927855\n",
      "Epoch [8580/100000], Train Loss: 0.0029664142057299614, Test Loss: 0.0008859804947860539\n",
      "Epoch [8590/100000], Train Loss: 0.002828992437571287, Test Loss: 0.0009195101447403431\n",
      "Epoch [8600/100000], Train Loss: 0.002793805906549096, Test Loss: 0.0008752791909500957\n",
      "Epoch [8610/100000], Train Loss: 0.002966448664665222, Test Loss: 0.0009288729634135962\n",
      "Epoch [8620/100000], Train Loss: 0.0028102900832891464, Test Loss: 0.0008416995406150818\n",
      "Epoch [8630/100000], Train Loss: 0.002923879073932767, Test Loss: 0.0009419213165529072\n",
      "Epoch [8640/100000], Train Loss: 0.0028515555895864964, Test Loss: 0.0009258771315217018\n",
      "Epoch [8650/100000], Train Loss: 0.002850553020834923, Test Loss: 0.0007819928578101099\n",
      "Epoch [8660/100000], Train Loss: 0.002917306497693062, Test Loss: 0.000938493583817035\n",
      "Epoch [8670/100000], Train Loss: 0.002959644654765725, Test Loss: 0.000821848283521831\n",
      "Epoch [8680/100000], Train Loss: 0.0028754870872944593, Test Loss: 0.000884059991221875\n",
      "Epoch [8690/100000], Train Loss: 0.0028029149398207664, Test Loss: 0.0008427322027273476\n",
      "Epoch [8700/100000], Train Loss: 0.0029813291039317846, Test Loss: 0.000868860341142863\n",
      "Epoch [8710/100000], Train Loss: 0.0028382271993905306, Test Loss: 0.0009920523734763265\n",
      "Epoch [8720/100000], Train Loss: 0.003012054832652211, Test Loss: 0.0007965681725181639\n",
      "Epoch [8730/100000], Train Loss: 0.002754785818979144, Test Loss: 0.0008512990898452699\n",
      "Epoch [8740/100000], Train Loss: 0.002987898653373122, Test Loss: 0.0008836305350996554\n",
      "Epoch [8750/100000], Train Loss: 0.0029269729275256395, Test Loss: 0.0008753507281653583\n",
      "Epoch [8760/100000], Train Loss: 0.002936949487775564, Test Loss: 0.0009108264348469675\n",
      "Epoch [8770/100000], Train Loss: 0.0028748302720487118, Test Loss: 0.0008384856046177447\n",
      "Epoch [8780/100000], Train Loss: 0.0029210997745394707, Test Loss: 0.000806307012680918\n",
      "Epoch [8790/100000], Train Loss: 0.002850800985470414, Test Loss: 0.0008634045370854437\n",
      "Epoch [8800/100000], Train Loss: 0.002779896603897214, Test Loss: 0.0008483413839712739\n",
      "Epoch [8810/100000], Train Loss: 0.0030871073249727488, Test Loss: 0.0009034728282131255\n",
      "Epoch [8820/100000], Train Loss: 0.0028417857829481363, Test Loss: 0.0008793500019237399\n",
      "Epoch [8830/100000], Train Loss: 0.002883866196498275, Test Loss: 0.0008983670268207788\n",
      "Epoch [8840/100000], Train Loss: 0.0031545243691653013, Test Loss: 0.0008146819309331477\n",
      "Epoch [8850/100000], Train Loss: 0.002935804193839431, Test Loss: 0.0008201102609746158\n",
      "Epoch [8860/100000], Train Loss: 0.002810979960486293, Test Loss: 0.0008833151659928262\n",
      "Epoch [8870/100000], Train Loss: 0.002838010201230645, Test Loss: 0.0008253415580838919\n",
      "Epoch [8880/100000], Train Loss: 0.0028371422085911036, Test Loss: 0.0008309096447192132\n",
      "Epoch [8890/100000], Train Loss: 0.002853250131011009, Test Loss: 0.000822153699118644\n",
      "Epoch [8900/100000], Train Loss: 0.0028590981382876635, Test Loss: 0.0008475151262246072\n",
      "Epoch [8910/100000], Train Loss: 0.0029649126809090376, Test Loss: 0.0009039000142365694\n",
      "Epoch [8920/100000], Train Loss: 0.002934505930170417, Test Loss: 0.0007740426808595657\n",
      "Epoch [8930/100000], Train Loss: 0.0029218862764537334, Test Loss: 0.0008374867611564696\n",
      "Epoch [8940/100000], Train Loss: 0.002787270350381732, Test Loss: 0.0008741017081774771\n",
      "Epoch [8950/100000], Train Loss: 0.0028056574519723654, Test Loss: 0.0008600306464359164\n",
      "Epoch [8960/100000], Train Loss: 0.002846228191629052, Test Loss: 0.0009489742806181312\n",
      "Epoch [8970/100000], Train Loss: 0.0028406644705682993, Test Loss: 0.000824744813144207\n",
      "Epoch [8980/100000], Train Loss: 0.002868382493034005, Test Loss: 0.0009419064735993743\n",
      "Epoch [8990/100000], Train Loss: 0.0028415608685463667, Test Loss: 0.0009100916795432568\n",
      "Epoch [9000/100000], Train Loss: 0.002855455968528986, Test Loss: 0.0008131237700581551\n",
      "Epoch [9010/100000], Train Loss: 0.002957257442176342, Test Loss: 0.0008598308777436614\n",
      "Epoch [9020/100000], Train Loss: 0.0028645023703575134, Test Loss: 0.0008785981335677207\n",
      "Epoch [9030/100000], Train Loss: 0.002886193571612239, Test Loss: 0.000814719358459115\n",
      "Epoch [9040/100000], Train Loss: 0.0028551165014505386, Test Loss: 0.0008387970156036317\n",
      "Epoch [9050/100000], Train Loss: 0.002917003585025668, Test Loss: 0.000844815862365067\n",
      "Epoch [9060/100000], Train Loss: 0.002855002647265792, Test Loss: 0.0009358127717860043\n",
      "Epoch [9070/100000], Train Loss: 0.0029092023614794016, Test Loss: 0.000840510823763907\n",
      "Epoch [9080/100000], Train Loss: 0.0029212080407887697, Test Loss: 0.0009896443225443363\n",
      "Epoch [9090/100000], Train Loss: 0.00276739988476038, Test Loss: 0.00086497439770028\n",
      "Epoch [9100/100000], Train Loss: 0.002789625898003578, Test Loss: 0.0008068174356594682\n",
      "Epoch [9110/100000], Train Loss: 0.002888037357479334, Test Loss: 0.0008419379591941833\n",
      "Epoch [9120/100000], Train Loss: 0.002833125414326787, Test Loss: 0.0008892182377167046\n",
      "Epoch [9130/100000], Train Loss: 0.0029661941807717085, Test Loss: 0.0008644335903227329\n",
      "Epoch [9140/100000], Train Loss: 0.0028104635421186686, Test Loss: 0.0007923975354060531\n",
      "Epoch [9150/100000], Train Loss: 0.002770777326077223, Test Loss: 0.0008148187771439552\n",
      "Epoch [9160/100000], Train Loss: 0.0029440398793667555, Test Loss: 0.0008043848793022335\n",
      "Epoch [9170/100000], Train Loss: 0.002778376219794154, Test Loss: 0.0008809186401776969\n",
      "Epoch [9180/100000], Train Loss: 0.00284148333594203, Test Loss: 0.0009035483235493302\n",
      "Epoch [9190/100000], Train Loss: 0.002896509598940611, Test Loss: 0.0007830881513655186\n",
      "Epoch [9200/100000], Train Loss: 0.0029184501618146896, Test Loss: 0.0008700544130988419\n",
      "Epoch [9210/100000], Train Loss: 0.0029418764170259237, Test Loss: 0.0009230780997313559\n",
      "Epoch [9220/100000], Train Loss: 0.0030019530095160007, Test Loss: 0.0008622645400464535\n",
      "Epoch [9230/100000], Train Loss: 0.0029546318110078573, Test Loss: 0.0009355393121950328\n",
      "Epoch [9240/100000], Train Loss: 0.0029491123277693987, Test Loss: 0.000836385297589004\n",
      "Epoch [9250/100000], Train Loss: 0.0028869668021798134, Test Loss: 0.0008934001089073718\n",
      "Epoch [9260/100000], Train Loss: 0.0028497043531388044, Test Loss: 0.0007937460904940963\n",
      "Epoch [9270/100000], Train Loss: 0.002875897102057934, Test Loss: 0.000758258393034339\n",
      "Epoch [9280/100000], Train Loss: 0.002934337593615055, Test Loss: 0.0008656586287543178\n",
      "Epoch [9290/100000], Train Loss: 0.0027835580985993147, Test Loss: 0.0008498912211507559\n",
      "Epoch [9300/100000], Train Loss: 0.0029442987870424986, Test Loss: 0.0009283973486162722\n",
      "Epoch [9310/100000], Train Loss: 0.0028997347690165043, Test Loss: 0.0007699819980189204\n",
      "Epoch [9320/100000], Train Loss: 0.002867959439754486, Test Loss: 0.0007993245963007212\n",
      "Epoch [9330/100000], Train Loss: 0.0028814896941184998, Test Loss: 0.0008453619084320962\n",
      "Epoch [9340/100000], Train Loss: 0.0028291416820138693, Test Loss: 0.0008755458402447402\n",
      "Epoch [9350/100000], Train Loss: 0.002867116592824459, Test Loss: 0.0007784756016917527\n",
      "Epoch [9360/100000], Train Loss: 0.002890409901738167, Test Loss: 0.0007812705589458346\n",
      "Epoch [9370/100000], Train Loss: 0.0027337619103491306, Test Loss: 0.0008334071026183665\n",
      "Epoch [9380/100000], Train Loss: 0.002876832615584135, Test Loss: 0.0008544381707906723\n",
      "Epoch [9390/100000], Train Loss: 0.0027977358549833298, Test Loss: 0.0008114138036035001\n",
      "Epoch [9400/100000], Train Loss: 0.002920853905379772, Test Loss: 0.0008188108913600445\n",
      "Epoch [9410/100000], Train Loss: 0.0027811811305582523, Test Loss: 0.0008520434494130313\n",
      "Epoch [9420/100000], Train Loss: 0.0029028900898993015, Test Loss: 0.0008291240083053708\n",
      "Epoch [9430/100000], Train Loss: 0.0027239604387432337, Test Loss: 0.0008371045696549118\n",
      "Epoch [9440/100000], Train Loss: 0.0027807499282062054, Test Loss: 0.0008195812697522342\n",
      "Epoch [9450/100000], Train Loss: 0.0029495388735085726, Test Loss: 0.000846787472255528\n",
      "Epoch [9460/100000], Train Loss: 0.0028758449479937553, Test Loss: 0.0009627382387407124\n",
      "Epoch [9470/100000], Train Loss: 0.00296534295193851, Test Loss: 0.0008553548832423985\n",
      "Epoch [9480/100000], Train Loss: 0.002822685521095991, Test Loss: 0.0007712880033068359\n",
      "Epoch [9490/100000], Train Loss: 0.0028073559515178204, Test Loss: 0.0007978479261510074\n",
      "Epoch [9500/100000], Train Loss: 0.002861153334379196, Test Loss: 0.0008922137203626335\n",
      "Epoch [9510/100000], Train Loss: 0.002890395000576973, Test Loss: 0.0008434051414951682\n",
      "Epoch [9520/100000], Train Loss: 0.0028540492057800293, Test Loss: 0.0008273242856375873\n",
      "Epoch [9530/100000], Train Loss: 0.0028690535109490156, Test Loss: 0.0008357870392501354\n",
      "Epoch [9540/100000], Train Loss: 0.002879973268136382, Test Loss: 0.000903426727745682\n",
      "Epoch [9550/100000], Train Loss: 0.002967776032164693, Test Loss: 0.0007381108007393777\n",
      "Epoch [9560/100000], Train Loss: 0.002875317819416523, Test Loss: 0.0008274996071122587\n",
      "Epoch [9570/100000], Train Loss: 0.002883677603676915, Test Loss: 0.0008442968246527016\n",
      "Epoch [9580/100000], Train Loss: 0.002842310117557645, Test Loss: 0.0007412362028844655\n",
      "Epoch [9590/100000], Train Loss: 0.0028196359053254128, Test Loss: 0.000911483250092715\n",
      "Epoch [9600/100000], Train Loss: 0.0028157972265034914, Test Loss: 0.0007762814057059586\n",
      "Epoch [9610/100000], Train Loss: 0.00281253713183105, Test Loss: 0.0008103847503662109\n",
      "Epoch [9620/100000], Train Loss: 0.0028472363483160734, Test Loss: 0.0008419521036557853\n",
      "Epoch [9630/100000], Train Loss: 0.0028430121019482613, Test Loss: 0.0007970696315169334\n",
      "Epoch [9640/100000], Train Loss: 0.00278330291621387, Test Loss: 0.0007631515618413687\n",
      "Epoch [9650/100000], Train Loss: 0.002745460718870163, Test Loss: 0.0007292365771718323\n",
      "Epoch [9660/100000], Train Loss: 0.002770244376733899, Test Loss: 0.0008270199177786708\n",
      "Epoch [9670/100000], Train Loss: 0.0028372996021062136, Test Loss: 0.0008554339874535799\n",
      "Epoch [9680/100000], Train Loss: 0.002669445937499404, Test Loss: 0.0007825276697985828\n",
      "Epoch [9690/100000], Train Loss: 0.002673534909263253, Test Loss: 0.0008015239145606756\n",
      "Epoch [9700/100000], Train Loss: 0.002747725695371628, Test Loss: 0.0008265509968623519\n",
      "Epoch [9710/100000], Train Loss: 0.0028412737883627415, Test Loss: 0.0007743150927126408\n",
      "Epoch [9720/100000], Train Loss: 0.0028685613069683313, Test Loss: 0.0008555076201446354\n",
      "Epoch [9730/100000], Train Loss: 0.002913947682827711, Test Loss: 0.0008447798900306225\n",
      "Epoch [9740/100000], Train Loss: 0.0027793559711426497, Test Loss: 0.0007492135628126562\n",
      "Epoch [9750/100000], Train Loss: 0.002746537560597062, Test Loss: 0.000800017558503896\n",
      "Epoch [9760/100000], Train Loss: 0.0028180128429085016, Test Loss: 0.0008164036553353071\n",
      "Epoch [9770/100000], Train Loss: 0.0028408996295183897, Test Loss: 0.0008274538558907807\n",
      "Epoch [9780/100000], Train Loss: 0.0028660607058554888, Test Loss: 0.0007669284241273999\n",
      "Epoch [9790/100000], Train Loss: 0.0028585304971784353, Test Loss: 0.0007984965341165662\n",
      "Epoch [9800/100000], Train Loss: 0.0028616583440452814, Test Loss: 0.00078876584302634\n",
      "Epoch [9810/100000], Train Loss: 0.002649656031280756, Test Loss: 0.0008378346683457494\n",
      "Epoch [9820/100000], Train Loss: 0.002816583961248398, Test Loss: 0.0008855054620653391\n",
      "Epoch [9830/100000], Train Loss: 0.0028449762612581253, Test Loss: 0.0007941030198708177\n",
      "Epoch [9840/100000], Train Loss: 0.002828117460012436, Test Loss: 0.0008921897388063371\n",
      "Epoch [9850/100000], Train Loss: 0.002812350634485483, Test Loss: 0.0008187807979993522\n",
      "Epoch [9860/100000], Train Loss: 0.0029502545949071646, Test Loss: 0.0008828251739032567\n",
      "Epoch [9870/100000], Train Loss: 0.0028413073159754276, Test Loss: 0.0008206588681787252\n",
      "Epoch [9880/100000], Train Loss: 0.0027492991648614407, Test Loss: 0.0007710650679655373\n",
      "Epoch [9890/100000], Train Loss: 0.0027327078860253096, Test Loss: 0.000781045004259795\n",
      "Epoch [9900/100000], Train Loss: 0.0028192049358040094, Test Loss: 0.0007888188119977713\n",
      "Epoch [9910/100000], Train Loss: 0.0027378711383789778, Test Loss: 0.0008150943322107196\n",
      "Epoch [9920/100000], Train Loss: 0.002760683186352253, Test Loss: 0.0007867751992307603\n",
      "Epoch [9930/100000], Train Loss: 0.0028133706655353308, Test Loss: 0.0007852510316297412\n",
      "Epoch [9940/100000], Train Loss: 0.0027663472574204206, Test Loss: 0.000857278355397284\n",
      "Epoch [9950/100000], Train Loss: 0.0027294554747641087, Test Loss: 0.0007675235392525792\n",
      "Epoch [9960/100000], Train Loss: 0.0028121513314545155, Test Loss: 0.0007899365737102926\n",
      "Epoch [9970/100000], Train Loss: 0.002832463476806879, Test Loss: 0.0008680311148054898\n",
      "Epoch [9980/100000], Train Loss: 0.0028718968387693167, Test Loss: 0.0008574983803555369\n",
      "Epoch [9990/100000], Train Loss: 0.002736323047429323, Test Loss: 0.0008496769587509334\n",
      "Epoch [10000/100000], Train Loss: 0.0027801874093711376, Test Loss: 0.0008618670981377363\n",
      "Epoch [10010/100000], Train Loss: 0.0027559311129152775, Test Loss: 0.0009376744856126606\n",
      "Epoch [10020/100000], Train Loss: 0.0028676725924015045, Test Loss: 0.0007301055011339486\n",
      "Epoch [10030/100000], Train Loss: 0.002798586618155241, Test Loss: 0.000790732738096267\n",
      "Epoch [10040/100000], Train Loss: 0.002689982298761606, Test Loss: 0.0007556942291557789\n",
      "Epoch [10050/100000], Train Loss: 0.00278622773475945, Test Loss: 0.0008378229686059058\n",
      "Epoch [10060/100000], Train Loss: 0.0027099610306322575, Test Loss: 0.0007678300607949495\n",
      "Epoch [10070/100000], Train Loss: 0.002705059014260769, Test Loss: 0.0007999872323125601\n",
      "Epoch [10080/100000], Train Loss: 0.0028887095395475626, Test Loss: 0.0008205360500141978\n",
      "Epoch [10090/100000], Train Loss: 0.0027096301782876253, Test Loss: 0.0007826733635738492\n",
      "Epoch [10100/100000], Train Loss: 0.002768143778666854, Test Loss: 0.0007955686305649579\n",
      "Epoch [10110/100000], Train Loss: 0.0027790418826043606, Test Loss: 0.0007571267196908593\n",
      "Epoch [10120/100000], Train Loss: 0.0027944492176175117, Test Loss: 0.000829544325824827\n",
      "Epoch [10130/100000], Train Loss: 0.0029812485445290804, Test Loss: 0.0007799766608513892\n",
      "Epoch [10140/100000], Train Loss: 0.002783578122034669, Test Loss: 0.0007429207907989621\n",
      "Epoch [10150/100000], Train Loss: 0.002837015315890312, Test Loss: 0.0008528234902769327\n",
      "Epoch [10160/100000], Train Loss: 0.0027080329600721598, Test Loss: 0.0007716227555647492\n",
      "Epoch [10170/100000], Train Loss: 0.0027421291451901197, Test Loss: 0.0007770825759507716\n",
      "Epoch [10180/100000], Train Loss: 0.0028007079381495714, Test Loss: 0.0008749281405471265\n",
      "Epoch [10190/100000], Train Loss: 0.002825795905664563, Test Loss: 0.0008259550668299198\n",
      "Epoch [10200/100000], Train Loss: 0.0027631670236587524, Test Loss: 0.000838938110973686\n",
      "Epoch [10210/100000], Train Loss: 0.0027165801730006933, Test Loss: 0.0007791111129336059\n",
      "Epoch [10220/100000], Train Loss: 0.0027961195446550846, Test Loss: 0.0008537219255231321\n",
      "Early stopping at epoch 10224\n"
     ]
    }
   ],
   "source": [
    "# Move the model to GPU if available\n",
    "model = DeepNN().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Early stopping\n",
    "patience = 1000\n",
    "best_loss = float('inf')\n",
    "best_model_state = None\n",
    "no_improvement_epochs = 0\n",
    "\n",
    "# Training the model\n",
    "num_epochs = 100000\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train)\n",
    "    train_loss = criterion(outputs, y_train)\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model(X_test)\n",
    "        test_loss = criterion(test_outputs, y_test)\n",
    "\n",
    "    train_losses.append(train_loss.item())\n",
    "    test_losses.append(test_loss.item())\n",
    "\n",
    "    if test_loss < best_loss:\n",
    "        best_loss = test_loss\n",
    "        best_model_state = model.state_dict()\n",
    "        no_improvement_epochs = 0\n",
    "    else:\n",
    "        no_improvement_epochs += 1\n",
    "\n",
    "    if no_improvement_epochs >= patience:\n",
    "        print(f'Early stopping at epoch {epoch+1}')\n",
    "        break\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss.item()}, Test Loss: {test_loss.item()}')\n",
    "\n",
    "# Load the best model state\n",
    "if best_model_state is not None:\n",
    "    model.load_state_dict(best_model_state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'avg_flightdata_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the saved model and Predicting with it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_79148/3300405255.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  modelloaded.load_state_dict(torch.load('avg_flightdata_model.pth'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeepNN(\n",
       "  (fc1): Linear(in_features=10, out_features=128, bias=True)\n",
       "  (dropout1): Dropout(p=0.5, inplace=False)\n",
       "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (dropout2): Dropout(p=0.5, inplace=False)\n",
       "  (fc3): Linear(in_features=64, out_features=32, bias=True)\n",
       "  (dropout3): Dropout(p=0.5, inplace=False)\n",
       "  (fc4): Linear(in_features=32, out_features=2, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the saved model\n",
    "modelloaded = DeepNN()  \n",
    "modelloaded.load_state_dict(torch.load('avg_flightdata_model.pth'))\n",
    "modelloaded.to(device)\n",
    "modelloaded.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Lift: 0.9813\n",
      "Predicted Induced Drag: 0.0494\n"
     ]
    }
   ],
   "source": [
    "# Sample input tensor with proper formatting\n",
    "sample_input = torch.tensor([\n",
    "    [\n",
    "        0.0,      # Feature 1\n",
    "        0.0,      # Feature 2\n",
    "        0.0,      # Feature 3\n",
    "        1.0,      # Feature 4\n",
    "        0.076923, # Feature 5\n",
    "        0.076923, # Feature 6\n",
    "        0.000000, # Feature 7\n",
    "        0.222222, # Feature 8\n",
    "        0.222222,  # Feature 9\n",
    "        0.285714  # Feature 10\n",
    "    ]\n",
    "], dtype=torch.float32).to(device)\n",
    "\n",
    "# Make prediction using model\n",
    "with torch.no_grad():\n",
    "    prediction = model(sample_input)\n",
    "    lift, induced_drag = prediction[0].cpu().numpy()\n",
    "\n",
    "print(f\"Predicted Lift: {lift:.4f}\")\n",
    "print(f\"Predicted Induced Drag: {induced_drag:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pteraenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
